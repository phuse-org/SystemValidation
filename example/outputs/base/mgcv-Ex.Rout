
R version 4.0.3 (2020-10-10) -- "Bunny-Wunnies Freak Out"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "mgcv"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('mgcv')
Loading required package: nlme
This is mgcv 1.8-33. For overview type 'help("mgcv-package")'.
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("Beta")
> ### * Beta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: betar
> ### Title: GAM beta regression family
> ### Aliases: betar
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some beta data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> mu <- binomial()$linkinv(dat$f/4-2)
> phi <- .5
> a <- mu*phi;b <- phi - a;
> dat$y <- rbeta(n,a,b) 
> 
> bm <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=betar(link="logit"),data=dat)
> 
> bm

Family: Beta regression(0.491) 
Link function: logit 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
1.73 1.63 5.62 1.00  total = 10.98 

REML score: -991.9735     
> plot(bm,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("FFdes")
> ### * FFdes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FFdes
> ### Title: Level 5 fractional factorial designs
> ### Aliases: FFdes
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   plot(rbind(0,FFdes(2,TRUE)),xlab="x",ylab="y",
+        col=c(2,1,1,1,1,4,4,4,4),pch=19,main="CCD")
>   FFdes(5)
      [,1] [,2] [,3] [,4] [,5]
 [1,]    1    1    1    1    1
 [2,]   -1    1    1    1   -1
 [3,]    1   -1    1    1   -1
 [4,]   -1   -1    1    1    1
 [5,]    1    1   -1    1   -1
 [6,]   -1    1   -1    1    1
 [7,]    1   -1   -1    1    1
 [8,]   -1   -1   -1    1   -1
 [9,]    1    1    1   -1   -1
[10,]   -1    1    1   -1    1
[11,]    1   -1    1   -1    1
[12,]   -1   -1    1   -1   -1
[13,]    1    1   -1   -1    1
[14,]   -1    1   -1   -1   -1
[15,]    1   -1   -1   -1   -1
[16,]   -1   -1   -1   -1    1
>   FFdes(5,TRUE)
           [,1]      [,2]      [,3]      [,4]      [,5]
 [1,]  1.000000  1.000000  1.000000  1.000000  1.000000
 [2,] -1.000000  1.000000  1.000000  1.000000 -1.000000
 [3,]  1.000000 -1.000000  1.000000  1.000000 -1.000000
 [4,] -1.000000 -1.000000  1.000000  1.000000  1.000000
 [5,]  1.000000  1.000000 -1.000000  1.000000 -1.000000
 [6,] -1.000000  1.000000 -1.000000  1.000000  1.000000
 [7,]  1.000000 -1.000000 -1.000000  1.000000  1.000000
 [8,] -1.000000 -1.000000 -1.000000  1.000000 -1.000000
 [9,]  1.000000  1.000000  1.000000 -1.000000 -1.000000
[10,] -1.000000  1.000000  1.000000 -1.000000  1.000000
[11,]  1.000000 -1.000000  1.000000 -1.000000  1.000000
[12,] -1.000000 -1.000000  1.000000 -1.000000 -1.000000
[13,]  1.000000  1.000000 -1.000000 -1.000000  1.000000
[14,] -1.000000  1.000000 -1.000000 -1.000000 -1.000000
[15,]  1.000000 -1.000000 -1.000000 -1.000000 -1.000000
[16,] -1.000000 -1.000000 -1.000000 -1.000000  1.000000
[17,]  2.236068  0.000000  0.000000  0.000000  0.000000
[18,]  0.000000  2.236068  0.000000  0.000000  0.000000
[19,]  0.000000  0.000000  2.236068  0.000000  0.000000
[20,]  0.000000  0.000000  0.000000  2.236068  0.000000
[21,]  0.000000  0.000000  0.000000  0.000000  2.236068
[22,] -2.236068  0.000000  0.000000  0.000000  0.000000
[23,]  0.000000 -2.236068  0.000000  0.000000  0.000000
[24,]  0.000000  0.000000 -2.236068  0.000000  0.000000
[25,]  0.000000  0.000000  0.000000 -2.236068  0.000000
[26,]  0.000000  0.000000  0.000000  0.000000 -2.236068
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix")
> ### * Predict.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix
> ### Title: Prediction methods for smooth terms in a GAM
> ### Aliases: Predict.matrix Predict.matrix2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> # See smooth.construct examples
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix.cr.smooth")
> ### * Predict.matrix.cr.smooth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix.cr.smooth
> ### Title: Predict matrix method functions
> ### Aliases: Predict.matrix.cr.smooth Predict.matrix.cs.smooth
> ###   Predict.matrix.cyclic.smooth Predict.matrix.pspline.smooth
> ###   Predict.matrix.tensor.smooth Predict.matrix.tprs.smooth
> ###   Predict.matrix.ts.smooth Predict.matrix.t2.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see smooth.construct
>  
> 
> 
> 
> 
> cleanEx()
> nameEx("Predict.matrix.soap.film")
> ### * Predict.matrix.soap.film
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Predict.matrix.soap.film
> ### Title: Prediction matrix for soap film smooth
> ### Aliases: Predict.matrix.soap.film Predict.matrix.sw Predict.matrix.sf
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## This is a lower level example. The basis and 
> ## penalties are obtained explicitly 
> ## and `magic' is used as the fitting routine...
> 
> require(mgcv)
> set.seed(66)
> 
> ## create a boundary...
> fsb <- list(fs.boundary())
> 
> ## create some internal knots...
> knots <- data.frame(x=rep(seq(-.5,3,by=.5),4),
+                     y=rep(c(-.6,-.3,.3,.6),rep(8,4)))
> 
> ## Simulate some fitting data, inside boundary...
> n<-1000
> x <- runif(n)*5-1;y<-runif(n)*2-1
> z <- fs.test(x,y,b=1)
> ind <- inSide(fsb,x,y) ## remove outsiders
> z <- z[ind];x <- x[ind]; y <- y[ind] 
> n <- length(z)
> z <- z + rnorm(n)*.3 ## add noise
> 
> ## plot boundary with knot and data locations
> plot(fsb[[1]]$x,fsb[[1]]$y,type="l");points(knots$x,knots$y,pch=20,col=2)
> points(x,y,pch=".",col=3);
> 
> ## set up the basis and penalties...
> sob <- smooth.construct2(s(x,y,bs="so",k=40,xt=list(bnd=fsb,nmax=100)),
+               data=data.frame(x=x,y=y),knots=knots)
> ## ... model matrix is element `X' of sob, penalties matrices 
> ## are in list element `S'.
> 
> ## fit using `magic'
> um <- magic(z,sob$X,sp=c(-1,-1),sob$S,off=c(1,1))
> beta <- um$b
> 
> ## produce plots...
> par(mfrow=c(2,2),mar=c(4,4,1,1))
> m<-100;n<-50 
> xm <- seq(-1,3.5,length=m);yn<-seq(-1,1,length=n)
> xx <- rep(xm,n);yy<-rep(yn,rep(m,n))
> 
> ## plot truth...
> tru <- matrix(fs.test(xx,yy),m,n) ## truth
> image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)
> 
> ## Plot soap, by first predicting on a fine grid...
> 
> ## First get prediction matrix...
> X <- Predict.matrix2(sob,data=list(x=xx,y=yy))
> 
> ## Now the predictions...
> fv <- X%*%beta
> 
> ## Plot the estimated function...
> image(xm,yn,matrix(fv,m,n),col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> points(x,y,pch=".")
> contour(xm,yn,matrix(fv,m,n),levels=seq(-5,5,by=.25),add=TRUE)
> 
> ## Plot TPRS...
> b <- gam(z~s(x,y,k=100))
> fv.gam <- predict(b,newdata=data.frame(x=xx,y=yy))
> names(sob$sd$bnd[[1]]) <- c("xx","yy","d")
> ind <- inSide(sob$sd$bnd,xx,yy)
> fv.gam[!ind]<-NA
> image(xm,yn,matrix(fv.gam,m,n),col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb[[1]]$x,fsb[[1]]$y,lwd=3)
> points(x,y,pch=".")
> contour(xm,yn,matrix(fv.gam,m,n),levels=seq(-5,5,by=.25),add=TRUE)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("Rrank")
> ### * Rrank
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Rrank
> ### Title: Find rank of upper triangular matrix
> ### Aliases: Rrank
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   set.seed(0)
>   n <- 10;p <- 5
>   X <- matrix(runif(n*(p-1)),n,p)
>   qrx <- qr(X,LAPACK=TRUE)
>   Rrank(qr.R(qrx))
[1] 4
> 
> 
> 
> cleanEx()
> nameEx("Tweedie")
> ### * Tweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tweedie
> ### Title: GAM Tweedie families
> ### Aliases: Tweedie tw
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3)
> n<-400
> ## Simulate data...
> dat <- gamSim(1,n=n,dist="poisson",scale=.2)
Gu & Wahba 4 term additive model
> dat$y <- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response
> 
> ## Fit a fixed p Tweedie, with wrong link ...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
+          data=dat)
> plot(b,pages=1)
> print(b)

Family: Tweedie(1.25) 
Link function: mu^0.1 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.71 2.66 7.36 1.00  total = 14.73 

GCV score: 0.5404011     
> 
> ## Same by approximate REML...
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=Tweedie(1.25,power(.1)),
+           data=dat,method="REML")
> plot(b1,pages=1)
> print(b1)

Family: Tweedie(1.25) 
Link function: mu^0.1 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.37 3.12 7.97 1.00  total = 16.46 

REML score: 860.0066     
> 
> ## estimate p as part of fitting
> 
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=tw(),
+           data=dat,method="REML")
> plot(b2,pages=1)
> print(b2)

Family: Tweedie(p=1.401) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.32 3.04 7.89 1.00  total = 16.26 

REML score: 846.1687     
> 
> rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("XWXd")
> ### * XWXd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: XWXd
> ### Title: Internal functions for discretized model matrix handling
> ### Aliases: XWXd XWyd Xbd diagXVXd
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   library(mgcv);library(Matrix)
>   ## simulate some data creating a marginal matrix sequence...
>   set.seed(0);n <- 4000
>   dat <- gamSim(1,n=n,dist="normal",scale=2)
Gu & Wahba 4 term additive model
>   dat$x4 <- runif(n)
>   dat$y <- dat$y + 3*exp(dat$x4*15-5)/(1+exp(dat$x4*15-5))
>   dat$fac <- factor(sample(1:20,n,replace=TRUE))
>   G <- gam(y ~ te(x0,x2,k=5,bs="bs",m=1)+s(x1)+s(x4)+s(x3,fac,bs="fs"),
+            fit=FALSE,data=dat,discrete=TRUE)
>   p <- ncol(G$X)
>   ## create a sparse version...
>   Xs <- list(); r <- G$kd*0; off <- list()
>   for (i in 1:length(G$Xd)) Xs[[i]] <- as(G$Xd[[i]],"dgCMatrix")
>   for (j in 1:nrow(G$ks)) { ## create the reverse indices...
+     nr <- nrow(Xs[[j]]) ## make sure we always tab to final stored row 
+     for (i in G$ks[j,1]:(G$ks[j,2]-1)) {
+       r[,i] <- (1:length(G$kd[,i]))[order(G$kd[,i])]
+       off[[i]] <- cumsum(c(1,tabulate(G$kd[,i],nbins=nr)))-1
+     }
+   }
>   attr(Xs,"off") <- off;attr(Xs,"r") <- r 
> 
>   par(mfrow=c(2,3))
> 
>   beta <- runif(p)
>   Xb0 <- Xbd(G$Xd,beta,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   Xb1 <- Xbd(Xs,beta,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(Xb0-Xb1);plot(Xb0,Xb1,pch=".")
[1] -1.776357e-15  2.220446e-15
> 
>   bb <- cbind(beta,beta+runif(p)*.3)
>   Xb0 <- Xbd(G$Xd,bb,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   Xb1 <- Xbd(Xs,bb,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(Xb0-Xb1);plot(Xb0,Xb1,pch=".")
[1] -2.664535e-15  3.552714e-15
>   
>   w <- runif(n)
>   XWy0 <- XWyd(G$Xd,w,y=dat$y,G$kd,G$ks,G$ts,G$dt,G$v,G$qc) 
>   XWy1 <- XWyd(Xs,w,y=dat$y,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(XWy1-XWy0);plot(XWy1,XWy0,pch=".")
[1] -2.273737e-12  2.501110e-12
> 
>   yy <- cbind(dat$y,dat$y+runif(n)-.5)
>   XWy0 <- XWyd(G$Xd,w,y=yy,G$kd,G$ks,G$ts,G$dt,G$v,G$qc) 
>   XWy1 <- XWyd(Xs,w,y=yy,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(XWy1-XWy0);plot(XWy1,XWy0,pch=".")
[1] -2.728484e-12  2.955858e-12
> 
>   A <- XWXd(G$Xd,w,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   B <- XWXd(Xs,w,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(A-B);plot(A,B,pch=".")
[1] -6.821210e-13  2.273737e-13
> 
>   V <- crossprod(matrix(runif(p*p),p,p))
>   ii <- c(20:30,100:200)
>   jj <- c(50:90,150:160)
>   V[ii,jj] <- 0;V[jj,ii] <- 0
>   d1 <- diagXVXd(G$Xd,V,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   Vs <- as(V,"dgCMatrix")
>   d2 <- diagXVXd(Xs,Vs,G$kd,G$ks,G$ts,G$dt,G$v,G$qc)
>   range(d1-d2);plot(d1,d2,pch=".")
[1] -3.637979e-12  3.637979e-12
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:Matrix’

> nameEx("anova.gam")
> ### * anova.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: anova.gam
> ### Title: Approximate hypothesis tests related to GAM fits
> ### Aliases: anova.gam print.anova.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> dat <- gamSim(5,n=200,scale=2)
Additive model + factor
> 
> b<-gam(y ~ x0 + s(x1) + s(x2) + s(x3),data=dat)
> anova(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ x0 + s(x1) + s(x2) + s(x3)

Parametric Terms:
   df     F p-value
x0  3 77.42  <2e-16

Approximate significance of smooth terms:
        edf Ref.df      F p-value
s(x1) 1.729  2.158 45.309  <2e-16
s(x2) 7.069  8.120 49.230  <2e-16
s(x3) 1.000  1.000  0.056   0.812
> b1<-gam(y ~ x0 + s(x1) + s(x2),data=dat)
> anova(b,b1,test="F")
Analysis of Deviance Table

Model 1: y ~ x0 + s(x1) + s(x2) + s(x3)
Model 2: y ~ x0 + s(x1) + s(x2)
  Resid. Df Resid. Dev       Df  Deviance      F Pr(>F)
1    184.72     793.73                                 
2    185.72     793.78 -0.99341 -0.051961 0.0123 0.9107
> 
> 
> 
> cleanEx()
> nameEx("bam")
> ### * bam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bam
> ### Title: Generalized additive models for very large datasets
> ### Aliases: bam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## See help("mgcv-parallel") for using bam in parallel
> 
> ## Sample sizes are small for fast run times.
> 
> set.seed(3)
> dat <- gamSim(1,n=25000,dist="normal",scale=20)
Gu & Wahba 4 term additive model
> bs <- "cr";k <- 12
> b <- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs),data=dat)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs) + s(x1, bs = bs) + s(x2, bs = bs, k = k) + 
    s(x3, bs = bs)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9459     0.1276   62.29   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 3.113  3.863  6.664   4e-05 ***
s(x1) 2.826  3.509 63.026  <2e-16 ***
s(x2) 8.602  9.889 52.098  <2e-16 ***
s(x3) 1.002  1.004  3.829  0.0503 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.0295   Deviance explained = 3.01%
fREML = 1.1057e+05  Scale est. = 406.15    n = 25000
> plot(b,pages=1,rug=FALSE)  ## plot smooths, but not rug
> plot(b,pages=1,rug=FALSE,seWithMean=TRUE) ## `with intercept' CIs
> 
> 
> ## A Poisson example...
> 
> k <- 15
> dat <- gamSim(1,n=21000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> 
> system.time(b1 <- bam(y ~ s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
+             data=dat,family=poisson()))
   user  system elapsed 
  3.116   1.815   1.999 
> b1

Family: poisson 
Link function: log 

Formula:
y ~ s(x0, bs = bs) + s(x1, bs = bs) + s(x2, bs = bs, k = k)

Estimated degrees of freedom:
 4.77  4.36 12.41  total = 22.54 

fREML score: 29782.35     
> 
> ## Similar using faster discrete method...
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("bam.update")
> ### * bam.update
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bam.update
> ### Title: Update a strictly additive bam model for new data.
> ### Aliases: bam.update
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## following is not *very* large, for obvious reasons...
> set.seed(8)
> n <- 5000
> dat <- gamSim(1,n=n,dist="normal",scale=5)
Gu & Wahba 4 term additive model
> dat[c(50,13,3000,3005,3100),]<- NA
> dat1 <- dat[(n-999):n,]
> dat0 <- dat[1:(n-1000),]
> bs <- "ps";k <- 20
> method <- "GCV.Cp"
> b <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat0,method=method)
> 
> b1 <- bam.update(b,dat1)
> 
> b2 <- bam.update(bam.update(b,dat1[1:500,]),dat1[501:1000,])
>  
> b3 <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat,method=method)
> b1;b2;b3

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.29 14.54  total = 35.16 

GCV score: 25.05259     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.29 14.54  total = 35.15 

GCV score: 25.05259     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Estimated degrees of freedom:
 3.29  3.04 13.26 14.54  total = 35.13 

GCV score: 25.05261     
> 
> ## example with AR1 errors...
> 
> e <- rnorm(n)
> for (i in 2:n) e[i] <- e[i-1]*.7 + e[i]
> dat$y <- dat$f + e*3
> dat[c(50,13,3000,3005,3100),]<- NA
> dat1 <- dat[(n-999):n,]
> dat0 <- dat[1:(n-1000),]
> 
> b <- bam(y ~ s(x0,bs=bs,k=k)+s(x1,bs=bs,k=k)+s(x2,bs=bs,k=k)+
+            s(x3,bs=bs,k=k),data=dat0,rho=0.7)
> 
> b1 <- bam.update(b,dat1)
> 
> 
> summary(b1);summary(b2);summary(b3)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9912     0.1419   56.31   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F p-value    
s(x0)  5.119  6.273  53.768  <2e-16 ***
s(x1)  4.519  5.552 486.540  <2e-16 ***
s(x2) 13.580 15.412 402.155  <2e-16 ***
s(x3)  1.000  1.000   1.662   0.197    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.391   Deviance explained = 39.4%
fREML =  10946  Scale est. = 17.771    n = 4995

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.98248    0.07062     113   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F p-value    
s(x0)  3.292  4.074  17.378  <2e-16 ***
s(x1)  3.037  3.764 157.017  <2e-16 ***
s(x2) 13.286 15.148  94.751  <2e-16 ***
s(x3) 14.540 16.204   1.337   0.146    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.298   Deviance explained = 30.3%
GCV = 25.053  Scale est. = 24.876    n = 4995

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = bs, k = k) + s(x1, bs = bs, k = k) + s(x2, bs = bs, 
    k = k) + s(x3, bs = bs, k = k)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.95364    0.07057   112.7   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F p-value    
s(x0)  3.292  4.074  17.378  <2e-16 ***
s(x1)  3.037  3.764 157.010  <2e-16 ***
s(x2) 13.259 15.124  94.902  <2e-16 ***
s(x3) 14.539 16.203   1.337   0.146    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.298   Deviance explained = 30.3%
GCV = 25.053  Scale est. = 24.876    n = 4995
> 
> 
> 
> 
> cleanEx()
> nameEx("bandchol")
> ### * bandchol
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bandchol
> ### Title: Choleski decomposition of a band diagonal matrix
> ### Aliases: bandchol
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## simulate a banded diagonal matrix
> n <- 7;set.seed(8)
> A <- matrix(0,n,n)
> sdiag(A) <- runif(n);sdiag(A,1) <- runif(n-1)
> sdiag(A,2) <- runif(n-2)
> A <- crossprod(A) 
> 
> ## full matrix form...
> bandchol(A)
          [,1]      [,2]      [,3]      [,4]      [,5]        [,6]      [,7]
[1,] 0.4662952 0.9322698 0.5449621 0.0000000 0.0000000 0.000000000 0.0000000
[2,] 0.0000000 0.2078233 0.7691470 0.1382243 0.0000000 0.000000000 0.0000000
[3,] 0.0000000 0.0000000 0.7996580 0.6444911 0.9278123 0.000000000 0.0000000
[4,] 0.0000000 0.0000000 0.0000000 0.6518713 0.4570449 0.001301721 0.0000000
[5,] 0.0000000 0.0000000 0.0000000 0.0000000 0.3215092 0.089301011 0.2644589
[6,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.718927504 0.4323914
[7,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000000 0.2908734
> chol(A) ## for comparison
          [,1]      [,2]      [,3]      [,4]      [,5]        [,6]      [,7]
[1,] 0.4662952 0.9322698 0.5449621 0.0000000 0.0000000 0.000000000 0.0000000
[2,] 0.0000000 0.2078233 0.7691470 0.1382243 0.0000000 0.000000000 0.0000000
[3,] 0.0000000 0.0000000 0.7996580 0.6444911 0.9278123 0.000000000 0.0000000
[4,] 0.0000000 0.0000000 0.0000000 0.6518713 0.4570449 0.001301721 0.0000000
[5,] 0.0000000 0.0000000 0.0000000 0.0000000 0.3215092 0.089301011 0.2644589
[6,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.718927504 0.4323914
[7,] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.000000000 0.2908734
> 
> ## compact storage form...
> B <- matrix(0,3,n)
> B[1,] <- sdiag(A);B[2,1:(n-1)] <- sdiag(A,1)
> B[3,1:(n-2)] <- sdiag(A,2)
> bandchol(B)
          [,1]      [,2]      [,3]        [,4]       [,5]      [,6]      [,7]
[1,] 0.4662952 0.2078233 0.7996580 0.651871318 0.32150918 0.7189275 0.2908734
[2,] 0.9322698 0.7691470 0.6444911 0.457044889 0.08930101 0.4323914 0.0000000
[3,] 0.5449621 0.1382243 0.9278123 0.001301721 0.26445886 0.0000000 0.0000000
> 
> 
> 
> 
> cleanEx()
> nameEx("cSplineDes")
> ### * cSplineDes
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cSplineDes
> ### Title: Evaluate cyclic B spline basis
> ### Aliases: cSplineDes
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>  require(mgcv)
>  ## create some x's and knots...
>  n <- 200
>  x <- 0:(n-1)/(n-1);k<- 0:5/5
>  X <- cSplineDes(x,k) ## cyclic spline design matrix
>  ## plot evaluated basis functions...
>  plot(x,X[,1],type="l"); for (i in 2:5) lines(x,X[,i],col=i)
>  ## check that the ends match up....
>  ee <- X[1,]-X[n,];ee 
[1] 0 0 0 0 0
>  tol <- .Machine$double.eps^.75
>  if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
+    stop("cyclic spline ends don't match!")
>  
>  ## similar with uneven data spacing...
>  x <- sort(runif(n)) + 1 ## sorting just makes end checking easy
>  k <- seq(min(x),max(x),length=8) ## create knots
>  X <- cSplineDes(x,k) ## get cyclic spline model matrix  
>  plot(x,X[,1],type="l"); for (i in 2:ncol(X)) lines(x,X[,i],col=i)
>  ee <- X[1,]-X[n,];ee ## do ends match??
[1] 0 0 0 0 0 0 0
>  tol <- .Machine$double.eps^.75
>  if (all.equal(ee,ee*0,tolerance=tol)!=TRUE) 
+    stop("cyclic spline ends don't match!")
> 
> 
> 
> cleanEx()
> nameEx("chol.down")
> ### * chol.down
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: choldrop
> ### Title: Deletion and rank one Cholesky factor update
> ### Aliases: choldrop cholup
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   set.seed(0)
>   n <- 6
>   A <- crossprod(matrix(runif(n*n),n,n))
>   R0 <- chol(A)
>   k <- 3
>   Rd <- choldrop(R0,k)
>   range(Rd-chol(A[-k,-k]))
[1] -1.110223e-16  3.608225e-16
>   Rd;chol(A[-k,-k])
         [,1]     [,2]      [,3]      [,4]      [,5]
[1,] 1.485502 1.185019 1.4400506 0.5772972 0.8583071
[2,] 0.000000 1.083479 0.7316678 0.1360929 0.5839283
[3,] 0.000000 0.000000 0.6833734 0.3742524 0.5321862
[4,] 0.000000 0.000000 0.0000000 0.8044618 0.5299088
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.1698106
         [,1]     [,2]      [,3]      [,4]      [,5]
[1,] 1.485502 1.185019 1.4400506 0.5772972 0.8583071
[2,] 0.000000 1.083479 0.7316678 0.1360929 0.5839283
[3,] 0.000000 0.000000 0.6833734 0.3742524 0.5321862
[4,] 0.000000 0.000000 0.0000000 0.8044618 0.5299088
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.1698106
>   
>   ## same but using lower triangular factor A = LL'
>   L <- t(R0)
>   Ld <- choldrop(L,k)
>   range(Ld-t(chol(A[-k,-k])))
[1] -1.110223e-16  3.885781e-16
>   Ld;t(chol(A[-k,-k]))
          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 1.4855016 0.0000000 0.0000000 0.0000000 0.0000000
[2,] 1.1850192 1.0834787 0.0000000 0.0000000 0.0000000
[3,] 1.4400506 0.7316678 0.6833734 0.0000000 0.0000000
[4,] 0.5772972 0.1360929 0.3742524 0.8044618 0.0000000
[5,] 0.8583071 0.5839283 0.5321862 0.5299088 0.1698106
          [,1]      [,2]      [,3]      [,4]      [,5]
[1,] 1.4855016 0.0000000 0.0000000 0.0000000 0.0000000
[2,] 1.1850192 1.0834787 0.0000000 0.0000000 0.0000000
[3,] 1.4400506 0.7316678 0.6833734 0.0000000 0.0000000
[4,] 0.5772972 0.1360929 0.3742524 0.8044618 0.0000000
[5,] 0.8583071 0.5839283 0.5321862 0.5299088 0.1698106
> 
>   ## Rank one update example
>   u <- runif(n)
>   R <- cholup(R0,u,TRUE)
>   Ru <- chol(A+u %*% t(u)) ## direct for comparison
>   R;Ru
         [,1]     [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 1.628976 1.406571 0.9782653 1.6101981 0.6952216 1.1195941
[2,] 0.000000 1.109311 0.3912867 0.7294399 0.1625639 0.6553831
[3,] 0.000000 0.000000 0.9526744 0.2573284 0.5032834 0.4459729
[4,] 0.000000 0.000000 0.0000000 0.6393759 0.1826402 0.3526321
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.7153685 0.5092498
[6,] 0.000000 0.000000 0.0000000 0.0000000 0.0000000 0.2808608
         [,1]     [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 1.628976 1.406571 0.9782653 1.6101981 0.6952216 1.1195941
[2,] 0.000000 1.109311 0.3912867 0.7294399 0.1625639 0.6553831
[3,] 0.000000 0.000000 0.9526744 0.2573284 0.5032834 0.4459729
[4,] 0.000000 0.000000 0.0000000 0.6393759 0.1826402 0.3526321
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.7153685 0.5092498
[6,] 0.000000 0.000000 0.0000000 0.0000000 0.0000000 0.2808608
>   range(R-Ru)
[1] -5.551115e-16  4.440892e-16
> 
>   ## Downdate - just going back from R to R0
>   Rd <-  cholup(R,u,FALSE)
>   R0;Rd
         [,1]     [,2]      [,3]      [,4]      [,5]       [,6]
[1,] 1.485502 1.185019 1.0241752 1.4400506 0.5772972 0.85830710
[2,] 0.000000 1.083479 0.4713134 0.7316678 0.1360929 0.58392831
[3,] 0.000000 0.000000 0.8573061 0.2425532 0.6105113 0.62251233
[4,] 0.000000 0.000000 0.0000000 0.6388796 0.1685336 0.33291018
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.6213663 0.30466089
[6,] 0.000000 0.000000 0.0000000 0.0000000 0.0000000 0.04113509
         [,1]     [,2]      [,3]      [,4]      [,5]       [,6]
[1,] 1.485502 1.185019 1.0241752 1.4400506 0.5772972 0.85830710
[2,] 0.000000 1.083479 0.4713134 0.7316678 0.1360929 0.58392831
[3,] 0.000000 0.000000 0.8573061 0.2425532 0.6105113 0.62251233
[4,] 0.000000 0.000000 0.0000000 0.6388796 0.1685336 0.33291018
[5,] 0.000000 0.000000 0.0000000 0.0000000 0.6213663 0.30466089
[6,] 0.000000 0.000000 0.0000000 0.0000000 0.0000000 0.04113509
>   range(R-Ru)
[1] -5.551115e-16  4.440892e-16
>   
> 
> 
> 
> cleanEx()
> nameEx("choose.k")
> ### * choose.k
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: choose.k
> ### Title: Basis dimension choice for smooths
> ### Aliases: choose.k
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Simulate some data ....
> library(mgcv)
> set.seed(1) 
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> 
> ## fit a GAM with quite low `k'
> b<-gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),data=dat)
> plot(b,pages=1,residuals=TRUE) ## hint of a problem in s(x2)
> 
> ## the following suggests a problem with s(x2)
> gam.check(b)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 17 iterations.
The RMS GCV score gradient at convergence was 2.675921e-07 .
The Hessian was positive definite.
Model rank =  21 / 21 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value  
s(x0) 5.00 2.63    0.98   0.260  
s(x1) 5.00 2.87    0.99   0.405  
s(x2) 5.00 4.96    0.92   0.065 .
s(x3) 5.00 1.00    1.02   0.660  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## Another approach (see below for more obvious method)....
> ## check for residual pattern, removeable by increasing `k'
> ## typically `k', below, chould be substantially larger than 
> ## the original, `k' but certainly less than n/2.
> ## Note use of cheap "cs" shrinkage smoothers, and gamma=1.4
> ## to reduce chance of overfitting...
> rsd <- residuals(b)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x2, k = 40, bs = "cs")

Estimated degrees of freedom:
9.01  total = 10.01 

GCV score: 4.494652     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 4.448352     
> 
> ## refit...
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=20)+s(x3,k=6),data=dat)
> gam.check(b) ## better

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 9 iterations.
The RMS GCV score gradient at convergence was 2.918624e-07 .
The Hessian was positive definite.
Model rank =  35 / 35 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

         k'   edf k-index p-value
s(x0)  5.00  2.70    0.98    0.37
s(x1)  5.00  2.62    0.98    0.33
s(x2) 19.00 11.22    0.98    0.37
s(x3)  5.00  1.00    1.02    0.70
> 
> ## similar example with multi-dimensional smooth
> b1 <- gam(y~s(x0)+s(x1,x2,k=15)+s(x3),data=dat)
> rsd <- residuals(b1)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 5.257725     
> gam(rsd~s(x1,x2,k=100,bs="ts"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, x2, k = 100, bs = "ts")

Estimated degrees of freedom:
30.6  total = 31.62 

GCV score: 5.066268     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 5.257725     
> 
> gam.check(b1) ## shows same problem

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 15 iterations.
The RMS GCV score gradient at convergence was 2.956997e-07 .
The Hessian was positive definite.
Model rank =  33 / 33 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

            k'   edf k-index p-value    
s(x0)     9.00  2.46    0.97    0.18    
s(x1,x2) 14.00 13.74    0.83  <2e-16 ***
s(x3)     9.00  1.00    1.00    0.47    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## and a `te' example
> b2 <- gam(y~s(x0)+te(x1,x2,k=4)+s(x3),data=dat)
> rsd <- residuals(b2)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 6.181669     
> gam(rsd~te(x1,x2,k=10,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ te(x1, x2, k = 10, bs = "cs")

Estimated degrees of freedom:
18.4  total = 19.43 

GCV score: 4.590678     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 6.181669     
> 
> gam.check(b2) ## shows same problem

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 15 iterations.
The RMS GCV score gradient at convergence was 2.875373e-07 .
The Hessian was positive definite.
Model rank =  34 / 34 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

             k'   edf k-index p-value    
s(x0)      9.00  2.60    1.05    0.83    
te(x1,x2) 15.00  9.94    0.68  <2e-16 ***
s(x3)      9.00  1.00    0.96    0.18    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> ## same approach works with other families in the original model
> dat <- gamSim(1,n=400,scale=.25,dist="poisson")
Gu & Wahba 4 term additive model
> bp<-gam(y~s(x0,k=5)+s(x1,k=5)+s(x2,k=5)+s(x3,k=5),
+         family=poisson,data=dat,method="ML")
> 
> gam.check(bp)

Method: ML   Optimizer: outer newton
full convergence after 9 iterations.
Gradient range [-0.0001745332,1.193507e-06]
(score 1063.418 & scale 1).
Hessian positive definite, eigenvalue range [0.0001744834,1.4833].
Model rank =  17 / 17 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value    
s(x0) 4.00 3.37    1.00    0.57    
s(x1) 4.00 3.01    1.02    0.63    
s(x2) 4.00 3.99    0.73  <2e-16 ***
s(x3) 4.00 1.00    0.98    0.34    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> 
> rsd <- residuals(bp)
> gam(rsd~s(x0,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x0, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 1.504487     
> gam(rsd~s(x1,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x1, k = 40, bs = "cs")

Estimated degrees of freedom:
0  total = 1 

GCV score: 1.504487     
> gam(rsd~s(x2,k=40,bs="cs"),gamma=1.4,data=dat) ## `k' too low

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x2, k = 40, bs = "cs")

Estimated degrees of freedom:
10.9  total = 11.85 

GCV score: 1.141074     
> gam(rsd~s(x3,k=40,bs="cs"),gamma=1.4,data=dat) ## fine

Family: gaussian 
Link function: identity 

Formula:
rsd ~ s(x3, k = 40, bs = "cs")

Estimated degrees of freedom:
3.79  total = 4.79 

GCV score: 1.515055     
> 
> rm(dat) 
> 
> ## More obvious, but more expensive tactic... Just increase 
> ## suspicious k until fit is stable.
> 
> set.seed(0) 
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> ## fit a GAM with quite low `k'
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=6)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 6) + s(x3, k = 6)

Estimated degrees of freedom:
2.90 2.71 4.94 1.00  total = 12.55 

REML score: 892.3014     
> ## edf for 3rd smooth is highest as proportion of k -- increase k
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=12)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 12) + s(x3, k = 6)

Estimated degrees of freedom:
3.10 2.67 9.07 1.00  total = 16.84 

REML score: 887.9842     
> ## edf substantially up, -ve REML substantially down
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=24)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 24) + s(x3, k = 6)

Estimated degrees of freedom:
 3.12  2.67 11.32  1.00  total = 19.11 

REML score: 887.0976     
> ## slight edf increase and -ve REML change
> b <- gam(y~s(x0,k=6)+s(x1,k=6)+s(x2,k=40)+s(x3,k=6),
+          data=dat,method="REML")
> b 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = 6) + s(x1, k = 6) + s(x2, k = 40) + s(x3, k = 6)

Estimated degrees of freedom:
 3.12  2.67 11.54  1.00  total = 19.33 

REML score: 887.1258     
> ## defintely stabilized (but really k around 20 would have been fine)
> 
> 
> 
> 
> cleanEx()
> nameEx("columb")
> ### * columb
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: columb
> ### Title: Reduced version of Columbus OH crime data
> ### Aliases: columb columb.polys
> 
> ### ** Examples
> 
> ## see ?mrf help files
> 
> 
> 
> cleanEx()
> nameEx("concurvity")
> ### * concurvity
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: concurvity
> ### Title: GAM concurvity measures
> ### Aliases: concurvity
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate data with concurvity...
> set.seed(8);n<- 200
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
+             (10 * x)^3 * (1 - x)^10
> t <- sort(runif(n)) ## first covariate
> ## make covariate x a smooth function of t + noise...
> x <- f2(t) + rnorm(n)*3
> ## simulate response dependent on t and x...
> y <- sin(4*pi*t) + exp(x/20) + rnorm(n)*.3
> 
> ## fit model...
> b <- gam(y ~ s(t,k=15) + s(x,k=15),method="REML")
> 
> ## assess concurvity between each term and `rest of model'...
> concurvity(b)
                 para       s(t)      s(x)
worst    1.096893e-24 0.60269087 0.6026909
observed 1.096893e-24 0.09576829 0.5728602
estimate 1.096893e-24 0.24513981 0.4659564
> 
> ## ... and now look at pairwise concurvity between terms...
> concurvity(b,full=FALSE)
$worst
             para         s(t)         s(x)
para 1.000000e+00 4.559126e-25 4.187411e-25
s(t) 4.577644e-25 1.000000e+00 6.026909e-01
s(x) 4.192086e-25 6.026909e-01 1.000000e+00

$observed
             para         s(t)         s(x)
para 1.000000e+00 5.596870e-28 7.976323e-33
s(t) 4.577644e-25 1.000000e+00 5.728602e-01
s(x) 4.192086e-25 9.576829e-02 1.000000e+00

$estimate
             para         s(t)         s(x)
para 1.000000e+00 3.407139e-28 1.244042e-27
s(t) 4.577644e-25 1.000000e+00 4.659564e-01
s(x) 4.192086e-25 2.451398e-01 1.000000e+00

> 
> 
> 
> 
> cleanEx()
> nameEx("coxph")
> ### * coxph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cox.ph
> ### Title: Additive Cox Proportional Hazard Model
> ### Aliases: cox.ph
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> library(survival) ## for data
> col1 <- colon[colon$etype==1,] ## concentrate on single event
> col1$differ <- as.factor(col1$differ)
> col1$sex <- as.factor(col1$sex)
> 
> b <- gam(time~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct+adhere,
+          family=cox.ph(),data=col1,weights=status)
> 
> summary(b) 

Family: Cox PH 
Link function: identity 

Formula:
time ~ s(age, by = sex) + sex + s(nodes) + perfor + rx + obstruct + 
    adhere

Parametric coefficients:
          Estimate Std. Error z value Pr(>|z|)    
sex1      -0.11393    0.09464  -1.204   0.2287    
perfor     0.19152    0.25698   0.745   0.4561    
rxLev     -0.03564    0.10857  -0.328   0.7427    
rxLev+5FU -0.51258    0.12088  -4.240 2.23e-05 ***
obstruct   0.22242    0.11716   1.898   0.0576 .  
adhere     0.28372    0.12736   2.228   0.0259 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
              edf Ref.df Chi.sq p-value    
s(age):sex0 1.003  1.006  4.547  0.0333 *  
s(age):sex1 1.383  1.677  0.878  0.4424    
s(nodes)    3.134  3.895 93.725  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 5.86%
-REML = 2906.6  Scale est. = 1         n = 911
> 
> plot(b,pages=1,all.terms=TRUE) ## plot effects
> 
> plot(b$linear.predictors,residuals(b))
> 
> ## plot survival function for patient j...
> 
> np <- 300;j <- 6
> newd <- data.frame(time=seq(0,3000,length=np))
> dname <- names(col1)
> for (n in dname) newd[[n]] <- rep(col1[[n]][j],np)
> newd$time <- seq(0,3000,length=np)
> fv <- predict(b,newdata=newd,type="response",se=TRUE)
> plot(newd$time,fv$fit,type="l",ylim=c(0,1),xlab="time",ylab="survival")
> lines(newd$time,fv$fit+2*fv$se.fit,col=2)
> lines(newd$time,fv$fit-2*fv$se.fit,col=2)
> 
> ## crude plot of baseline survival...
> 
> plot(b$family$data$tr,exp(-b$family$data$h),type="l",ylim=c(0,1),
+      xlab="time",ylab="survival")
> lines(b$family$data$tr,exp(-b$family$data$h + 2*b$family$data$q^.5),col=2)
> lines(b$family$data$tr,exp(-b$family$data$h - 2*b$family$data$q^.5),col=2)
> lines(b$family$data$tr,exp(-b$family$data$km),lty=2) ## Kaplan Meier
> 
> ## Checking the proportional hazards assumption via scaled score plots as
> ## in Klein and Moeschberger Section 11.4 p374-376... 
> 
> ph.resid <- function(b,stratum=1) {
+ ## convenience function to plot scaled score residuals against time,
+ ## by term. Reference lines at 5% exceedance prob for Brownian bridge
+ ## (see KS test statistic distribution).
+   rs <- residuals(b,"score");term <- attr(rs,"term")
+   if (is.matrix(b$y)) {
+     ii <- b$y[,2] == stratum;b$y <- b$y[ii,1];rs <- rs[ii,]
+   }
+   oy <- order(b$y)
+   for (i in 1:length(term)) {
+     ii <- term[[i]]; m <- length(ii)
+     plot(b$y[oy],rs[oy,ii[1]],ylim=c(-3,3),type="l",ylab="score residuals",
+          xlab="time",main=names(term)[i])
+     if (m>1) for (k in 2:m) lines(b$y[oy],rs[oy,ii[k]],col=k);
+     abline(-1.3581,0,lty=2);abline(1.3581,0,lty=2) 
+   }  
+ }
> par(mfrow=c(2,2))
> ph.resid(b)
> 
> ## stratification example, with 2 randomly allocated strata
> ## so that results should be similar to previous....
> col1$strata <- sample(1:2,nrow(col1),replace=TRUE) 
> bs <- gam(cbind(time,strata)~s(age,by=sex)+sex+s(nodes)+perfor+rx+obstruct
+           +adhere,family=cox.ph(),data=col1,weights=status)
> plot(bs,pages=1,all.terms=TRUE) ## plot effects
> 
> ## baseline survival plots by strata...
> 
> for (i in 1:2) { ## loop over strata
+ ## create index selecting elements of stored hazard info for stratum i...
+ ind <- which(bs$family$data$tr.strat == i)
+ if (i==1) plot(bs$family$data$tr[ind],exp(-bs$family$data$h[ind]),type="l",
+       ylim=c(0,1),xlab="time",ylab="survival",lwd=2,col=i) else
+       lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind]),lwd=2,col=i)
+ lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind] +
+       2*bs$family$data$q[ind]^.5),lty=2,col=i) ## upper ci
+ lines(bs$family$data$tr[ind],exp(-bs$family$data$h[ind] -
+       2*bs$family$data$q[ind]^.5),lty=2,col=i) ## lower ci
+ lines(bs$family$data$tr[ind],exp(-bs$family$data$km[ind]),col=i) ## KM
+ }
> 
> 
> ## Simple simulated known truth example...
> ph.weibull.sim <- function(eta,gamma=1,h0=.01,t1=100) { 
+   lambda <- h0*exp(eta) 
+   n <- length(eta)
+   U <- runif(n)
+   t <- (-log(U)/lambda)^(1/gamma)
+   d <- as.numeric(t <= t1)
+   t[!d] <- t1
+   list(t=t,d=d)
+ }
> n <- 500;set.seed(2)
> x0 <- runif(n, 0, 1);x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> f3 <- function(x) 0*x
> f <- f0(x0) + f1(x1)  + f2(x2)
> g <- (f-mean(f))/5
> surv <- ph.weibull.sim(g)
> surv$x0 <- x0;surv$x1 <- x1;surv$x2 <- x2;surv$x3 <- x3
> 
> b <- gam(t~s(x0)+s(x1)+s(x2,k=15)+s(x3),family=cox.ph,weights=d,data=surv)
> 
> plot(b,pages=1)
> 
> ## Another one, including a violation of proportional hazards for
> ## effect of x2...
> 
> set.seed(2)
> h <- exp((f0(x0)+f1(x1)+f2(x2)-10)/5)
> t <- rexp(n,h);d <- as.numeric(t<20)
> 
> ## first with no violation of PH in the simulation...
> b <- gam(t~s(x0)+s(x1)+s(x2)+s(x3),family=cox.ph,weights=d)
> plot(b,pages=1)
> ph.resid(b) ## fine
> 
> ## Now violate PH for x2 in the simulation...
> ii <- t>1.5
> h1 <- exp((f0(x0)+f1(x1)+3*f2(x2)-10)/5)
> t[ii] <- 1.5 + rexp(sum(ii),h1[ii]);d <- as.numeric(t<20)
> 
> b <- gam(t~s(x0)+s(x1)+s(x2)+s(x3),family=cox.ph,weights=d)
> plot(b,pages=1)
> ph.resid(b) ## The checking plot picks up the problem in s(x2) 
> 
> 
> ## conditional logistic regression models are often estimated using the 
> ## cox proportional hazards partial likelihood with a strata for each
> ## case-control group. A dummy vector of times is created (all equal). 
> ## The following compares to 'clogit' for a simple case. Note that
> ## the gam log likelihood is not exact if there is more than one case
> ## per stratum, corresponding to clogit's approximate method.
> library(survival);library(mgcv)
> infert$dumt <- rep(1,nrow(infert))
> mg <- gam(cbind(dumt,stratum) ~ spontaneous + induced, data=infert,
+           family=cox.ph,weights=case)
> ms <- clogit(case ~ spontaneous + induced + strata(stratum), data=infert,
+              method="approximate")
> summary(mg)$p.table[1:2,]; ms
            Estimate Std. Error  z value     Pr(>|z|)
spontaneous 1.985876  0.3524435 5.634592 1.754734e-08
induced     1.409012  0.3607124 3.906191 9.376245e-05
Call:
clogit(case ~ spontaneous + induced + strata(stratum), data = infert, 
    method = "approximate")

              coef exp(coef) se(coef)     z        p
spontaneous 1.9859    7.2854   0.3524 5.635 1.75e-08
induced     1.4090    4.0919   0.3607 3.906 9.38e-05

Likelihood ratio test=53.15  on 2 df, p=2.869e-12
n= 248, number of events= 83 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:survival’

> nameEx("coxpht")
> ### * coxpht
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cox.pht
> ### Title: Additive Cox proportional hazard models with time varying
> ###   covariates
> ### Aliases: cox.pht
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv);require(survival)
Loading required package: survival
> 
> ## First define functions for producing Poisson model data frame
> 
> app <- function(x,t,to) {
+ ## wrapper to approx for calling from apply...
+   y <- if (sum(!is.na(x))<1) rep(NA,length(to)) else
+        approx(t,x,to,method="constant",rule=2)$y
+   if (is.factor(x)) factor(levels(x)[y],levels=levels(x)) else y
+ } ## app
> 
> tdpois <- function(dat,event="z",et="futime",t="day",status="status1",
+                                                              id="id") {
+ ## dat is data frame. id is patient id; et is event time; t is
+ ## observation time; status is 1 for death 0 otherwise;
+ ## event is name for Poisson response.
+   if (event %in% names(dat)) warning("event name in use")
+   require(utils) ## for progress bar
+   te <- sort(unique(dat[[et]][dat[[status]]==1])) ## event times
+   sid <- unique(dat[[id]])
+   inter <- interactive()
+   if (inter) prg <- txtProgressBar(min = 0, max = length(sid), initial = 0,
+          char = "=",width = NA, title="Progress", style = 3)
+   ## create dataframe for poisson model data
+   dat[[event]] <- 0; start <- 1
+   dap <- dat[rep(1:length(sid),length(te)),]
+   for (i in 1:length(sid)) { ## work through patients
+     di <- dat[dat[[id]]==sid[i],] ## ith patient's data
+     tr <- te[te <= di[[et]][1]] ## times required for this patient
+     ## Now do the interpolation of covariates to event times...
+     um <- data.frame(lapply(X=di,FUN=app,t=di[[t]],to=tr))
+     ## Mark the actual event...
+     if (um[[et]][1]==max(tr)&&um[[status]][1]==1) um[[event]][nrow(um)] <- 1 
+     um[[et]] <- tr ## reset time to relevant event times
+     dap[start:(start-1+nrow(um)),] <- um ## copy to dap
+     start <- start + nrow(um)
+     if (inter) setTxtProgressBar(prg, i)
+   }
+   if (inter) close(prg)
+   dap[1:(start-1),]
+ } ## tdpois
> 
> ## The following typically takes a minute or less...
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("exclude.too.far")
> ### * exclude.too.far
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: exclude.too.far
> ### Title: Exclude prediction grid points too far from data
> ### Aliases: exclude.too.far
> ### Keywords: hplot
> 
> ### ** Examples
> 
> library(mgcv)
> x<-rnorm(100);y<-rnorm(100) # some "data"
> n<-40 # generate a grid....
> mx<-seq(min(x),max(x),length=n)
> my<-seq(min(y),max(y),length=n)
> gx<-rep(mx,n);gy<-rep(my,rep(n,n))
> tf<-exclude.too.far(gx,gy,x,y,0.1)
> plot(gx[!tf],gy[!tf],pch=".");points(x,y,col=2)
> 
> 
> 
> cleanEx()
> nameEx("extract.lme.cov")
> ### * extract.lme.cov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: extract.lme.cov
> ### Title: Extract the data covariance matrix from an lme object
> ### Aliases: extract.lme.cov extract.lme.cov2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see also ?formXtViX for use of extract.lme.cov2
> require(mgcv)
> library(nlme)
> data(Rail)
> b <- lme(travel~1,Rail,~1|Rail)
> extract.lme.cov(b)
          [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]
 [1,] 631.4778 615.3111 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000
 [2,] 615.3111 631.4778 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000
 [3,] 615.3111 615.3111 631.4778   0.0000   0.0000   0.0000   0.0000   0.0000
 [4,]   0.0000   0.0000   0.0000 631.4778 615.3111 615.3111   0.0000   0.0000
 [5,]   0.0000   0.0000   0.0000 615.3111 631.4778 615.3111   0.0000   0.0000
 [6,]   0.0000   0.0000   0.0000 615.3111 615.3111 631.4778   0.0000   0.0000
 [7,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 631.4778 615.3111
 [8,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111 631.4778
 [9,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111 615.3111
[10,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[11,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[12,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[13,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[14,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[15,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[16,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[17,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[18,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
          [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]
 [1,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [2,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [3,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [4,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [5,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [6,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [7,] 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [8,] 615.3111   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
 [9,] 631.4778   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000
[10,]   0.0000 631.4778 615.3111 615.3111   0.0000   0.0000   0.0000   0.0000
[11,]   0.0000 615.3111 631.4778 615.3111   0.0000   0.0000   0.0000   0.0000
[12,]   0.0000 615.3111 615.3111 631.4778   0.0000   0.0000   0.0000   0.0000
[13,]   0.0000   0.0000   0.0000   0.0000 631.4778 615.3111 615.3111   0.0000
[14,]   0.0000   0.0000   0.0000   0.0000 615.3111 631.4778 615.3111   0.0000
[15,]   0.0000   0.0000   0.0000   0.0000 615.3111 615.3111 631.4778   0.0000
[16,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 631.4778
[17,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111
[18,]   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000 615.3111
         [,17]    [,18]
 [1,]   0.0000   0.0000
 [2,]   0.0000   0.0000
 [3,]   0.0000   0.0000
 [4,]   0.0000   0.0000
 [5,]   0.0000   0.0000
 [6,]   0.0000   0.0000
 [7,]   0.0000   0.0000
 [8,]   0.0000   0.0000
 [9,]   0.0000   0.0000
[10,]   0.0000   0.0000
[11,]   0.0000   0.0000
[12,]   0.0000   0.0000
[13,]   0.0000   0.0000
[14,]   0.0000   0.0000
[15,]   0.0000   0.0000
[16,] 615.3111 615.3111
[17,] 631.4778 615.3111
[18,] 615.3111 631.4778
> extract.lme.cov2(b)
$V
$V[[1]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778

$V[[2]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778

$V[[3]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778

$V[[4]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778

$V[[5]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778

$V[[6]]
         [,1]     [,2]     [,3]
[1,] 631.4778 615.3111 615.3111
[2,] 615.3111 631.4778 615.3111
[3,] 615.3111 615.3111 631.4778


$ind
 [1]  4  5  6 13 14 15  1  2  3 16 17 18  7  8  9 10 11 12

> 
> 
> 
> cleanEx()
> nameEx("fixDependence")
> ### * fixDependence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fixDependence
> ### Title: Detect linear dependencies of one matrix on another
> ### Aliases: fixDependence
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> n<-20;c1<-4;c2<-7
> X1<-matrix(runif(n*c1),n,c1)
> X2<-matrix(runif(n*c2),n,c2)
> X2[,3]<-X1[,2]+X2[,4]*.1
> X2[,5]<-X1[,1]*.2+X1[,2]*.04
> fixDependence(X1,X2)
[1] 3 5
> fixDependence(X1,X2,strict=TRUE)
[1] 5
> 
> 
> 
> cleanEx()
> nameEx("formXtViX")
> ### * formXtViX
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: formXtViX
> ### Title: Form component of GAMM covariance matrix
> ### Aliases: formXtViX
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> library(nlme)
> data(ergoStool)
> b <- lme(effort ~ Type, data=ergoStool, random=~1|Subject)
> V1 <- extract.lme.cov(b, ergoStool)
> V2 <- extract.lme.cov2(b, ergoStool)
> X <- model.matrix(b, data=ergoStool)
> crossprod(formXtViX(V2, X))
            (Intercept)    TypeT2    TypeT3    TypeT4
(Intercept)    4.330827  1.082707  1.082707  1.082707
TypeT2         1.082707  5.846203 -1.587832 -1.587832
TypeT3         1.082707 -1.587832  5.846203 -1.587832
TypeT4         1.082707 -1.587832 -1.587832  5.846203
> t(X)
            1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25
(Intercept) 1 1 1 1 1 1 1 1 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
TypeT2      0 1 0 0 0 1 0 0 0  1  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0
TypeT3      0 0 1 0 0 0 1 0 0  0  1  0  0  0  1  0  0  0  1  0  0  0  1  0  0
TypeT4      0 0 0 1 0 0 0 1 0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  1  0
            26 27 28 29 30 31 32 33 34 35 36
(Intercept)  1  1  1  1  1  1  1  1  1  1  1
TypeT2       1  0  0  0  1  0  0  0  1  0  0
TypeT3       0  1  0  0  0  1  0  0  0  1  0
TypeT4       0  0  1  0  0  0  1  0  0  0  1
attr(,"assign")
[1] 0 1 1 1
attr(,"contrasts")
attr(,"contrasts")$Type
[1] "contr.treatment"

> 
> 
> 
> cleanEx()
> nameEx("fs.test")
> ### * fs.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fs.test
> ### Title: FELSPLINE test function
> ### Aliases: fs.test fs.boundary
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## plot the function, and its boundary...
> fsb <- fs.boundary()
> m<-300;n<-150 
> xm <- seq(-1,4,length=m);yn<-seq(-1,1,length=n)
> xx <- rep(xm,n);yy<-rep(yn,rep(m,n))
> tru <- matrix(fs.test(xx,yy),m,n) ## truth
> image(xm,yn,tru,col=heat.colors(100),xlab="x",ylab="y")
> lines(fsb$x,fsb$y,lwd=3)
> contour(xm,yn,tru,levels=seq(-5,5,by=.25),add=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("gam")
> ### * gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam
> ### Title: Generalized additive models with integrated smoothness
> ###   estimation
> ### Aliases: gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see also examples in ?gam.models (e.g. 'by' variables, 
> ## random effects and tricks for large binary datasets)
> 
> library(mgcv)
> set.seed(2) ## simulate some data... 
> dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.83328    0.09878    79.3   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 2.500  3.115  6.921 0.000131 ***
s(x1) 2.401  2.984 81.858  < 2e-16 ***
s(x2) 7.698  8.564 88.158  < 2e-16 ***
s(x3) 1.000  1.000  4.343 0.037818 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.715   Deviance explained = 72.5%
GCV = 4.0505  Scale est. = 3.9027    n = 400
> plot(b,pages=1,residuals=TRUE)  ## show partial residuals
> plot(b,pages=1,seWithMean=TRUE) ## `with intercept' CIs
> ## run some basic model checks, including checking
> ## smoothing basis dimensions...
> gam.check(b)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 12 iterations.
The RMS GCV score gradient at convergence was 1.739918e-07 .
The Hessian was positive definite.
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

       k' edf k-index p-value
s(x0) 9.0 2.5    1.04    0.77
s(x1) 9.0 2.4    1.03    0.68
s(x2) 9.0 7.7    0.97    0.23
s(x3) 9.0 1.0    1.03    0.70
> 
> ## same fit in two parts .....
> G <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat)
> b <- gam(G=G)
> print(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     
> 
> ## 2 part fit enabling manipulation of smoothing parameters...
> G <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),fit=FALSE,data=dat,sp=b$sp)
> G$lsp0 <- log(b$sp*10) ## provide log of required sp vec
> gam(G=G) ## it's smoother

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
1.43 1.39 5.20 1.00  total = 10.02 

GCV score: 4.365998     
> 
> ## change the smoothness selection method to REML
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
> ## use alternative plotting scheme, and way intervals include
> ## smoothing parameter uncertainty...
> plot(b0,pages=1,scheme=1,unconditional=TRUE) 
> 
> ## Would a smooth interaction of x0 and x1 be better?
> ## Use tensor product smooth of x0 and x1, basis 
> ## dimension 49 (see ?te for details, also ?t2).
> bt <- gam(y~te(x0,x1,k=7)+s(x2)+s(x3),data=dat,
+           method="REML")
> plot(bt,pages=1) 
> plot(bt,pages=1,scheme=2) ## alternative visualization
> AIC(b0,bt) ## interaction worse than additive
         df      AIC
b0 17.68999 1698.504
bt 24.48943 1698.189
> 
> ## Alternative: test for interaction with a smooth ANOVA 
> ## decomposition (this time between x2 and x1)
> bt <- gam(y~s(x0)+s(x1)+s(x2)+s(x3)+ti(x1,x2,k=6),
+             data=dat,method="REML")
> summary(bt)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3) + ti(x1, x2, k = 6)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.83338    0.09883   79.26   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0)     3.013  3.744  6.226 0.000148 ***
s(x1)     2.840  3.524 69.568  < 2e-16 ***
s(x2)     8.012  8.736 85.971  < 2e-16 ***
s(x3)     1.000  1.000  4.176 0.041676 *  
ti(x1,x2) 1.006  1.011  0.087 0.779671    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.715   Deviance explained = 72.6%
-REML = 860.37  Scale est. = 3.9068    n = 400
> 
> ## If it is believed that x0 and x1 are naturally on 
> ## the same scale, and should be treated isotropically 
> ## then could try...
> bs <- gam(y~s(x0,x1,k=40)+s(x2)+s(x3),data=dat,
+           method="REML")
> plot(bs,pages=1)
> AIC(b0,bt,bs) ## additive still better. 
         df      AIC
b0 17.68999 1698.504
bt 18.79727 1700.592
bs 23.78889 1699.827
> 
> ## Now do automatic terms selection as well
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,
+        method="REML",select=TRUE)
> plot(b1,pages=1)
> 
> 
> ## set the smoothing parameter for the first term, estimate rest ...
> bp <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),sp=c(0.01,-1,-1,-1),data=dat)
> plot(bp,pages=1,scheme=1)
> ## alternatively...
> bp <- gam(y~s(x0,sp=.01)+s(x1)+s(x2)+s(x3),data=dat)
> 
> 
> # set lower bounds on smoothing parameters ....
> bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
+         min.sp=c(0.001,0.01,0,10),data=dat) 
> print(b);print(bp)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.5 2.4 7.7 1.0  total = 14.6 

GCV score: 4.050519     
> 
> # same with REML
> bp<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),
+         min.sp=c(0.1,0.1,0,10),data=dat,method="REML") 
> print(b0);print(bp)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.89 

REML score: 861.1296     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.88 

REML score: 861.1292     
> 
> 
> ## now a GAM with 3df regression spline term & 2 penalized terms
> 
> b0 <- gam(y~s(x0,k=4,fx=TRUE,bs="tp")+s(x1,k=12)+s(x2,k=15),data=dat)
> plot(b0,pages=1)
> 
> 
> 
> 
> cleanEx()
> nameEx("gam.check")
> ### * gam.check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.check
> ### Title: Some diagnostics for a fitted gam model
> ### Aliases: gam.check
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> dat <- gamSim(1,n=200)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> gam.check(b,pch=19,cex=.3)

Method: GCV   Optimizer: magic
Smoothing parameter selection converged after 8 iterations.
The RMS GCV score gradient at convergence was 1.072609e-05 .
The Hessian was positive definite.
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value
s(x0) 9.00 2.32    1.00    0.51
s(x1) 9.00 2.31    0.97    0.29
s(x2) 9.00 7.65    0.96    0.27
s(x3) 9.00 1.23    1.04    0.67
> 
> 
> 
> cleanEx()
> nameEx("gam.mh")
> ### * gam.mh
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.mh
> ### Title: Simple posterior simulation with gam fits
> ### Aliases: gam.mh posterior.simulation
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3);n <- 400
> 
> ############################################
> ## First example: simulated Tweedie model...
> ############################################
> 
> dat <- gamSim(1,n=n,dist="poisson",scale=.2)
Gu & Wahba 4 term additive model
> dat$y <- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=tw(),
+           data=dat,method="REML")
> 
> ## simulate directly from Gaussian approximate posterior...
> br <- rmvn(1000,coef(b),vcov(b))
> 
> ## Alternatively use MH sampling...
> br <- gam.mh(b,thin=2,ns=2000,rw.scale=.15)$bs
> ## If 'coda' installed, can check effective sample size
> ## require(coda);effectiveSize(as.mcmc(br))
> 
> ## Now compare simulation results and Gaussian approximation for
> ## smooth term confidence intervals...
> x <- seq(0,1,length=100)
> pd <- data.frame(x0=x,x1=x,x2=x,x3=x)
> X <- predict(b,newdata=pd,type="lpmatrix")
> par(mfrow=c(2,2))
> for(i in 1:4) {
+   plot(b,select=i,scale=0,scheme=1)
+   ii <- b$smooth[[i]]$first.para:b$smooth[[i]]$last.para
+   ff <- X[,ii]
+   fq <- apply(ff,1,quantile,probs=c(.025,.16,.84,.975))
+   lines(x,fq[1,],col=2,lty=2);lines(x,fq[4,],col=2,lty=2)
+   lines(x,fq[2,],col=2);lines(x,fq[3,],col=2)
+ }
> 
> ###############################################################
> ## Second example, where Gaussian approximation is a failure...
> ###############################################################
> 
> y <- c(rep(0, 89), 1, 0, 1, 0, 0, 1, rep(0, 13), 1, 0, 0, 1, 
+        rep(0, 10), 1, 0, 0, 1, 1, 0, 1, rep(0,4), 1, rep(0,3),  
+        1, rep(0, 3), 1, rep(0, 10), 1, rep(0, 4), 1, 0, 1, 0, 0, 
+        rep(1, 4), 0, rep(1, 5), rep(0, 4), 1, 1, rep(0, 46))
> set.seed(3);x <- sort(c(0:10*5,rnorm(length(y)-11)*20+100))
> b <- gam(y ~ s(x, k = 15),method = 'REML', family = binomial)
> br <- gam.mh(b,thin=2,ns=2000,rw.scale=.4)$bs
> X <- model.matrix(b)
> par(mfrow=c(1,1))
> plot(x, y, col = rgb(0,0,0,0.25), ylim = c(0,1))
> ff <- X
> linv <- b$family$linkinv
> ## Get intervals for the curve on the response scale...
> fq <- linv(apply(ff,1,quantile,probs=c(.025,.16,.5,.84,.975)))
> lines(x,fq[1,],col=2,lty=2);lines(x,fq[5,],col=2,lty=2)
> lines(x,fq[2,],col=2);lines(x,fq[4,],col=2)
> lines(x,fq[3,],col=4)
> ## Compare to the Gaussian posterior approximation
> fv <- predict(b,se=TRUE)
> lines(x,linv(fv$fit))
> lines(x,linv(fv$fit-2*fv$se.fit),lty=3)
> lines(x,linv(fv$fit+2*fv$se.fit),lty=3)
> ## ... Notice the useless 95% CI (black dotted) based on the
> ## Gaussian approximation!
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.models")
> ### * gam.models
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.models
> ### Title: Specifying generalized additive models
> ### Aliases: gam.models
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(10)
> ## simulate date from y = f(x2)*x1 + error
> dat <- gamSim(3,n=400)
Continuous `by' variable example
> 
> b<-gam(y ~ s(x2,by=x1),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x2, by = x1)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept)  -0.1317     0.2128  -0.619    0.536

Approximate significance of smooth terms:
           edf Ref.df     F p-value    
s(x2):x1 9.199  9.822 32.29  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.439   Deviance explained = 45.2%
GCV = 4.5182  Scale est. = 4.403     n = 400
> 
> ## Factor `by' variable example (with a spurious covariate x0)
> ## simulate data...
> 
> dat <- gamSim(4)
Factor `by' variable example
> 
> ## fit model...
> b <- gam(y ~ fac+s(x2,by=fac)+s(x0),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2, by = fac) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.3294     0.1872   7.100 6.14e-12 ***
fac2         -1.6389     0.2624  -6.247 1.12e-09 ***
fac3          2.0614     0.2644   7.797 6.11e-14 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F p-value    
s(x2):fac1 2.585  3.223  3.125  0.0224 *  
s(x2):fac2 2.222  2.757 33.526  <2e-16 ***
s(x2):fac3 7.853  8.638 20.604  <2e-16 ***
s(x0)      2.877  3.580  1.069  0.3049    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.532   Deviance explained = 55.3%
GCV = 4.6776  Scale est. = 4.4609    n = 400
> 
> ## note that the preceding fit is the same as....
> b1<-gam(y ~ s(x2,by=as.numeric(fac==1))+s(x2,by=as.numeric(fac==2))+
+             s(x2,by=as.numeric(fac==3))+s(x0)-1,data=dat)
> ## ... the `-1' is because the intercept is confounded with the 
> ## *uncentred* smooths here.
> plot(b1,pages=1)
> summary(b1)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x2, by = as.numeric(fac == 1)) + s(x2, by = as.numeric(fac == 
    2)) + s(x2, by = as.numeric(fac == 3)) + s(x0) - 1

Approximate significance of smooth terms:
                             edf Ref.df      F p-value    
s(x2):as.numeric(fac == 1) 3.585  4.223 14.731  <2e-16 ***
s(x2):as.numeric(fac == 2) 3.222  3.757 24.607  <2e-16 ***
s(x2):as.numeric(fac == 3) 8.853  9.638 50.709  <2e-16 ***
s(x0)                      2.877  3.580  1.069   0.305    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.532   Deviance explained = 63.5%
GCV = 4.6776  Scale est. = 4.4609    n = 400
> 
> ## repeat forcing all s(x2) terms to have the same smoothing param
> ## (not a very good idea for these data!)
> b2 <- gam(y ~ fac+s(x2,by=fac,id=1)+s(x0),data=dat)
> plot(b2,pages=1)
> summary(b2)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2, by = fac, id = 1) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.3218     0.1910   6.921 1.95e-11 ***
fac2         -1.6408     0.2673  -6.139 2.12e-09 ***
fac3          2.0049     0.2658   7.542 3.53e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F p-value    
s(x2):fac1 6.387  7.534  1.895   0.066 .  
s(x2):fac2 6.378  7.512 12.607  <2e-16 ***
s(x2):fac3 6.268  7.395 22.056  <2e-16 ***
s(x0)      2.901  3.604  1.137   0.281    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.526   Deviance explained = 55.5%
GCV = 4.8189  Scale est. = 4.5186    n = 400
> 
> ## now repeat with a single reference level smooth, and 
> ## two `difference' smooths...
> dat$fac <- ordered(dat$fac)
> b3 <- gam(y ~ fac+s(x2)+s(x2,by=fac)+s(x0),data=dat,method="REML")
> plot(b3,pages=1)
> summary(b3)

Family: gaussian 
Link function: identity 

Formula:
y ~ fac + s(x2) + s(x2, by = fac) + s(x0)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   1.4613     0.1071  13.648  < 2e-16 ***
fac.L         1.4373     0.1868   7.696 1.22e-13 ***
fac.Q         2.1744     0.1853  11.737  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df      F p-value    
s(x2)      2.884  3.580  2.442  0.0496 *  
s(x2):fac2 3.011  3.737 12.806  <2e-16 ***
s(x2):fac3 7.202  8.191 13.240  <2e-16 ***
s(x0)      2.454  3.060  1.084  0.3322    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.53   Deviance explained = 55.1%
-REML = 886.13  Scale est. = 4.4827    n = 400
> 
> 
> rm(dat)
> 
> ## An example of a simple random effects term implemented via 
> ## penalization of the parametric part of the model...
> 
> dat <- gamSim(1,n=400,scale=2) ## simulate 4 term additive truth
Gu & Wahba 4 term additive model
> ## Now add some random effects to the simulation. Response is 
> ## grouped into one of 20 groups by `fac' and each groups has a
> ## random effect added....
> fac <- as.factor(sample(1:20,400,replace=TRUE))
> dat$X <- model.matrix(~fac-1)
> b <- rnorm(20)*.5
> dat$y <- dat$y + dat$X%*%b
> 
> ## now fit appropriate random effect model...
> PP <- list(X=list(rank=20,diag(20)))
> rm <- gam(y~ X+s(x0)+s(x1)+s(x2)+s(x3),data=dat,paraPen=PP)
> plot(rm,pages=1)
> ## Get estimated random effects standard deviation...
> sig.b <- sqrt(rm$sig2/rm$sp[1]);sig.b 
      X 
0.49167 
> 
> ## a much simpler approach uses "re" terms...
> 
> rm1 <- gam(y ~ s(fac,bs="re")+s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="ML")
> gam.vcomp(rm1)

Standard deviations and 0.95 confidence intervals:

           std.dev        lower        upper
s(fac)  0.47544904 2.539353e-01 8.901943e-01
s(x0)   4.59854379 1.418796e+00 1.490461e+01
s(x1)   6.69755946 1.742838e+00 2.573807e+01
s(x2)  98.79933812 5.574346e+01 1.751113e+02
s(x3)   0.02249211 2.625083e-42 1.927158e+38
scale   2.13250606 1.983199e+00 2.293054e+00

Rank: 6/6
> 
> ## Simple comparison with lme, using Rail data. 
> ## See ?random.effects for a simpler method
> require(nlme)
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="ML") 
> Z <- model.matrix(~Rail-1,data=Rail,
+      contrasts.arg=list(Rail="contr.treatment"))
> b <- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="ML")
> 
> b0 
Linear mixed-effects model fit by maximum likelihood
  Data: Rail 
  Log-likelihood: -64.28002
  Fixed: travel ~ 1 
(Intercept) 
       66.5 

Random effects:
 Formula: ~1 | Rail
        (Intercept) Residual
StdDev:    22.62435 4.020779

Number of Observations: 18
Number of Groups: 6 
> (b$reml.scale/b$sp)^.5 ## `gam' ML estimate of Rail sd
       Z 
22.62435 
> b$reml.scale^.5         ## `gam' ML estimate of residual sd
[1] 4.020779
> 
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="REML") 
> Z <- model.matrix(~Rail-1,data=Rail,
+      contrasts.arg=list(Rail="contr.treatment"))
> b <- gam(travel~Z,data=Rail,paraPen=list(Z=list(diag(6))),method="REML")
> 
> b0 
Linear mixed-effects model fit by REML
  Data: Rail 
  Log-restricted-likelihood: -61.0885
  Fixed: travel ~ 1 
(Intercept) 
       66.5 

Random effects:
 Formula: ~1 | Rail
        (Intercept) Residual
StdDev:    24.80547 4.020779

Number of Observations: 18
Number of Groups: 6 
> (b$reml.scale/b$sp)^.5 ## `gam' REML estimate of Rail sd
       Z 
24.80547 
> b$reml.scale^.5         ## `gam' REML estimate of residual sd
[1] 4.020779
> 
> ################################################################
> ## Approximate large dataset logistic regression for rare events
> ## based on subsampling the zeroes, and adding an offset to
> ## approximately allow for this.
> ## Doing the same thing, but upweighting the sampled zeroes
> ## leads to problems with smoothness selection, and CIs.
> ################################################################
> n <- 50000  ## simulate n data 
> dat <- gamSim(1,n=n,dist="binary",scale=.33)
Gu & Wahba 4 term additive model
> p <- binomial()$linkinv(dat$f-6) ## make 1's rare
> dat$y <- rbinom(p,1,p)      ## re-simulate rare response
> 
> ## Now sample all the 1's but only proportion S of the 0's
> S <- 0.02                   ## sampling fraction of zeroes
> dat <- dat[dat$y==1 | runif(n) < S,] ## sampling
> 
> ## Create offset based on total sampling fraction
> dat$s <- rep(log(nrow(dat)/n),nrow(dat))
> 
> lr.fit <- gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+s(x3,bs="cr")+
+               offset(s),family=binomial,data=dat,method="REML")
> 
> ## plot model components with truth overlaid in red
> op <- par(mfrow=c(2,2))
> fn <- c("f0","f1","f2","f3");xn <- c("x0","x1","x2","x3")
> for (k in 1:4) {
+        plot(lr.fit,select=k,scale=0)
+        ff <- dat[[fn[k]]];xx <- dat[[xn[k]]]
+        ind <- sort.int(xx,index.return=TRUE)$ix
+        lines(xx[ind],(ff-mean(ff))[ind]*.33,col=2)
+ }
> par(op)
> rm(dat)
> 
> ## A Gamma example, by modify `gamSim' output...
>  
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## Note that `shape' and `scale' in `rgamma' are almost
> ## opposite terminology to that used with GLM/GAM...
> dat$y <- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
> bg <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
+           data=dat,method="REML")
> plot(bg,pages=1,scheme=1)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.selection")
> ### * gam.selection
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.selection
> ### Title: Generalized Additive Model Selection
> ### Aliases: gam.selection
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## an example of automatic model selection via null space penalization
> library(mgcv)
> set.seed(3);n<-200
> dat <- gamSim(1,n=n,scale=.15,dist="poisson") ## simulate data
Gu & Wahba 4 term additive model
> dat$x4 <- runif(n, 0, 1);dat$x5 <- runif(n, 0, 1) ## spurious
> 
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3)+s(x4)+s(x5),data=dat,
+          family=poisson,select=TRUE,method="REML")
> summary(b)

Family: poisson 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3) + s(x4) + s(x5)

Parametric coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  1.21758    0.04082   29.83   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df  Chi.sq p-value    
s(x0) 1.7655088      9   5.264  0.0392 *  
s(x1) 1.9271040      9  65.356  <2e-16 ***
s(x2) 6.1351414      9 156.204  <2e-16 ***
s(x3) 0.0002849      9   0.000  0.4181    
s(x4) 0.0003044      9   0.000  0.9703    
s(x5) 0.1756926      9   0.195  0.3018    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.545   Deviance explained = 51.6%
-REML = 430.78  Scale est. = 1         n = 200
> plot(b,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("gam.side")
> ### * gam.side
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.side
> ### Title: Identifiability side conditions for a GAM
> ### Aliases: gam.side
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## The first two examples here iluustrate models that cause
> ## gam.side to impose constraints, but both are a bad way 
> ## of estimating such models. The 3rd example is the right
> ## way.... 
> set.seed(7)
> require(mgcv)
> dat <- gamSim(n=400,scale=2) ## simulate data
Gu & Wahba 4 term additive model
> ## estimate model with redundant smooth interaction (bad idea).
> b<-gam(y~s(x0)+s(x1)+s(x0,x1)+s(x2),data=dat)
> plot(b,pages=1)
> 
> ## Simulate data with real interation...
> dat <- gamSim(2,n=500,scale=.1)
Bivariate smoothing example
> old.par<-par(mfrow=c(2,2))
> 
> ## a fully nested tensor product example (bad idea)
> b <- gam(y~s(x,bs="cr",k=6)+s(z,bs="cr",k=6)+te(x,z,k=6),
+        data=dat$data)
> plot(b)
> 
> old.par<-par(mfrow=c(2,2))
> ## A fully nested tensor product example, done properly,
> ## so that gam.side is not needed to ensure identifiability.
> ## ti terms are designed to produce interaction smooths
> ## suitable for adding to main effects (we could also have
> ## used s(x) and s(z) without a problem, but not s(z,x) 
> ## or te(z,x)).
> b <- gam(y ~ ti(x,k=6) + ti(z,k=6) + ti(x,z,k=6),
+        data=dat$data)
> plot(b)
> 
> par(old.par)
> rm(dat)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("gam.vcomp")
> ### * gam.vcomp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gam.vcomp
> ### Title: Report gam smoothness estimates as variance components
> ### Aliases: gam.vcomp
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
>   set.seed(3) 
>   require(mgcv)
>   ## simulate some data, consisting of a smooth truth + random effects
> 
>   dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
>   a <- factor(sample(1:10,400,replace=TRUE))
>   b <- factor(sample(1:7,400,replace=TRUE))
>   Xa <- model.matrix(~a-1)    ## random main effects
>   Xb <-  model.matrix(~b-1)
>   Xab <- model.matrix(~a:b-1) ## random interaction
>   dat$y <- dat$y + Xa%*%rnorm(10)*.5 + 
+            Xb%*%rnorm(7)*.3 + Xab%*%rnorm(70)*.7
>   dat$a <- a;dat$b <- b
> 
>   ## Fit the model using "re" terms, and smoother linkage  
>   
>   mod <- gam(y~s(a,bs="re")+s(b,bs="re")+s(a,b,bs="re")+s(x0,id=1)+s(x1,id=1)+
+                s(x2,k=15)+s(x3),data=dat,method="ML")
> 
>   gam.vcomp(mod) 

Standard deviations and 0.95 confidence intervals:

            std.dev        lower        upper
s(a)    0.432006710 1.873022e-01 9.964099e-01
s(b)    0.003836725 6.386260e-38 2.305020e+32
s(a,b)  0.501689907 2.224610e-01 1.131402e+00
s(x0)   8.363917220 4.576898e+00 1.528439e+01
s(x2)  88.726027158 5.167683e+01 1.523373e+02
s(x3)   0.031029563 3.364168e-28 2.862027e+24
scale   2.096361706 1.937807e+00 2.267889e+00

Rank: 7/7

All smooth components:
        s(a)         s(b)       s(a,b)        s(x0)        s(x1)        s(x2) 
 0.432006710  0.003836725  0.501689907  8.363917220  8.363917220 88.726027158 
       s(x3) 
 0.031029563 
> 
> 
> 
> 
> cleanEx()
> nameEx("gamSim")
> ### * gamSim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamSim
> ### Title: Simulate example data for GAMs
> ### Aliases: gamSim
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see ?gam
> 
> 
> 
> cleanEx()
> nameEx("gamm")
> ### * gamm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamm
> ### Title: Generalized Additive Mixed Models
> ### Aliases: gamm
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simple examples using gamm as alternative to gam
> set.seed(0) 
> dat <- gamSim(1,n=200,scale=2)
Gu & Wahba 4 term additive model
> b <- gamm(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b$gam,pages=1)
> summary(b$lme) # details of underlying lme fit
Linear mixed-effects model fit by maximum likelihood
 Data: strip.offset(mf) 
       AIC      BIC    logLik
  929.4264 962.4096 -454.7132

Random effects:
 Formula: ~Xr - 1 | g
 Structure: pdIdnot
             Xr1      Xr2      Xr3      Xr4      Xr5      Xr6      Xr7      Xr8
StdDev: 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454 1.832454

 Formula: ~Xr.0 - 1 | g.0 %in% g
 Structure: pdIdnot
           Xr.01    Xr.02    Xr.03    Xr.04    Xr.05    Xr.06    Xr.07    Xr.08
StdDev: 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091 1.517091

 Formula: ~Xr.1 - 1 | g.1 %in% g.0 %in% g
 Structure: pdIdnot
           Xr.11    Xr.12    Xr.13    Xr.14    Xr.15    Xr.16    Xr.17    Xr.18
StdDev: 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583 24.29583

 Formula: ~Xr.2 - 1 | g.2 %in% g.1 %in% g.0 %in% g
 Structure: pdIdnot
               Xr.21        Xr.22        Xr.23        Xr.24        Xr.25
StdDev: 0.0001727025 0.0001727025 0.0001727025 0.0001727025 0.0001727025
               Xr.26        Xr.27        Xr.28 Residual
StdDev: 0.0001727025 0.0001727025 0.0001727025 2.142508

Fixed effects: y.0 ~ X - 1 
                 Value Std.Error  DF  t-value p-value
X(Intercept)  7.318201 0.1534282 195 47.69790  0.0000
Xs(x0)Fx1    -0.152108 0.6446780 195 -0.23594  0.8137
Xs(x1)Fx1     1.858918 0.5380989 195  3.45460  0.0007
Xs(x2)Fx1     4.576313 2.8309536 195  1.61653  0.1076
Xs(x3)Fx1     0.071710 0.1572356 195  0.45607  0.6488
 Correlation: 
          X(Int) X(0)F1 X(1)F1 X(2)F1
Xs(x0)Fx1  0.000                     
Xs(x1)Fx1  0.000 -0.009              
Xs(x2)Fx1  0.000  0.025 -0.031       
Xs(x3)Fx1  0.000 -0.052  0.023 -0.064

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.63556105 -0.66952301 -0.05179304  0.65709885  2.25213793 

Number of Observations: 200
Number of Groups: 
                           g                   g.0 %in% g 
                           1                            1 
         g.1 %in% g.0 %in% g g.2 %in% g.1 %in% g.0 %in% g 
                           1                            1 
> summary(b$gam) # gam style summary of fitted model

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1519   48.19   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 2.489  2.489  5.266 0.00555 ** 
s(x1) 2.209  2.209 56.511 < 2e-16 ***
s(x2) 7.305  7.305 36.985 < 2e-16 ***
s(x3) 1.000  1.000  0.212 0.64554    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.686   
  Scale est. = 4.5903    n = 200
> anova(b$gam) 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Approximate significance of smooth terms:
        edf Ref.df      F p-value
s(x0) 2.489  2.489  5.266 0.00555
s(x1) 2.209  2.209 56.511 < 2e-16
s(x2) 7.305  7.305 36.985 < 2e-16
s(x3) 1.000  1.000  0.212 0.64554
> gam.check(b$gam) # simple checking plots

'gamm' based fit - care required with interpretation.
Checks based on working residuals may be misleading.
Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value
s(x0) 9.00 2.49    1.00    0.53
s(x1) 9.00 2.21    0.97    0.32
s(x2) 9.00 7.30    0.96    0.26
s(x3) 9.00 1.00    1.04    0.65
> 
> b <- gamm(y~te(x0,x1)+s(x2)+s(x3),data=dat) 
> op <- par(mfrow=c(2,2))
> plot(b$gam)
> par(op)
> rm(dat)
> 
> 
> ## Add a factor to the linear predictor, to be modelled as random
> dat <- gamSim(6,n=200,scale=.2,dist="poisson")
4 term additive + random effectGu & Wahba 4 term additive model
> b2 <- gamm(y~s(x0)+s(x1)+s(x2),family=poisson,
+            data=dat,random=list(fac=~1))

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
iteration 3
> plot(b2$gam,pages=1)
> fac <- dat$fac
> rm(dat)
> vis.gam(b2$gam)
> 
> ## In the generalized case the 'gam' object is based on the working
> ## model used in the PQL fitting. Residuals for this are not
> ## that useful on their own as the following illustrates...
> 
> gam.check(b2$gam) 

'gamm' based fit - care required with interpretation.
Checks based on working residuals may be misleading.
Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value
s(x0) 9.00 3.83    1.04    0.78
s(x1) 9.00 4.09    1.08    0.91
s(x2) 9.00 8.52    0.95    0.33
> 
> ## But more useful residuals are easy to produce on a model
> ## by model basis. For example...
> 
> fv <- exp(fitted(b2$lme)) ## predicted values (including re)
> rsd <- (b2$gam$y - fv)/sqrt(fv) ## Pearson residuals (Poisson case)
> op <- par(mfrow=c(1,2))
> qqnorm(rsd);plot(fv^.5,rsd)
> par(op)
> 
> ## now an example with autocorrelated errors....
> n <- 200;sig <- 2
> x <- 0:(n-1)/(n-1)
> f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> e <- rnorm(n,0,sig)
> for (i in 2:n) e[i] <- 0.6*e[i-1] + e[i]
> y <- f + e
> op <- par(mfrow=c(2,2))
> ## Fit model with AR1 residuals
> b <- gamm(y~s(x,k=20),correlation=corAR1())
> plot(b$gam);lines(x,f-mean(f),col=2)
> ## Raw residuals still show correlation, of course...
> acf(residuals(b$gam),main="raw residual ACF")
> ## But standardized are now fine...
> acf(residuals(b$lme,type="normalized"),main="standardized residual ACF")
> ## compare with model without AR component...
> b <- gam(y~s(x,k=20))
> plot(b);lines(x,f-mean(f),col=2)
> 
> ## more complicated autocorrelation example - AR errors
> ## only within groups defined by `fac'
> e <- rnorm(n,0,sig)
> for (i in 2:n) e[i] <- 0.6*e[i-1]*(fac[i-1]==fac[i]) + e[i]
> y <- f + e
> b <- gamm(y~s(x,k=20),correlation=corAR1(form=~1|fac))
> plot(b$gam);lines(x,f-mean(f),col=2)
> par(op) 
> 
> ## more complex situation with nested random effects and within
> ## group correlation 
> 
> set.seed(0)
> n.g <- 10
> n<-n.g*10*4
> ## simulate smooth part...
> dat <- gamSim(1,n=n,scale=2)
Gu & Wahba 4 term additive model
> f <- dat$f
> ## simulate nested random effects....
> fa <- as.factor(rep(1:10,rep(4*n.g,10)))
> ra <- rep(rnorm(10),rep(4*n.g,10))
> fb <- as.factor(rep(rep(1:4,rep(n.g,4)),10))
> rb <- rep(rnorm(4),rep(n.g,4))
> for (i in 1:9) rb <- c(rb,rep(rnorm(4),rep(n.g,4)))
> ## simulate auto-correlated errors within groups
> e<-array(0,0)
> for (i in 1:40) {
+   eg <- rnorm(n.g, 0, sig)
+   for (j in 2:n.g) eg[j] <- eg[j-1]*0.6+ eg[j]
+   e<-c(e,eg)
+ }
> dat$y <- f + ra + rb + e
> dat$fa <- fa;dat$fb <- fb
> ## fit model .... 
> b <- gamm(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
+   s(x3,bs="cr"),data=dat,random=list(fa=~1,fb=~1),
+   correlation=corAR1())
> plot(b$gam,pages=1)
> summary(b$gam)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "cr") + s(x1, bs = "cr") + s(x2, bs = "cr") + 
    s(x3, bs = "cr")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.7756     0.4413   17.62   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df       F  p-value    
s(x0) 3.393  3.393  11.033 7.38e-07 ***
s(x1) 2.757  2.757 127.438  < 2e-16 ***
s(x2) 8.006  8.006 111.492  < 2e-16 ***
s(x3) 1.000  1.000   0.203    0.652    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.567   
  Scale est. = 4.9578    n = 400
> vis.gam(b$gam)
> 
> ## Prediction from gam object, optionally adding 
> ## in random effects. 
> 
> ## Extract random effects and make names more convenient...
> refa <- ranef(b$lme,level=5)
> rownames(refa) <- substr(rownames(refa),start=9,stop=20)
> refb <- ranef(b$lme,level=6)
> rownames(refb) <- substr(rownames(refb),start=9,stop=20)
> 
> ## make a prediction, with random effects zero...
> p0 <- predict(b$gam,data.frame(x0=.3,x1=.6,x2=.98,x3=.77))
> 
> ## add in effect for fa = "2" and fb="2/4"...
> p <- p0 + refa["2",1] + refb["2/4",1] 
> 
> ## and a "spatial" example...
> library(nlme);set.seed(1);n <- 100
> dat <- gamSim(2,n=n,scale=0) ## standard example
Bivariate smoothing example
> attach(dat)
> old.par<-par(mfrow=c(2,2))
> contour(truth$x,truth$z,truth$f)  ## true function
> f <- data$f                       ## true expected response
> ## Now simulate correlated errors...
> cstr <- corGaus(.1,form = ~x+z)  
> cstr <- Initialize(cstr,data.frame(x=data$x,z=data$z))
> V <- corMatrix(cstr) ## correlation matrix for data
> Cv <- chol(V)
> e <- t(Cv) %*% rnorm(n)*0.05 # correlated errors
> ## next add correlated simulated errors to expected values
> data$y <- f + e ## ... to produce response
> b<- gamm(y~s(x,z,k=50),correlation=corGaus(.1,form=~x+z),
+          data=data)
> plot(b$gam) # gamm fit accounting for correlation
> # overfits when correlation ignored.....  
> b1 <- gamm(y~s(x,z,k=50),data=data);plot(b1$gam) 
> b2 <- gam(y~s(x,z,k=50),data=data);plot(b2)
> par(old.par)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘dat’

> nameEx("gammals")
> ### * gammals
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gammals
> ### Title: Gamma location-scale model family
> ### Aliases: gammals
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> f3 <- function(x) 0 * x
> n <- 400;set.seed(9)
> x0 <- runif(n);x1 <- runif(n);
> x2 <- runif(n);x3 <- runif(n);
> mu <- exp((f0(x0)+f2(x2))/5)
> th <- exp(f1(x1)/2-2)
> y <- rgamma(n,shape=1/th,scale=mu*th)
> 
> b1 <- gam(list(y~s(x0)+s(x2),~s(x1)+s(x3)),family=gammals)
> plot(b1,pages=1)
> summary(b1)

Family: gammals 
Link function: identity log 

Formula:
y ~ s(x0) + s(x2)
~s(x1) + s(x3)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    0.89430    0.03489   25.63   <2e-16 ***
(Intercept).1  6.60736    0.06466  102.19   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df  Chi.sq p-value    
s(x0)   4.511  5.513  17.921 0.00326 ** 
s(x2)   6.591  7.696 214.767 < 2e-16 ***
s.1(x1) 2.485  3.074 233.362 < 2e-16 ***
s.1(x3) 1.608  1.991   1.055 0.60162    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 36.4%
-REML = 655.32  Scale est. = 1         n = 400
> gam.check(b1)

Method: REML   Optimizer: outer newton
full convergence after 5 iterations.
Gradient range [-0.0005943413,1.486465e-05]
(score 655.3212 & scale 1).
Hessian positive definite, eigenvalue range [0.08576749,2.116199].
Model rank =  38 / 38 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

          k'  edf k-index p-value
s(x0)   9.00 4.51    0.97    0.91
s(x2)   9.00 6.59    0.87    0.21
s.1(x1) 9.00 2.48    0.94    0.71
s.1(x3) 9.00 1.61    0.94    0.69
> plot(mu,fitted(b1)[,1]);abline(0,1,col=2)
> plot(log(th),fitted(b1)[,2]);abline(0,1,col=2)
> 
> 
> 
> 
> cleanEx()
> nameEx("gaulss")
> ### * gaulss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gaulss
> ### Title: Gaussian location-scale model family
> ### Aliases: gaulss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv);library(MASS)
> b <- gam(list(accel~s(times,k=20,bs="ad"),~s(times)),
+             data=mcycle,family=gaulss())
> summary(b) 

Family: gaulss 
Link function: identity logb 

Formula:
accel ~ s(times, k = 20, bs = "ad")
~s(times)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)   -25.3249     1.8530  -13.67   <2e-16 ***
(Intercept).1   2.6131     0.0632   41.35   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
              edf Ref.df Chi.sq p-value    
s(times)   10.584 11.951  548.1  <2e-16 ***
s.1(times)  7.265  8.233  272.7  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 98.7%
-REML = 580.04  Scale est. = 1         n = 133
> plot(b,pages=1,scale=0)
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("get.var")
> ### * get.var
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: get.var
> ### Title: Get named variable or evaluate expression from list or
> ###   data.frame
> ### Aliases: get.var
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> y <- 1:4;dat<-data.frame(x=5:10)
> get.var("x",dat)
[1]  5  6  7  8  9 10
> get.var("y",dat)
NULL
> get.var("x==6",dat)
NULL
> dat <- list(X=matrix(1:6,3,2))
> get.var("X",dat)
[1] 1 2 3 4 5 6
attr(,"matrix")
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("gevlss")
> ### * gevlss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gevlss
> ### Title: Generalized Extreme Value location-scale model family
> ### Aliases: gevlss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> Fi.gev <- function(z,mu,sigma,xi) {
+ ## GEV inverse cdf.
+   xi[abs(xi)<1e-8] <- 1e-8 ## approximate xi=0, by small xi
+   x <- mu + ((-log(z))^-xi-1)*sigma/xi
+ }
> 
> ## simulate test data...
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> set.seed(1)
> n <- 500
> x0 <- runif(n);x1 <- runif(n);x2 <- runif(n)
> mu <- f2(x2)
> rho <- f0(x0)
> xi <- (f1(x1)-4)/9
> y <- Fi.gev(runif(n),mu,exp(rho),xi)
> dat <- data.frame(y,x0,x1,x2);pairs(dat)
> 
> ## fit model....
> b <- gam(list(y~s(x2),~s(x0),~s(x1)),family=gevlss,data=dat)
> 
> ## same fit using the extended Fellner-Schall method which
> ## can provide improved numerical robustness... 
> b <- gam(list(y~s(x2),~s(x0),~s(x1)),family=gevlss,data=dat,
+          optimizer="efs")
> 
> ## plot and look at residuals...
> plot(b,pages=1,scale=0)
> summary(b)

Family: gevlss 
Link function: identity identity logit 

Formula:
y ~ s(x2)
~s(x0)
~s(x1)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    3.41804    0.11512  29.690  < 2e-16 ***
(Intercept).1  1.35086    0.03607  37.447  < 2e-16 ***
(Intercept).2  0.49110    0.11538   4.256 2.08e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df Chi.sq p-value    
s(x2)   7.687  8.513 655.26  <2e-16 ***
s.1(x0) 5.541  6.669 479.94  <2e-16 ***
s.2(x1) 2.152  2.667  46.96  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained =   NA%
-REML = 1481.5  Scale est. = 1         n = 500
> 
> par(mfrow=c(2,2))
> mu <- fitted(b)[,1];rho <- fitted(b)[,2]
> xi <- fitted(b)[,3]
> ## Get the predicted expected response... 
> fv <- mu + exp(rho)*(gamma(1-xi)-1)/xi
> rsd <- residuals(b)
> plot(fv,rsd);qqnorm(rsd)
> plot(fv,residuals(b,"pearson"))
> plot(fv,residuals(b,"response"))
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ginla")
> ### * ginla
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ginla
> ### Title: GAM Integrated Nested Laplace Approximation Newton Enhanced
> ### Aliases: ginla
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv); require(MASS)
Loading required package: MASS
> 
>   ## example using a scale location model for the motorcycle data. A simple plotting
>   ## routine is produced first...
> 
>   plot.inla <- function(x,inla,k=1,levels=c(.025,.1,.5,.9,.975),
+                lcol = c(2,4,4,4,2),lwd = c(1,1,2,1,1),lty=c(1,1,1,1,1),
+ 	       xlab="x",ylab="y",cex.lab=1.5) {
+     ## a simple effect plotter, when distributions of function values of
+     ## 1D smooths have been computed
+     require(splines)
+     p <- length(x) 
+     betaq <- matrix(0,length(levels),p) ## storage for beta quantiles 
+     for (i in 1:p) { ## work through x and betas
+       j <- i + k - 1
+       p <- cumsum(inla$density[j,])*(inla$beta[j,2]-inla$beta[j,1])
+       ## getting quantiles of function values...
+       betaq[,i] <- approx(p,y=inla$beta[j,],levels)$y
+     }
+     xg <- seq(min(x),max(x),length=200)
+     ylim <- range(betaq)
+     ylim <- 1.1*(ylim-mean(ylim))+mean(ylim)
+     for (j in 1:length(levels)) { ## plot the quantiles
+       din <- interpSpline(x,betaq[j,])
+       if (j==1) {
+         plot(xg,predict(din,xg)$y,ylim=ylim,type="l",col=lcol[j],
+              xlab=xlab,ylab=ylab,lwd=lwd[j],cex.lab=1.5,lty=lty[j])
+       } else lines(xg,predict(din,xg)$y,col=lcol[j],lwd=lwd[j],lty=lty[j])
+     }
+   } ## plot.inla
> 
>   ## set up the model with a `gam' call...
> 
>   G <- gam(list(accel~s(times,k=20,bs="ad"),~s(times)),
+             data=mcycle,family=gaulss(),fit=FALSE)
>   b <- gam(G=G,method="REML") ## regular GAM fit for comparison
> 
>   ## Now use ginla to get posteriors of estimated effect values
>   ## at evenly spaced times. Create A matrix for this...
>   
>   rat <- range(mcycle$times)
>   pd0 <- data.frame(times=seq(rat[1],rat[2],length=20))
>   X0 <- predict(b,newdata=pd0,type="lpmatrix")
>   X0[,21:30] <- 0
>   pd1 <- data.frame(times=seq(rat[1],rat[2],length=10))
>   X1 <- predict(b,newdata=pd1,type="lpmatrix")
>   X1[,1:20] <- 0
>   A <- rbind(X0,X1) ## A maps coefs to required function values
> 
>   ## call ginla. Set int to 1 for integrated version.
>   ## Set interactive = 1 or 2 to plot marginal posterior distributions
>   ## (red) and simple Gaussian approximation (black).
>  
>   inla <- ginla(G,A,int=0)
> 
>   par(mfrow=c(1,2),mar=c(5,5,1,1))
>   fv <- predict(b,se=TRUE) ## usual Gaussian approximation, for comparison
> 
>   ## plot inla mean smooth effect...
>   plot.inla(pd0$times,inla,k=1,xlab="time",ylab=expression(f[1](time))) 
Loading required package: splines
> 
>   ## overlay simple Gaussian equivalent (in grey) ...
>   points(mcycle$times,mcycle$accel,col="grey")
>   lines(mcycle$times,fv$fit[,1],col="grey",lwd=2)
>   lines(mcycle$times,fv$fit[,1]+2*fv$se.fit[,1],lty=2,col="grey",lwd=2)
>   lines(mcycle$times,fv$fit[,1]-2*fv$se.fit[,1],lty=2,col="grey",lwd=2)
> 
>   ## same for log sd smooth...
>   plot.inla(pd1$times,inla,k=21,xlab="time",ylab=expression(f[2](time)))
>   lines(mcycle$times,fv$fit[,2],col="grey",lwd=2)
>   lines(mcycle$times,fv$fit[,2]+2*fv$se.fit[,2],col="grey",lty=2,lwd=2)
>   lines(mcycle$times,fv$fit[,2]-2*fv$se.fit[,2],col="grey",lty=2,lwd=2)
> 
>   ## ... notice some real differences for the log sd smooth, especially
>   ## at the lower and upper ends of the time interval.
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:splines’, ‘package:MASS’

> nameEx("gumbls")
> ### * gumbls
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gumbls
> ### Title: Gumbel location-scale model family
> ### Aliases: gumbls
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 400;set.seed(9)
> x0 <- runif(n);x1 <- runif(n);
> x2 <- runif(n);x3 <- runif(n);
> mu <- f0(x0)+f1(x1)
> beta <- exp(f2(x2)/5)
> y <- mu - beta*log(-log(runif(n))) ## Gumbel quantile function
> 
> b <- gam(list(y~s(x0)+s(x1),~s(x2)+s(x3)),family=gumbls)
> plot(b,pages=1,scale=0)
> summary(b)

Family: gumbls 
Link function: identity log 

Formula:
y ~ s(x0) + s(x1)
~s(x2) + s(x3)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    4.26458    0.07932   53.77   <2e-16 ***
(Intercept).1  7.62294    0.03942  193.38   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df  Chi.sq p-value    
s(x0)   3.611  4.464  67.674  <2e-16 ***
s(x1)   3.880  4.778 586.049  <2e-16 ***
s.1(x2) 6.579  7.654 198.637  <2e-16 ***
s.1(x3) 1.000  1.001   0.051   0.822    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained =   NA%
-REML = 914.32  Scale est. = 1         n = 400
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 16 iterations.
Gradient range [-0.0002240745,2.45483e-06]
(score 914.3217 & scale 1).
Hessian positive definite, eigenvalue range [0.0002239976,2.182041].
Model rank =  38 / 38 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

          k'  edf k-index p-value
s(x0)   9.00 3.61    0.88    0.20
s(x1)   9.00 3.88    0.88    0.26
s.1(x2) 9.00 6.58    0.93    0.77
s.1(x3) 9.00 1.00    0.87    0.20
> 
> 
> 
> 
> cleanEx()
> nameEx("identifiability")
> ### * identifiability
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: identifiability
> ### Title: Identifiability constraints
> ### Aliases: identifiability
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> ## Example of three groups, each with a different smooth dependence on x
> ## but each starting at the same value...
> require(mgcv)
> set.seed(53)
> n <- 100;x <- runif(3*n);z <- runif(3*n)
> fac <- factor(rep(c("a","b","c"),each=100))
> y <- c(sin(x[1:100]*4),exp(3*x[101:200])/10-.1,exp(-10*(x[201:300]-.5))/
+        (1+exp(-10*(x[201:300]-.5)))-0.9933071) + z*(1-z)*5 + rnorm(100)*.4
> 
> ## 'pc' used to constrain smooths to 0 at x=0...
> b <- gam(y~s(x,by=fac,pc=0)+s(z)) 
> plot(b,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("in.out")
> ### * in.out
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: in.out
> ### Title: Which of a set of points lie within a polygon defined region
> ### Aliases: in.out
> 
> ### ** Examples
> 
> library(mgcv)
> data(columb.polys)
> bnd <- columb.polys[[2]]
> plot(bnd,type="n")
> polygon(bnd)
> x <- seq(7.9,8.7,length=20)
> y <- seq(13.7,14.3,length=20)
> gr <- as.matrix(expand.grid(x,y))
> inside <- in.out(bnd,gr)
> points(gr,col=as.numeric(inside)+1)
> 
> 
> 
> cleanEx()
> nameEx("inSide")
> ### * inSide
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inSide
> ### Title: Are points inside boundary?
> ### Aliases: inSide
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> m <- 300;n <- 150
> xm <- seq(-1,4,length=m);yn<-seq(-1,1,length=n)
> x <- rep(xm,n);y<-rep(yn,rep(m,n))
> er <- matrix(fs.test(x,y),m,n)
> bnd <- fs.boundary()
> in.bnd <- inSide(bnd,x,y)
> plot(x,y,col=as.numeric(in.bnd)+1,pch=".")
> lines(bnd$x,bnd$y,col=3)
> points(x,y,col=as.numeric(in.bnd)+1,pch=".")
> ## check boundary details ...
> plot(x,y,col=as.numeric(in.bnd)+1,pch=".",ylim=c(-1,0),xlim=c(3,3.5))
> lines(bnd$x,bnd$y,col=3)
> points(x,y,col=as.numeric(in.bnd)+1,pch=".")
> 
> 
> 
> 
> cleanEx()
> nameEx("jagam")
> ### * jagam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: jagam
> ### Title: Just Another Gibbs Additive Modeller: JAGS support for mgcv.
> ### Aliases: jagam sim2jam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## the following illustrates a typical workflow. To run the 
> ## 'Not run' code you need rjags (and JAGS) to be installed.
> require(mgcv)
>   
> set.seed(2) ## simulate some data... 
> n <- 400
> dat <- gamSim(1,n=n,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> ## regular gam fit for comparison...
> b0 <- gam(y~s(x0)+s(x1) + s(x2)+s(x3),data=dat,method="REML")
> 
> ## Set directory and file name for file containing jags code.
> ## In real use you would *never* use tempdir() for this. It is
> ## only done here to keep CRAN happy, and avoid any chance of
> ## an accidental overwrite. Instead you would use
> ## setwd() to set an appropriate working directory in which
> ## to write the file, and just set the file name to what you
> ## want to call it (e.g. "test.jags" here). 
> 
> jags.file <- paste(tempdir(),"/test.jags",sep="") 
> 
> ## Set up JAGS code and data. In this one might want to diagonalize
> ## to use conjugate samplers. Usually call 'setwd' first, to set
> ## directory in which model file ("test.jags") will be written.
> 
> jd <- jagam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,file=jags.file,
+             sp.prior="gamma",diagonalize=TRUE)
> 
> ## In normal use the model in "test.jags" would now be edited to add 
> ## the non-standard stochastic elements that require use of JAGS....
> 
> ## Not run: 
> ##D require(rjags)
> ##D load.module("glm") ## improved samplers for GLMs often worth loading
> ##D jm <-jags.model(jags.file,data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
> ##D list.samplers(jm)
> ##D sam <- jags.samples(jm,c("b","rho","scale"),n.iter=10000,thin=10)
> ##D jam <- sim2jam(sam,jd$pregam)
> ##D plot(jam,pages=1)
> ##D jam
> ##D pd <- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
> ##D fv <- predict(jam,newdata=pd)
> ##D ## and some minimal checking...
> ##D require(coda)
> ##D effectiveSize(as.mcmc.list(sam$b))
> ## End(Not run)
> 
> ## a gamma example...
> set.seed(1); n <- 400
> dat <- gamSim(1,n=n,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> scale <- .5; Ey <- exp(dat$f/2)
> dat$y <- rgamma(n,shape=1/scale,scale=Ey*scale)
> jd <- jagam(y~s(x0)+te(x1,x2)+s(x3),data=dat,family=Gamma(link=log),
+             file=jags.file,sp.prior="log.uniform")
> 
> ## In normal use the model in "test.jags" would now be edited to add 
> ## the non-standard stochastic elements that require use of JAGS....
> 
> ## Not run: 
> ##D require(rjags)
> ##D ## following sets random seed, but note that under JAGS 3.4 many
> ##D ## models are still not fully repeatable (JAGS 4 should fix this)
> ##D jd$jags.ini$.RNG.name <- "base::Mersenne-Twister" ## setting RNG
> ##D jd$jags.ini$.RNG.seed <- 6 ## how to set RNG seed
> ##D jm <-jags.model(jags.file,data=jd$jags.data,inits=jd$jags.ini,n.chains=1)
> ##D list.samplers(jm)
> ##D sam <- jags.samples(jm,c("b","rho","scale","mu"),n.iter=10000,thin=10)
> ##D jam <- sim2jam(sam,jd$pregam)
> ##D plot(jam,pages=1)
> ##D jam
> ##D pd <- data.frame(x0=c(.5,.6),x1=c(.4,.2),x2=c(.8,.4),x3=c(.1,.1))
> ##D fv <- predict(jam,newdata=pd)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("k.check")
> ### * k.check
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: k.check
> ### Title: Checking smooth basis dimension
> ### Aliases: k.check
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> dat <- gamSim(1,n=200)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> k.check(b)
      k'      edf   k-index p-value
s(x0)  9 2.318172 0.9959628  0.4800
s(x1)  9 2.305695 0.9693887  0.2825
s(x2)  9 7.654740 0.9605490  0.2700
s(x3)  9 1.232618 1.0372831  0.6775
> 
> 
> 
> cleanEx()
> nameEx("ldTweedie")
> ### * ldTweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ldTweedie
> ### Title: Log Tweedie density evaluation
> ### Aliases: ldTweedie
> ### Keywords: models regression
> 
> ### ** Examples
> 
>   library(mgcv)
>   ## convergence to Poisson illustrated
>   ## notice how p>1.1 is OK
>   y <- seq(1e-10,10,length=1000)
>   p <- c(1.0001,1.001,1.01,1.1,1.2,1.5,1.8,2)
>   phi <- .5
>   fy <- exp(ldTweedie(y,mu=2,p=p[1],phi=phi)[,1])
>   plot(y,fy,type="l",ylim=c(0,3),main="Tweedie density as p changes")
>   for (i in 2:length(p)) {
+     fy <- exp(ldTweedie(y,mu=2,p=p[i],phi=phi)[,1])
+     lines(y,fy,col=i)
+   }
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("linear.functional.terms")
> ### * linear.functional.terms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: linear.functional.terms
> ### Title: Linear functionals of a smooth in GAMs
> ### Aliases: linear.functional.terms function.predictors signal.regression
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ### matrix argument `linear operator' smoothing
> library(mgcv)
> set.seed(0)
> 
> ###############################
> ## simple summation example...#
> ###############################
> 
> n<-400
> sig<-2
> x <- runif(n, 0, .9)
> f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> x1 <- x + .1
> 
> f <- f2(x) + f2(x1)  ## response is sum of f at two adjacent x values 
> y <- f + rnorm(n)*sig
> 
> X <- matrix(c(x,x1),n,2) ## matrix covariate contains both x values
> b <- gam(y~s(X))         
> 
> plot(b)  ## reconstruction of f
> plot(f,fitted(b))
> 
> ## example of prediction with summation convention...
> predict(b,list(X=X[1:3,]))
        1         2         3 
 1.159171 15.150664 10.027513 
> 
> ## example of prediction that simply evaluates smooth (no summation)...
> predict(b,data.frame(X=c(.2,.3,.7))) 
        1         2         3 
12.158338 11.657881  6.872067 
> 
> ######################################################################
> ## Simple random effect model example.
> ## model: y[i] = f(x[i]) + b[k[i]] - b[j[i]] + e[i]
> ## k[i] and j[i] index levels of i.i.d. random effects, b.
> ######################################################################
> 
> set.seed(7)
> n <- 200
> x <- runif(n) ## a continuous covariate
> 
> ## set up a `factor matrix'...
> fac <- factor(sample(letters,n*2,replace=TRUE))
> dim(fac) <- c(n,2)
> 
> ## simulate data from such a model...
> nb <- length(levels(fac))
> b <- rnorm(nb)
> y <- 20*(x-.3)^4 + b[fac[,1]] - b[fac[,2]] + rnorm(n)*.5
> 
> L <- matrix(-1,n,2);L[,1] <- 1 ## the differencing 'by' variable 
> 
> mod <- gam(y ~ s(x) + s(fac,by=L,bs="re"),method="REML")
> gam.vcomp(mod)

Standard deviations and 0.95 confidence intervals:

            std.dev     lower      upper
s(x)     13.5440523 7.0835453 25.8968277
s(fac):L  0.9976386 0.7522712  1.3230372
scale     0.4885647 0.4389027  0.5438461

Rank: 3/3
> plot(mod,page=1)
> 
> ## example of prediction using matrices...
> dat <- list(L=L[1:20,],fac=fac[1:20,],x=x[1:20],y=y[1:20])
> predict(mod,newdata=dat)
         1          2          3          4          5          6          7 
 5.9902893  2.9240468 -3.4194203  2.3879203  0.3303235  1.2299799 -2.7043569 
         8          9         10         11         12         13         14 
 4.5336424 -0.4248181  1.7320775 -0.3087738  0.7897546  0.6222690  0.3148356 
        15         16         17         18         19         20 
 0.8057572  2.2307314 -0.2481124 -3.0469777  3.4895282 -0.8621211 
> 
> 
> ######################################################################
> ## multivariate integral example. Function `test1' will be integrated# 
> ## (by midpoint quadrature) over 100 equal area sub-squares covering # 
> ## the unit square. Noise is added to the resulting simulated data.  #
> ## `test1' is estimated from the resulting data using two alternative#
> ## smooths.                                                          #
> ######################################################################
> 
> test1 <- function(x,z,sx=0.3,sz=0.4)
+   { (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+     0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+   }
> 
> ## create quadrature (integration) grid, in useful order
> ig <- 5 ## integration grid within square
> mx <- mz <- (1:ig-.5)/ig
> ix <- rep(mx,ig);iz <- rep(mz,rep(ig,ig))
> 
> og <- 10 ## observarion grid
> mx <- mz <- (1:og-1)/og
> ox <- rep(mx,og);ox <- rep(ox,rep(ig^2,og^2))
> oz <- rep(mz,rep(og,og));oz <- rep(oz,rep(ig^2,og^2))
> 
> x <- ox + ix/og;z <- oz + iz/og ## full grid, subsquare by subsquare
> 
> ## create matrix covariates...
> X <- matrix(x,og^2,ig^2,byrow=TRUE)
> Z <- matrix(z,og^2,ig^2,byrow=TRUE)
> 
> ## create simulated test data...
> dA <- 1/(og*ig)^2  ## quadrature square area
> F <- test1(X,Z)    ## evaluate on grid
> f <- rowSums(F)*dA ## integrate by midpoint quadrature
> y <- f + rnorm(og^2)*5e-4 ## add noise
> ## ... so each y is a noisy observation of the integral of `test1'
> ## over a 0.1 by 0.1 sub-square from the unit square
> 
> ## Now fit model to simulated data...
> 
> L <- X*0 + dA
> 
> ## ... let F be the matrix of the smooth evaluated at the x,z values
> ## in matrices X and Z. rowSums(L*F) gives the model predicted
> ## integrals of `test1' corresponding to the observed `y'
> 
> L1 <- rowSums(L) ## smooths are centred --- need to add in L%*%1
> 
> ## fit models to reconstruct `test1'....
> 
> b <- gam(y~s(X,Z,by=L)+L1-1)   ## (L1 and const are confounded here)
> b1 <- gam(y~te(X,Z,by=L)+L1-1) ## tensor product alternative
> 
> ## plot results...
> 
> old.par<-par(mfrow=c(2,2))
> x<-runif(n);z<-runif(n);
> xs<-seq(0,1,length=30);zs<-seq(0,1,length=30)
> pr<-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth<-matrix(test1(pr$x,pr$z),30,30)
> contour(xs,zs,truth)
> plot(b)
> vis.gam(b,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")
> vis.gam(b1,view=c("X","Z"),cond=list(L1=1,L=1),plot.type="contour")
> 
> ####################################
> ## A "signal" regression example...#
> ####################################
> 
> rf <- function(x=seq(0,1,length=100)) {
+ ## generates random functions...
+   m <- ceiling(runif(1)*5) ## number of components
+   f <- x*0;
+   mu <- runif(m,min(x),max(x));sig <- (runif(m)+.5)*(max(x)-min(x))/10
+   for (i in 1:m) f <- f+ dnorm(x,mu[i],sig[i])
+   f
+ }
> 
> x <- seq(0,1,length=100) ## evaluation points
> 
> ## example functional predictors...
> par(mfrow=c(3,3));for (i in 1:9) plot(x,rf(x),type="l",xlab="x")
> 
> ## simulate 200 functions and store in rows of L...
> L <- matrix(NA,200,100) 
> for (i in 1:200) L[i,] <- rf()  ## simulate the functional predictors
> 
> f2 <- function(x) { ## the coefficient function
+   (0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10)/10 
+ }
> 
> f <- f2(x) ## the true coefficient function
> 
> y <- L%*%f + rnorm(200)*20 ## simulated response data
> 
> ## Now fit the model E(y) = L%*%f(x) where f is a smooth function.
> ## The summation convention is used to evaluate smooth at each value
> ## in matrix X to get matrix F, say. Then rowSum(L*F) gives E(y).
> 
> ## create matrix of eval points for each function. Note that
> ## `smoothCon' is smart and will recognize the duplication...
> X <- matrix(x,200,100,byrow=TRUE) 
> 
> b <- gam(y~s(X,by=L,k=20)) 
> par(mfrow=c(1,1))
> plot(b,shade=TRUE);lines(x,f,col=2)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ls.size")
> ### * ls.size
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ls.size
> ### Title: Size of list elements
> ### Aliases: ls.size
> 
> ### ** Examples
> 
> library(mgcv)
> b <- list(M=matrix(runif(100),10,10),quote=
+ "The world is ruled by idiots because only an idiot would want to rule the world.",
+ fam=binomial())
> ls.size(b)
     M  quote    fam 
  1016    232 172424 
> 
> 
> 
> cleanEx()
> nameEx("magic")
> ### * magic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: magic
> ### Title: Stable Multiple Smoothing Parameter Estimation by GCV or UBRE
> ### Aliases: magic
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Use `magic' for a standard additive model fit ... 
>    library(mgcv)
>    set.seed(1);n <- 200;sig <- 1
>    dat <- gamSim(1,n=n,scale=sig)
Gu & Wahba 4 term additive model
>    k <- 30
> ## set up additive model
>    G <- gam(y~s(x0,k=k)+s(x1,k=k)+s(x2,k=k)+s(x3,k=k),fit=FALSE,data=dat)
> ## fit using magic (and gam default tolerance)
>    mgfit <- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,
+                   control=list(tol=1e-7,step.half=15))
> ## and fit using gam as consistency check
>    b <- gam(G=G)
>    mgfit$sp;b$sp  # compare smoothing parameter estimates
[1] 1.232642e+00 3.131825e+00 6.530096e-03 2.606228e+07
       s(x0)        s(x1)        s(x2)        s(x3) 
1.232642e+00 3.131825e+00 6.530096e-03 2.606228e+07 
>    edf <- magic.post.proc(G$X,mgfit,G$w)$edf # get e.d.f. per param
>    range(edf-b$edf)  # compare
[1] 0 0
> 
> ## p>n example... fit model to first 100 data only, so more
> ## params than data...
> 
>    mgfit <- magic(G$y[1:100],G$X[1:100,],G$sp,G$S,G$off,rank=G$rank)
>    edf <- magic.post.proc(G$X[1:100,],mgfit,G$w[1:100])$edf
> 
> ## constrain first two smooths to have identical smoothing parameters
>    L <- diag(3);L <- rbind(L[1,],L)
>    mgfit <- magic(G$y,G$X,rep(-1,3),G$S,G$off,L=L,rank=G$rank,C=G$C)
> 
> ## Now a correlated data example ... 
>     library(nlme)
> ## simulate truth
>     set.seed(1);n<-400;sig<-2
>     x <- 0:(n-1)/(n-1)
>     f <- 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10
> ## produce scaled covariance matrix for AR1 errors...
>     V <- corMatrix(Initialize(corAR1(.6),data.frame(x=x)))
>     Cv <- chol(V)  # t(Cv)%*%Cv=V
> ## Simulate AR1 errors ...
>     e <- t(Cv)%*%rnorm(n,0,sig) # so cov(e) = V * sig^2
> ## Observe truth + AR1 errors
>     y <- f + e 
> ## GAM ignoring correlation
>     par(mfrow=c(1,2))
>     b <- gam(y~s(x,k=20))
>     plot(b);lines(x,f-mean(f),col=2);title("Ignoring correlation")
> ## Fit smooth, taking account of *known* correlation...
>     w <- solve(t(Cv)) # V^{-1} = w'w
>     ## Use `gam' to set up model for fitting...
>     G <- gam(y~s(x,k=20),fit=FALSE)
>     ## fit using magic, with weight *matrix*
>     mgfit <- magic(G$y,G$X,G$sp,G$S,G$off,rank=G$rank,C=G$C,w=w)
> ## Modify previous gam object using new fit, for plotting...    
>     mg.stuff <- magic.post.proc(G$X,mgfit,w)
>     b$edf <- mg.stuff$edf;b$Vp <- mg.stuff$Vb
>     b$coefficients <- mgfit$b 
>     plot(b);lines(x,f-mean(f),col=2);title("Known correlation")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("mgcv-package")
> ### * mgcv-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgcv.package
> ### Title: Mixed GAM Computation Vehicle with GCV/AIC/REML smoothness
> ###   estimation and GAMMs by REML/PQL
> ### Aliases: mgcv.package mgcv-package mgcv
> ### Keywords: package models smooth regression
> 
> ### ** Examples
> 
> ## see examples for gam and gamm
> 
> 
> 
> cleanEx()
> nameEx("mgcv-parallel")
> ### * mgcv-parallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mgcv.parallel
> ### Title: Parallel computation in mgcv.
> ### Aliases: mgcv.parallel
> ### Keywords: package models smooth regression
> 
> ### ** Examples
> 
> ## illustration of multi-threading with gam...
> 
> require(mgcv);set.seed(9)
> dat <- gamSim(1,n=2000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> k <- 12;bs <- "cr";ctrl <- list(nthreads=2)
> 
> system.time(b1<-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k)
+             ,family=poisson,data=dat,method="REML"))[3]
elapsed 
  0.139 
> 
> system.time(b2<-gam(y~s(x0,bs=bs)+s(x1,bs=bs)+s(x2,bs=bs,k=k),
+             family=poisson,data=dat,method="REML",control=ctrl))[3]
elapsed 
  0.405 
> 
> ## Poisson example on a cluster with 'bam'. 
> ## Note that there is some overhead in initializing the 
> ## computation on the cluster, associated with loading 
> ## the Matrix package on each node. Sample sizes are low
> ## here to keep example quick -- for such a small model
> ## little or no advantage is likely to be seen.
> k <- 13;set.seed(9)
> dat <- gamSim(1,n=6000,dist="poisson",scale=.1)
Gu & Wahba 4 term additive model
> ## Alternative, better scaling example, using the discrete option with bam...
> 
> system.time(b4 <- bam(y ~ s(x0,bs=bs,k=7)+s(x1,bs=bs,k=7)+s(x2,bs=bs,k=k)
+             ,data=dat,family=poisson(),discrete=TRUE,nthreads=2))
   user  system elapsed 
  1.579   0.401   1.282 
> 
> 
> 
> 
> cleanEx()
> nameEx("missing.data")
> ### * missing.data
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: missing.data
> ### Title: Missing data in GAMs
> ### Aliases: missing.data
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## The example takes a couple of minutes to run...
> 
> 
> 
> cleanEx()
> nameEx("model.matrix.gam")
> ### * model.matrix.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: model.matrix.gam
> ### Title: Extract model matrix from GAM fit
> ### Aliases: model.matrix.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 15
> x <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=6),knots=list(x=seq(0,1,length=6)))
> model.matrix(mod)
   (Intercept)      s(x).1      s(x).2      s(x).3       s(x).4
1            1  0.64005926  0.28713835 -0.22520339 -0.074786976
2            1  0.01026326  0.89582182 -0.18450287 -0.069653495
3            1 -0.23534306  0.03365477  0.81648663 -0.244122129
4            1 -0.59048631 -0.16217974 -0.46763064  0.186386299
5            1  0.74479124 -0.12347968 -0.19396253 -0.203204806
6            1 -0.56089257 -0.14955417 -0.45727097  0.269751709
7            1 -0.66201291 -0.21573683 -0.48187308 -0.120102457
8            1 -0.11632150 -0.22703879  0.70081516  0.201332878
9            1 -0.15249390 -0.18723726  0.81733754 -0.005827532
10           1 -0.20178755 -0.37543794 -0.35493419 -0.581672810
11           1  0.75078035 -0.10086471 -0.19420184 -0.191330859
12           1  0.66625477 -0.23439198 -0.20146231 -0.281093403
13           1 -0.09734475 -0.22684275  0.56076919  0.384389211
14           1 -0.05556123  0.92102724 -0.15332991 -0.083659778
15           1 -0.13990510 -0.13487834  0.01896321  0.813594149
attr(,"model.offset")
[1] 0
> 
> 
> 
> cleanEx()
> nameEx("mono.con")
> ### * mono.con
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mono.con
> ### Title: Monotonicity constraints for a cubic regression spline
> ### Aliases: mono.con
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## see ?pcls
> 
> 
> 
> cleanEx()
> nameEx("mroot")
> ### * mroot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mroot
> ### Title: Smallest square root of matrix
> ### Aliases: mroot
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   set.seed(0)
>   a <- matrix(runif(24),6,4)
>   A <- a%*%t(a) ## A is +ve semi-definite, rank 4
>   B <- mroot(A) ## default pivoted choleski method
>   tol <- 100*.Machine$double.eps
>   chol.err <- max(abs(A-B%*%t(B)));chol.err
[1] 4.440892e-16
>   if (chol.err>tol) warning("mroot (chol) suspect")
>   B <- mroot(A,method="svd") ## svd method
>   svd.err <- max(abs(A-B%*%t(B)));svd.err
[1] 3.108624e-15
>   if (svd.err>tol) warning("mroot (svd) suspect")  
> 
> 
> 
> cleanEx()
> nameEx("multinom")
> ### * multinom
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multinom
> ### Title: GAM multinomial logistic regression
> ### Aliases: multinom
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(6)
> ## simulate some data from a three class model
> n <- 1000
> f1 <- function(x) sin(3*pi*x)*exp(-x)
> f2 <- function(x) x^3
> f3 <- function(x) .5*exp(-x^2)-.2
> f4 <- function(x) 1
> x1 <- runif(n);x2 <- runif(n)
> eta1 <- 2*(f1(x1) + f2(x2))-.5
> eta2 <- 2*(f3(x1) + f4(x2))-1
> p <- exp(cbind(0,eta1,eta2))
> p <- p/rowSums(p) ## prob. of each category 
> cp <- t(apply(p,1,cumsum)) ## cumulative prob.
> ## simulate multinomial response with these probabilities
> ## see also ?rmultinom
> y <- apply(cp,1,function(x) min(which(x>runif(1))))-1
> ## plot simulated data...
> plot(x1,x2,col=y+3)
> 
> ## now fit the model...
> b <- gam(list(y~s(x1)+s(x2),~s(x1)+s(x2)),family=multinom(K=2))
> plot(b,pages=1)
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-1.260955e-05,7.167128e-05]
(score 729.5551 & scale 1).
Hessian positive definite, eigenvalue range [0.1937807,2.149332].
Model rank =  38 / 38 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

          k'  edf k-index p-value
s(x1)   9.00 6.75    1.03    0.88
s(x2)   9.00 2.70    0.96    0.15
s.1(x1) 9.00 2.34    1.03    0.84
s.1(x2) 9.00 1.87    0.96    0.16
> 
> ## now a simple classification plot...
> expand.grid(x1=seq(0,1,length=40),x2=seq(0,1,length=40)) -> gr
> pp <- predict(b,newdata=gr,type="response")
> pc <- apply(pp,1,function(x) which(max(x)==x)[1])-1
> plot(gr,col=pc+3,pch=19)
> 
> 
> 
> 
> cleanEx()
> nameEx("mvn")
> ### * mvn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mvn
> ### Title: Multivariate normal additive models
> ### Aliases: mvn
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data...
> V <- matrix(c(2,1,1,2),2,2)
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 300
> x0 <- runif(n);x1 <- runif(n);
> x2 <- runif(n);x3 <- runif(n)
> y <- matrix(0,n,2)
> for (i in 1:n) {
+   mu <- c(f0(x0[i])+f1(x1[i]),f2(x2[i]))
+   y[i,] <- rmvn(1,mu,V)
+ }
> dat <- data.frame(y0=y[,1],y1=y[,2],x0=x0,x1=x1,x2=x2,x3=x3)
> 
> ## fit model...
> 
> b <- gam(list(y0~s(x0)+s(x1),y1~s(x2)+s(x3)),family=mvn(d=2),data=dat)
> b

Family: Multivariate normal 
Link function: 

Formula:
y0 ~ s(x0) + s(x1)
y1 ~ s(x2) + s(x3)

Estimated degrees of freedom:
3.73 3.79 8.47 1.00  total = 21.99 

REML score: 537.5186     
> summary(b)

Family: Multivariate normal 
Link function: 

Formula:
y0 ~ s(x0) + s(x1)
y1 ~ s(x2) + s(x3)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    4.59554    0.08571   53.62   <2e-16 ***
(Intercept).1  3.07060    0.08636   35.56   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df Chi.sq p-value    
s(x0)   3.729  4.620   69.0  <2e-16 ***
s(x1)   3.793  4.682  706.7  <2e-16 ***
s.1(x2) 8.471  8.920 1401.5  <2e-16 ***
s.1(x3) 1.001  1.002    0.0   0.999    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 79.6%
-REML = 537.52  Scale est. = 1         n = 300
> plot(b,pages=1)
> solve(crossprod(b$family$data$R)) ## estimated cov matrix
         [,1]     [,2]
[1,] 2.204048 1.185759
[2,] 1.185759 2.237386
> 
> 
> 
> 
> cleanEx()
> nameEx("negbin")
> ### * negbin
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: negbin
> ### Title: GAM negative binomial families
> ### Aliases: negbin nb
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3)
> n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> g <- exp(dat$f/5)
> 
> ## negative binomial data... 
> dat$y <- rnbinom(g,size=3,mu=g)
> ## known theta fit ...
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=negbin(3),data=dat)
> plot(b0,pages=1)
> print(b0)

Family: Negative Binomial(3) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.04 2.47 6.35 1.00  total = 12.85 

UBRE score: 0.187248     
> 
> ## same with theta estimation...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(),data=dat)
> plot(b,pages=1)
> print(b)

Family: Negative Binomial(2.693) 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.27 2.76 6.92 1.00  total = 13.95 

REML score: 1062.259     
> b$family$getTheta(TRUE) ## extract final theta estimate
[1] 2.692607
> 
> 
> ## another example...
> set.seed(1)
> f <- dat$f
> f <- f - min(f)+5;g <- f^2/10
> dat$y <- rnbinom(g,size=3,mu=g)
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=nb(link="sqrt"),
+          data=dat,method="REML") 
> plot(b2,pages=1)
> print(b2)

Family: Negative Binomial(2.786) 
Link function: sqrt 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.49 2.08 7.29 1.85  total = 14.71 

REML score: 1356.263     
> rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("new.name")
> ### * new.name
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: new.name
> ### Title: Obtain a name for a new variable that is not already in use
> ### Aliases: new.name
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> old <- c("a","tuba","is","tubby")
> new.name("tubby",old)
[1] "tubby.0"
> 
> 
> 
> cleanEx()
> nameEx("notExp")
> ### * notExp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: notExp
> ### Title: Functions for better-than-log positive parameterization
> ### Aliases: notExp notLog
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Illustrate the notExp function: 
> ## less steep than exp, but still monotonic.
> require(mgcv)
> x <- -100:100/10
> op <- par(mfrow=c(2,2))
> plot(x,notExp(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> x <- x/4
> plot(x,notExp(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> par(op)
> range(notLog(notExp(x))-x) # show that inverse works!
[1] -4.440892e-16  4.440892e-16
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("notExp2")
> ### * notExp2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: notExp2
> ### Title: Alternative to log parameterization for variance components
> ### Aliases: notExp2 notLog2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Illustrate the notExp2 function:
> require(mgcv)
> x <- seq(-50,50,length=1000)
> op <- par(mfrow=c(2,2))
> plot(x,notExp2(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp2(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> x <- x/4
> plot(x,notExp2(x),type="l")
> lines(x,exp(x),col=2)
> plot(x,log(notExp2(x)),type="l")
> lines(x,log(exp(x)),col=2) # redundancy intended
> par(op)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("null.space.dimension")
> ### * null.space.dimension
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: null.space.dimension
> ### Title: The basis of the space of un-penalized functions for a TPRS
> ### Aliases: null.space.dimension
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> null.space.dimension(2,0)
[1] 3
> 
> 
> 
> cleanEx()
> nameEx("ocat")
> ### * ocat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ocat
> ### Title: GAM ordered categorical family
> ### Aliases: ocat ordered.categorical
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some ordered categorical data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$f <- dat$f - mean(dat$f)
> 
> alpha <- c(-Inf,-1,0,5,Inf)
> R <- length(alpha)-1
> y <- dat$f
> u <- runif(n)
> u <- dat$f + log(u/(1-u)) 
> for (i in 1:R) {
+   y[u > alpha[i]&u <= alpha[i+1]] <- i
+ }
> dat$y <- y
> 
> ## plot the data...
> par(mfrow=c(2,2))
> with(dat,plot(x0,y));with(dat,plot(x1,y))
> with(dat,plot(x2,y));with(dat,plot(x3,y))
> 
> ## fit ocat model to data...
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ocat(R=R),data=dat)
> b

Family: Ordered Categorical(-1,0.07,5.15) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.32 3.11 7.81 1.59  total = 16.84 

REML score: 283.8245     
> plot(b,pages=1)
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 5 iterations.
Gradient range [-4.800542e-06,4.973802e-08]
(score 283.8245 & scale 1).
Hessian positive definite, eigenvalue range [0.09977744,150.0804].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value
s(x0) 9.00 3.32    0.94    0.14
s(x1) 9.00 3.11    1.06    0.90
s(x2) 9.00 7.81    0.95    0.12
s(x3) 9.00 1.59    0.97    0.28
> summary(b)

Family: Ordered Categorical(-1,0.07,5.15) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)   0.1221     0.1319   0.926    0.354

Approximate significance of smooth terms:
        edf Ref.df  Chi.sq  p-value    
s(x0) 3.317  4.116  21.623 0.000264 ***
s(x1) 3.115  3.871 188.368  < 2e-16 ***
s(x2) 7.814  8.616 402.300  < 2e-16 ***
s(x3) 1.593  1.970   0.936 0.641757    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 57.6%
-REML = 283.82  Scale est. = 1         n = 400
> b$family$getTheta(TRUE) ## the estimated cut points
[1] -1.00000000  0.07295739  5.14663505
> 
> ## predict probabilities of being in each category
> predict(b,dat[1:2,],type="response",se=TRUE)
$fit
        [,1]        [,2]        [,3]         [,4]
1 0.99085777 0.005996704 0.003125774 0.0000197507
2 0.06793525 0.107745442 0.795787468 0.0285318416

$se.fit
         [,1]       [,2]        [,3]         [,4]
1 0.006829264 0.00446533 0.002349044 1.488965e-05
2 0.028948637 0.03725874 0.053535372 1.267200e-02

> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("one.se.rule")
> ### * one.se.rule
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: one.se.rule
> ### Title: The one standard error rule for smoother models
> ### Aliases: one.se.rule
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> set.seed(2) ## simulate some data...
> dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML")
> b

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.89 

REML score: 861.1296     
> ## only the first 3 smoothing parameters are candidates for
> ## increasing here...
> V <- sp.vcov(b)[1:3,1:3] ## the approx cov matrix of sps
> d <- diag(V)^.5          ## sp se.
> ## compute the log smoothing parameter step...
> d <- sqrt(2*length(d))/d
> sp <- b$sp ## extract original sp estimates
> sp[1:3] <- sp[1:3]*exp(d) ## apply the step
> ## refit with the increased smoothing parameters...
> b1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="REML",sp=sp)
> b;b1 ## compare fits

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.02 2.84 8.02 1.00  total = 15.89 

REML score: 861.1296     

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
1.62 1.59 3.49 1.00  total = 8.7 

REML score: 926.1002     
> 
> 
> 
> cleanEx()
> nameEx("pcls")
> ### * pcls
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pcls
> ### Title: Penalized Constrained Least Squares Fitting
> ### Aliases: pcls
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> # first an un-penalized example - fit E(y)=a+bx subject to a>0
> set.seed(0)
> n <- 100
> x <- runif(n); y <- x - 0.2 + rnorm(n)*0.1
> M <- list(X=matrix(0,n,2),p=c(0.1,0.5),off=array(0,0),S=list(),
+ Ain=matrix(0,1,2),bin=0,C=matrix(0,0,0),sp=array(0,0),y=y,w=y*0+1)
> M$X[,1] <- 1; M$X[,2] <- x; M$Ain[1,] <- c(1,0)
> pcls(M) -> M$p
> plot(x,y); abline(M$p,col=2); abline(coef(lm(y~x)),col=3)
> 
> # Penalized example: monotonic penalized regression spline .....
> 
> # Generate data from a monotonic truth.
> x <- runif(100)*4-1;x <- sort(x);
> f <- exp(4*x)/(1+exp(4*x)); y <- f+rnorm(100)*0.1; plot(x,y)
> dat <- data.frame(x=x,y=y)
> # Show regular spline fit (and save fitted object)
> f.ug <- gam(y~s(x,k=10,bs="cr")); lines(x,fitted(f.ug))
> # Create Design matrix, constraints etc. for monotonic spline....
> sm <- smoothCon(s(x,k=10,bs="cr"),dat,knots=NULL)[[1]]
> F <- mono.con(sm$xp);   # get constraints
> G <- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,p=sm$xp,y=y,w=y*0+1)
> G$Ain <- F$A;G$bin <- F$b;G$S <- sm$S;G$off <- 0
> 
> p <- pcls(G);  # fit spline (using s.p. from unconstrained fit)
> 
> fv<-Predict.matrix(sm,data.frame(x=x))%*%p
> lines(x,fv,col=2)
> 
> # now a tprs example of the same thing....
> 
> f.ug <- gam(y~s(x,k=10)); lines(x,fitted(f.ug))
> # Create Design matrix, constriants etc. for monotonic spline....
> sm <- smoothCon(s(x,k=10,bs="tp"),dat,knots=NULL)[[1]]
> xc <- 0:39/39 # points on [0,1]  
> nc <- length(xc)  # number of constraints
> xc <- xc*4-1  # points at which to impose constraints
> A0 <- Predict.matrix(sm,data.frame(x=xc)) 
> # ... A0%*%p evaluates spline at xc points
> A1 <- Predict.matrix(sm,data.frame(x=xc+1e-6)) 
> A <- (A1-A0)/1e-6    
> ##  ... approx. constraint matrix (A%*%p is -ve 
> ## spline gradient at points xc)
> G <- list(X=sm$X,C=matrix(0,0,0),sp=f.ug$sp,y=y,w=y*0+1,S=sm$S,off=0)
> G$Ain <- A;    # constraint matrix
> G$bin <- rep(0,nc);  # constraint vector
> G$p <- rep(0,10); G$p[10] <- 0.1  
> # ... monotonic start params, got by setting coefs of polynomial part
> p <- pcls(G);  # fit spline (using s.p. from unconstrained fit)
> 
> fv2 <- Predict.matrix(sm,data.frame(x=x))%*%p
> lines(x,fv2,col=3)
> 
> ######################################
> ## monotonic additive model example...
> ######################################
> 
> ## First simulate data...
> 
> set.seed(10)
> f1 <- function(x) 5*exp(4*x)/(1+exp(4*x));
> f2 <- function(x) {
+   ind <- x > .5
+   f <- x*0
+   f[ind] <- (x[ind] - .5)^2*10
+   f 
+ }
> f3 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 
+       10 * (10 * x)^3 * (1 - x)^10
> n <- 200
> x <- runif(n); z <- runif(n); v <- runif(n)
> mu <- f1(x) + f2(z) + f3(v)
> y <- mu + rnorm(n)
> 
> ## Preliminary unconstrained gam fit...
> G <- gam(y~s(x)+s(z)+s(v,k=20),fit=FALSE)
> b <- gam(G=G)
> 
> ## generate constraints, by finite differencing
> ## using predict.gam ....
> eps <- 1e-7
> pd0 <- data.frame(x=seq(0,1,length=100),z=rep(.5,100),
+                   v=rep(.5,100))
> pd1 <- data.frame(x=seq(0,1,length=100)+eps,z=rep(.5,100),
+                   v=rep(.5,100))
> X0 <- predict(b,newdata=pd0,type="lpmatrix")
> X1 <- predict(b,newdata=pd1,type="lpmatrix")
> Xx <- (X1 - X0)/eps ## Xx %*% coef(b) must be positive 
> pd0 <- data.frame(z=seq(0,1,length=100),x=rep(.5,100),
+                   v=rep(.5,100))
> pd1 <- data.frame(z=seq(0,1,length=100)+eps,x=rep(.5,100),
+                   v=rep(.5,100))
> X0 <- predict(b,newdata=pd0,type="lpmatrix")
> X1 <- predict(b,newdata=pd1,type="lpmatrix")
> Xz <- (X1-X0)/eps
> G$Ain <- rbind(Xx,Xz) ## inequality constraint matrix
> G$bin <- rep(0,nrow(G$Ain))
> G$C = matrix(0,0,ncol(G$X))
> G$sp <- b$sp
> G$p <- coef(b)
> G$off <- G$off-1 ## to match what pcls is expecting
> ## force inital parameters to meet constraint
> G$p[11:18] <- G$p[2:9]<- 0
> p <- pcls(G) ## constrained fit
> par(mfrow=c(2,3))
> plot(b) ## original fit
> b$coefficients <- p
> plot(b) ## constrained fit
> ## note that standard errors in preceding plot are obtained from
> ## unconstrained fit
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("pdIdnot")
> ### * pdIdnot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pdIdnot
> ### Title: Overflow proof pdMat class for multiples of the identity matrix
> ### Aliases: pdIdnot pdConstruct.pdIdnot pdFactor.pdIdnot pdMatrix.pdIdnot
> ###   coef.pdIdnot corMatrix.pdIdnot Dim.pdIdnot logDet.pdIdnot
> ###   solve.pdIdnot summary.pdIdnot
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # see gamm
> 
> 
> 
> cleanEx()
> nameEx("pdTens")
> ### * pdTens
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pdTens
> ### Title: Functions implementing a pdMat class for tensor product smooths
> ### Aliases: pdTens pdConstruct.pdTens pdFactor.pdTens pdMatrix.pdTens
> ###   coef.pdTens summary.pdTens
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # see gamm
> 
> 
> 
> cleanEx()
> nameEx("pen.edf")
> ### * pen.edf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pen.edf
> ### Title: Extract the effective degrees of freedom associated with each
> ###   penalty in a gam fit
> ### Aliases: pen.edf
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
>   require(mgcv)
>   set.seed(20) 
>   dat <- gamSim(1,n=400,scale=2) ## simulate data
Gu & Wahba 4 term additive model
>   ## following `t2' smooth basically separates smooth 
>   ## of x0,x1 into main effects + interaction.... 
>   
>   b <- gam(y~t2(x0,x1,bs="tp",m=1,k=7)+s(x2)+s(x3),
+            data=dat,method="ML")
>   pen.edf(b)
t2(x0,x1)rr t2(x0,x1)nr t2(x0,x1)rn       s(x2)       s(x3) 
0.001236044 5.269350274 4.454572599 8.118427635 1.000034170 
>   
>   ## label "rr" indicates interaction edf (range space times range space)
>   ## label "nr" (null space for x0 times range space for x1) is main
>   ##            effect for x1.
>   ## label "rn" is main effect for x0
>   ## clearly interaction is negligible
>   
>   ## second example with higher order marginals. 
>   
>   b <- gam(y~t2(x0,x1,bs="tp",m=2,k=7,full=TRUE)
+              +s(x2)+s(x3),data=dat,method="ML")
>   pen.edf(b)
 t2(x0,x1)rr  t2(x0,x1)1r  t2(x0,x1)2r  t2(x0,x1)r1  t2(x0,x1)r2        s(x2) 
8.341040e-06 1.281162e+00 7.873262e-05 2.137126e+00 8.127323e-05 8.134455e+00 
       s(x3) 
1.000096e+00 
>   
>   ## In this case the EDF is negligible for all terms in the t2 smooth
>   ## apart from the `main effects' (r2 and 2r). To understand the labels
>   ## consider the following 2 examples....
>   ## "r1" relates to the interaction of the range space of the first 
>   ##      marginal smooth and the first basis function of the null 
>   ##      space of the second marginal smooth
>   ## "2r" relates to the interaction of the second basis function of 
>   ##      the null space of the first marginal smooth with the range 
>   ##      space of the second marginal smooth. 
> 
> 
> 
> cleanEx()
> nameEx("place.knots")
> ### * place.knots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: place.knots
> ### Title: Automatically place a set of knots evenly through covariate
> ###   values
> ### Aliases: place.knots
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> x<-runif(30)
> place.knots(x,7)
[1] 0.01339033 0.20525913 0.36153227 0.44190667 0.66953948 0.87447399 0.99190609
> rm(x)
> 
> 
> 
> cleanEx()
> nameEx("plot.gam")
> ### * plot.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.gam
> ### Title: Default GAM plotting
> ### Aliases: plot.gam
> ### Keywords: models smooth regression hplot
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> ## fake some data...
> f1 <- function(x) {exp(2 * x)}
> f2 <- function(x) { 
+   0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 
+ }
> f3 <- function(x) {x*0}
> 
> n<-200
> sig2<-4
> x0 <- rep(1:4,50)
> x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1)
> x3 <- runif(n, 0, 1)
> e <- rnorm(n, 0, sqrt(sig2))
> y <- 2*x0 + f1(x1) + f2(x2) + f3(x3) + e
> x0 <- factor(x0)
> 
> ## fit and plot...
> b<-gam(y~x0+s(x1)+s(x2)+s(x3))
> plot(b,pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col=2)
> plot(b,pages=1,seWithMean=TRUE) ## better coverage intervals
> 
> ## just parametric term alone...
> termplot(b,terms="x0",se=TRUE)
> 
> ## more use of color...
> op <- par(mfrow=c(2,2),bg="blue")
> x <- 0:1000/1000
> for (i in 1:3) {
+   plot(b,select=i,rug=FALSE,col="green",
+     col.axis="white",col.lab="white",all.terms=TRUE)
+   for (j in 1:2) axis(j,col="white",labels=FALSE)
+   box(col="white")
+   eval(parse(text=paste("fx <- f",i,"(x)",sep="")))
+   fx <- fx-mean(fx)
+   lines(x,fx,col=2) ## overlay `truth' in red
+ }
> par(op)
> 
> ## example with 2-d plots, and use of schemes...
> b1 <- gam(y~x0+s(x1,x2)+s(x3))
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE)
> par(op) 
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE,scheme=1)
> par(op)
> op <- par(mfrow=c(2,2))
> plot(b1,all.terms=TRUE,scheme=c(2,1))
> par(op)
> 
> ## 3 and 4 D smooths can also be plotted
> dat <- gamSim(1,n=400)
Gu & Wahba 4 term additive model
> b1 <- gam(y~te(x0,x1,x2,d=c(1,2),k=c(5,15))+s(x3),data=dat)
> 
> ## Now plot. Use cex.lab and cex.axis to control axis label size,
> ## n3 to control number of panels, n2 to control panel grid size,
> ## scheme=1 to get greyscale...
> 
> plot(b1,pages=1) 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("polys.plot")
> ### * polys.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: polys.plot
> ### Title: Plot geographic regions defined as polygons
> ### Aliases: polys.plot
> ### Keywords: hplot models smooth regression
> 
> ### ** Examples
> 
> ## see also ?mrf for use of z
> require(mgcv)
> data(columb.polys)
> polys.plot(columb.polys)
> 
> 
> 
> cleanEx()
> nameEx("predict.bam")
> ### * predict.bam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.bam
> ### Title: Prediction from fitted Big Additive Model model
> ### Aliases: predict.bam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## for parallel computing see examples for ?bam
> 
> ## for general useage follow examples in ?predict.gam
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.gam")
> ### * predict.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.gam
> ### Title: Prediction from fitted GAM model
> ### Aliases: predict.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> n<-200
> sig <- 2
> dat <- gamSim(1,n=n,scale=sig)
Gu & Wahba 4 term additive model
> 
> b<-gam(y~s(x0)+s(I(x1^2))+s(x2)+offset(x3),data=dat)
> 
> newd <- data.frame(x0=(0:30)/30,x1=(0:30)/30,x2=(0:30)/30,x3=(0:30)/30)
> pred <- predict.gam(b,newd)
> pred0 <- predict(b,newd,exclude="s(x0)") ## prediction excluding a term
> ## ...and the same, but without needing to provide x0 prediction data...
> newd1 <- newd;newd1$x0 <- NULL ## remove x0 from `newd1'
> pred1 <- predict(b,newd1,exclude="s(x0)",newdata.guaranteed=TRUE)
> 
> #############################################
> ## difference between "terms" and "iterms"
> #############################################
> nd2 <- data.frame(x0=c(.25,.5),x1=c(.25,.5),x2=c(.25,.5),x3=c(.25,.5))
> predict(b,nd2,type="terms",se=TRUE)
$fit
        s(x0) s(I(x1^2))     s(x2)
1 -0.05203567 -1.4080075 5.8224469
2  0.46405199 -0.3019055 0.2471463

$se.fit
      s(x0) s(I(x1^2))     s(x2)
1 0.1991066 0.12604172 0.5158322
2 0.1799396 0.02702592 0.3930834

attr(,"constant")
(Intercept) 
   6.836839 
> predict(b,nd2,type="iterms",se=TRUE)
$fit
        s(x0) s(I(x1^2))     s(x2)
1 -0.05203567 -1.4080075 5.8224469
2  0.46405199 -0.3019055 0.2471463

$se.fit
      s(x0) s(I(x1^2))     s(x2)
1 0.2523494  0.1998081 0.5386275
2 0.2375185  0.1573759 0.4225534

attr(,"constant")
(Intercept) 
   6.836839 
> 
> #########################################################
> ## now get variance of sum of predictions using lpmatrix
> #########################################################
> 
> Xp <- predict(b,newd,type="lpmatrix") 
> 
> ## Xp %*% coef(b) yields vector of predictions
> 
> a <- rep(1,31)
> Xs <- t(a) %*% Xp ## Xs %*% coef(b) gives sum of predictions
> var.sum <- Xs %*% b$Vp %*% t(Xs)
> 
> 
> #############################################################
> ## Now get the variance of non-linear function of predictions
> ## by simulation from posterior distribution of the params
> #############################################################
> 
> rmvn <- function(n,mu,sig) { ## MVN random deviates
+   L <- mroot(sig);m <- ncol(L);
+   t(mu + L%*%matrix(rnorm(m*n),m,n)) 
+ }
> 
> br <- rmvn(1000,coef(b),b$Vp) ## 1000 replicate param. vectors
> res <- rep(0,1000)
> for (i in 1:1000)
+ { pr <- Xp %*% br[i,] ## replicate predictions
+   res[i] <- sum(log(abs(pr))) ## example non-linear function
+ }
> mean(res);var(res)
[1] 57.36799
[1] 2.141379
> 
> ## loop is replace-able by following .... 
> 
> res <- colSums(log(abs(Xp %*% t(br))))
> 
> 
> 
> ##################################################################
> ## The following shows how to use use an "lpmatrix" as a lookup 
> ## table for approximate prediction. The idea is to create 
> ## approximate prediction matrix rows by appropriate linear 
> ## interpolation of an existing prediction matrix. The additivity 
> ## of a GAM makes this possible. 
> ## There is no reason to ever do this in R, but the following 
> ## code provides a useful template for predicting from a fitted 
> ## gam *outside* R: all that is needed is the coefficient vector 
> ## and the prediction matrix. Use larger `Xp'/ smaller `dx' and/or 
> ## higher order interpolation for higher accuracy.  
> ###################################################################
> 
> xn <- c(.341,.122,.476,.981) ## want prediction at these values
> x0 <- 1         ## intercept column
> dx <- 1/30      ## covariate spacing in `newd'
> for (j in 0:2) { ## loop through smooth terms
+   cols <- 1+j*9 +1:9      ## relevant cols of Xp
+   i <- floor(xn[j+1]*30)  ## find relevant rows of Xp
+   w1 <- (xn[j+1]-i*dx)/dx ## interpolation weights
+   ## find approx. predict matrix row portion, by interpolation
+   x0 <- c(x0,Xp[i+2,cols]*w1 + Xp[i+1,cols]*(1-w1))
+ }
> dim(x0)<-c(1,28) 
> fv <- x0%*%coef(b) + xn[4];fv    ## evaluate and add offset
         [,1]
[1,] 6.801448
> se <- sqrt(x0%*%b$Vp%*%t(x0));se ## get standard error
          [,1]
[1,] 0.4654633
> ## compare to normal prediction
> predict(b,newdata=data.frame(x0=xn[1],x1=xn[2],
+         x2=xn[3],x3=xn[4]),se=TRUE)
$fit
       1 
6.790702 

$se.fit
        1 
0.4754891 

> 
> ##################################################################
> # illustration of unsafe scale dependent transforms in smooths....
> ##################################################################
> 
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+x3,data=dat) ## safe
> b1 <- gam(y~s(x0)+s(I(x1/2))+s(x2)+scale(x3),data=dat) ## safe
> b2 <- gam(y~s(x0)+s(scale(x1))+s(x2)+scale(x3),data=dat) ## unsafe
> pd <- dat; pd$x1 <- pd$x1/2; pd$x3 <- pd$x3/2
> par(mfrow=c(1,2))
> plot(predict(b0,pd),predict(b1,pd),main="b0 and b1 predictions match")
> abline(0,1,col=2)
> plot(predict(b0,pd),predict(b2,pd),main="b2 unsafe, doesn't match")
> abline(0,1,col=2)
> 
> 
> ####################################################################
> ## Differentiating the smooths in a model (with CIs for derivatives)
> ####################################################################
> 
> ## simulate data and fit model...
> dat <- gamSim(1,n=300,scale=sig)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> 
> ## now evaluate derivatives of smooths with associated standard 
> ## errors, by finite differencing...
> x.mesh <- seq(0,1,length=200) ## where to evaluate derivatives
> newd <- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
> X0 <- predict(b,newd,type="lpmatrix") 
> 
> eps <- 1e-7 ## finite difference interval
> x.mesh <- x.mesh + eps ## shift the evaluation mesh
> newd <- data.frame(x0 = x.mesh,x1 = x.mesh, x2=x.mesh,x3=x.mesh)
> X1 <- predict(b,newd,type="lpmatrix")
> 
> Xp <- (X1-X0)/eps ## maps coefficients to (fd approx.) derivatives
> colnames(Xp)      ## can check which cols relate to which smooth
 [1] "(Intercept)" "s(x0).1"     "s(x0).2"     "s(x0).3"     "s(x0).4"    
 [6] "s(x0).5"     "s(x0).6"     "s(x0).7"     "s(x0).8"     "s(x0).9"    
[11] "s(x1).1"     "s(x1).2"     "s(x1).3"     "s(x1).4"     "s(x1).5"    
[16] "s(x1).6"     "s(x1).7"     "s(x1).8"     "s(x1).9"     "s(x2).1"    
[21] "s(x2).2"     "s(x2).3"     "s(x2).4"     "s(x2).5"     "s(x2).6"    
[26] "s(x2).7"     "s(x2).8"     "s(x2).9"     "s(x3).1"     "s(x3).2"    
[31] "s(x3).3"     "s(x3).4"     "s(x3).5"     "s(x3).6"     "s(x3).7"    
[36] "s(x3).8"     "s(x3).9"    
> 
> par(mfrow=c(2,2))
> for (i in 1:4) {  ## plot derivatives and corresponding CIs
+   Xi <- Xp*0 
+   Xi[,(i-1)*9+1:9+1] <- Xp[,(i-1)*9+1:9+1] ## Xi%*%coef(b) = smooth deriv i
+   df <- Xi%*%coef(b)              ## ith smooth derivative 
+   df.sd <- rowSums(Xi%*%b$Vp*Xi)^.5 ## cheap diag(Xi%*%b$Vp%*%t(Xi))^.5
+   plot(x.mesh,df,type="l",ylim=range(c(df+2*df.sd,df-2*df.sd)))
+   lines(x.mesh,df+2*df.sd,lty=2);lines(x.mesh,df-2*df.sd,lty=2)
+ }
> 
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("psum.chisq")
> ### * psum.chisq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: psum.chisq
> ### Title: Evaluate the c.d.f. of a weighted sum of chi-squared deviates
> ### Aliases: psum.chisq
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   lb <- c(4.1,1.2,1e-3,-1) ## weights
>   df <- c(2,1,1,1) ## degrees of freedom
>   nc <- c(1,1.5,4,1) ## non-centrality parameter
>   q <- c(1,6,20) ## quantiles to evaluate
> 
>   psum.chisq(q,lb,df,nc)
[1] 0.9054735 0.6857363 0.2296298
> 
>   ## same by simulation...
>   
>   psc.sim <- function(q,lb,df=lb*0+1,nc=df*0,ns=10000) {
+     r <- length(lb);p <- q
+     X <- rowSums(rep(lb,each=ns) *
+          matrix(rchisq(r*ns,rep(df,each=ns),rep(nc,each=ns)),ns,r))
+     apply(matrix(q),1,function(q) mean(X>q))	 
+   } ## psc.sim
>   
>   psum.chisq(q,lb,df,nc)
[1] 0.9054735 0.6857363 0.2296298
>   psc.sim(q,lb,df,nc,100000)
[1] 0.90560 0.68586 0.23014
> 
> 
> 
> cleanEx()
> nameEx("qq.gam")
> ### * qq.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: qq.gam
> ### Title: QQ plots for gam model residuals
> ### Aliases: qq.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> library(mgcv)
> ## simulate binomial data...
> set.seed(0)
> n.samp <- 400
> dat <- gamSim(1,n=n.samp,dist="binary",scale=.33)
Gu & Wahba 4 term additive model
> p <- binomial()$linkinv(dat$f) ## binomial p
> n <- sample(c(1,3),n.samp,replace=TRUE) ## binomial n
> dat$y <- rbinom(n,n,p)
> dat$n <- n
> 
> lr.fit <- gam(y/n~s(x0)+s(x1)+s(x2)+s(x3)
+              ,family=binomial,data=dat,weights=n,method="REML")
> 
> par(mfrow=c(2,2))
> ## normal QQ-plot of deviance residuals
> qqnorm(residuals(lr.fit),pch=19,cex=.3)
> ## Quick QQ-plot of deviance residuals
> qq.gam(lr.fit,pch=19,cex=.3)
> ## Simulation based QQ-plot with reference bands 
> qq.gam(lr.fit,rep=100,level=.9)
> ## Simulation based QQ-plot, Pearson resids, all
> ## simulated reference plots shown...  
> qq.gam(lr.fit,rep=100,level=1,type="pearson",pch=19,cex=.2)
> 
> ## Now fit the wrong model and check....
> 
> pif <- gam(y~s(x0)+s(x1)+s(x2)+s(x3)
+              ,family=poisson,data=dat,method="REML")
> par(mfrow=c(2,2))
> qqnorm(residuals(pif),pch=19,cex=.3)
> qq.gam(pif,pch=19,cex=.3)
> qq.gam(pif,rep=100,level=.9)
> qq.gam(pif,rep=100,level=1,type="pearson",pch=19,cex=.2)
> 
> ## Example of binary data model violation so gross that you see a problem 
> ## on the QQ plot...
> 
> y <- c(rep(1,10),rep(0,20),rep(1,40),rep(0,10),rep(1,40),rep(0,40))
> x <- 1:160
> b <- glm(y~x,family=binomial)
> par(mfrow=c(2,2))
> ## Note that the next two are not necessarily similar under gross 
> ## model violation...
> qq.gam(b)
> qq.gam(b,rep=50,level=1)
> ## and a much better plot for detecting the problem
> plot(x,residuals(b),pch=19,cex=.3)
> plot(x,y);lines(x,fitted(b))
> 
> ## alternative model
> b <- gam(y~s(x,k=5),family=binomial,method="ML")
> qq.gam(b)
> qq.gam(b,rep=50,level=1)
> plot(x,residuals(b),pch=19,cex=.3)
> plot(b,residuals=TRUE,pch=19,cex=.3)
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("rTweedie")
> ### * rTweedie
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rTweedie
> ### Title: Generate Tweedie random deviates
> ### Aliases: rTweedie
> ### Keywords: models regression
> 
> ### ** Examples
> 
>  library(mgcv)
>  f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 *
+             (10 * x)^3 * (1 - x)^10
>  n <- 300
>  x <- runif(n)
>  mu <- exp(f2(x)/3+.1);x <- x*10 - 4
>  y <- rTweedie(mu,p=1.5,phi=1.3)
>  b <- gam(y~s(x,k=20),family=Tweedie(p=1.5))
>  b

Family: Tweedie(1.5) 
Link function: log 

Formula:
y ~ s(x, k = 20)

Estimated degrees of freedom:
7.12  total = 8.12 

GCV score: 1.598862     
>  plot(b) 
> 
> 
> 
> 
> cleanEx()
> nameEx("random.effects")
> ### * random.effects
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: random.effects
> ### Title: Random effects in GAMs
> ### Aliases: random.effects
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## see also examples for gam.models, gam.vcomp, gamm
> ## and smooth.construct.re.smooth.spec
> 
> ## simple comparison of lme and gam
> require(mgcv)
> require(nlme)
> b0 <- lme(travel~1,data=Rail,~1|Rail,method="REML") 
> 
> b <- gam(travel~s(Rail,bs="re"),data=Rail,method="REML")
> 
> intervals(b0)
Approximate 95% confidence intervals

 Fixed effects:
               lower est.    upper
(Intercept) 44.33921 66.5 88.66079
attr(,"label")
[1] "Fixed effects:"

 Random Effects:
  Level: Rail 
                   lower     est.    upper
sd((Intercept)) 13.27434 24.80547 46.35344

 Within-group standard error:
   lower     est.    upper 
2.695003 4.020779 5.998757 
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

          std.dev     lower     upper
s(Rail) 24.805465 13.274315 46.353510
scale    4.020779  2.695004  5.998753

Rank: 2/2
> anova(b)

Family: gaussian 
Link function: identity 

Formula:
travel ~ s(Rail, bs = "re")

Approximate significance of smooth terms:
          edf Ref.df     F p-value
s(Rail) 4.957  5.000 114.2  <2e-16
> plot(b)
> 
> ## simulate example...
> dat <- gamSim(1,n=400,scale=2) ## simulate 4 term additive truth
Gu & Wahba 4 term additive model
> 
> fac <- sample(1:20,400,replace=TRUE)
> b <- rnorm(20)*.5
> dat$y <- dat$y + b[fac]
> dat$fac <- as.factor(fac)
> 
> rm1 <- gam(y ~ s(fac,bs="re")+s(x0)+s(x1)+s(x2)+s(x3),data=dat,method="ML")
> gam.vcomp(rm1)

Standard deviations and 0.95 confidence intervals:

            std.dev        lower        upper
s(fac)   0.63346180 3.869924e-01 1.036904e+00
s(x0)    7.78342852 3.166580e+00 1.913160e+01
s(x1)    7.51248002 2.710674e+00 2.082042e+01
s(x2)  101.43673011 5.691102e+01 1.807982e+02
s(x3)    0.03326905 5.876691e-21 1.883423e+17
scale    2.08810433 1.941875e+00 2.245346e+00

Rank: 6/6
> 
> fv0 <- predict(rm1,exclude="s(fac)") ## predictions setting r.e. to 0
> fv1 <- predict(rm1) ## predictions setting r.e. to predicted values
> ## prediction setting r.e. to 0 and not having to provide 'fac'...
> pd <- dat; pd$fac <- NULL
> fv0 <- predict(rm1,pd,exclude="s(fac)",newdata.guaranteed=TRUE)
> 
> ## Prediction with levels of fac not in fit data.
> ## The effect of the new factor levels (or any interaction involving them)
> ## is set to zero.
> xx <- seq(0,1,length=10)
> pd <- data.frame(x0=xx,x1=xx,x2=xx,x3=xx,fac=c(1:10,21:30))
> fv <- predict(rm1,pd)
Warning in predict.gam(rm1, pd) :
  factor levels 21, 22, 23, 24, 25, 26, 27, 28, 29, 30 not in original fit
> pd$fac <- NULL
> fv0 <- predict(rm1,pd,exclude="s(fac)",newdata.guaranteed=TRUE)
> 
> 
> 
> 
> cleanEx()
> nameEx("rig")
> ### * rig
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rig
> ### Title: Generate inverse Gaussian random deviates
> ### Aliases: rig
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(7)
> ## An inverse.gaussian GAM example, by modify `gamSim' output... 
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## simulate inverse Gaussian response...
> dat$y <- rig(Ey,mean=Ey,scale=.2)
> big <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=inverse.gaussian(link=log),
+           data=dat,method="REML")
> plot(big,pages=1)
> gam.check(big)

Method: REML   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [-0.0003255033,4.689363e-06]
(score 1154.207 & scale 0.1808111).
Hessian positive definite, eigenvalue range [0.2487388,197.5501].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value  
s(x0) 9.00 3.02    0.76   0.075 .
s(x1) 9.00 2.98    0.89   0.935  
s(x2) 9.00 6.41    0.85   0.665  
s(x3) 9.00 1.89    0.89   0.950  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> summary(big)

Family: inverse.gaussian 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.93014    0.06261   30.83   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 3.020  3.744  3.894 0.00556 ** 
s(x1) 2.985  3.712 20.371 < 2e-16 ***
s(x2) 6.412  7.524 12.139 < 2e-16 ***
s(x3) 1.887  2.343  1.637 0.16824    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.0874   Deviance explained = 38.6%
-REML = 1154.2  Scale est. = 0.18081   n = 400
> 
> 
> 
> cleanEx()
> nameEx("rmvn")
> ### * rmvn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rmvn
> ### Title: Generate from or evaluate multivariate normal or t densities.
> ### Aliases: rmvn dmvn r.mvt d.mvt
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> V <- matrix(c(2,1,1,2),2,2) 
> mu <- c(1,3)
> n <- 1000
> z <- rmvn(n,mu,V)
> crossprod(sweep(z,2,colMeans(z)))/n ## observed covariance matrix
         [,1]     [,2]
[1,] 2.273002 1.135038
[2,] 1.135038 2.084711
> colMeans(z) ## observed mu
[1] 0.9353139 2.9894941
> dmvn(z,mu,V)
[1] -5.083725 -3.255265
> 
> 
> 
> cleanEx()
> nameEx("s")
> ### * s
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: s
> ### Title: Defining smooths in GAM formulae
> ### Aliases: s
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> # example utilising `by' variables
> library(mgcv)
> set.seed(0)
> n<-200;sig2<-4
> x1 <- runif(n, 0, 1);x2 <- runif(n, 0, 1);x3 <- runif(n, 0, 1)
> fac<-c(rep(1,n/2),rep(2,n/2)) # create factor
> fac.1<-rep(0,n)+(fac==1);fac.2<-1-fac.1 # and dummy variables
> fac<-as.factor(fac)
> f1 <-  exp(2 * x1) - 3.75887
> f2 <-  0.2 * x1^11 * (10 * (1 - x1))^6 + 10 * (10 * x1)^3 * (1 - x1)^10
> f<-f1*fac.1+f2*fac.2+x2
> e <- rnorm(n, 0, sqrt(abs(sig2)))
> y <- f + e
> # NOTE: smooths will be centered, so need to include fac in model....
> b<-gam(y~fac+s(x1,by=fac)+x2) 
> plot(b,pages=1)
> 
> 
> 
> cleanEx()
> nameEx("scat")
> ### * scat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scat
> ### Title: GAM scaled t family for heavy tailed data
> ### Aliases: scat t.scaled
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Simulate some t data...
> set.seed(3);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$y <- dat$f + rt(n,df=4)*2
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=scat(link="identity"),data=dat)
> 
> b

Family: Scaled t(5.376,2.088) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
3.07 2.69 8.01 2.31  total = 17.08 

REML score: 961.0407     
> plot(b,pages=1)
> 
> 
> 
> 
> cleanEx()
> nameEx("sdiag")
> ### * sdiag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sdiag
> ### Title: Extract or modify diagonals of a matrix
> ### Aliases: sdiag sdiag<-
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> A <- matrix(1:35,7,5)
> A
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    8   15   22   29
[2,]    2    9   16   23   30
[3,]    3   10   17   24   31
[4,]    4   11   18   25   32
[5,]    5   12   19   26   33
[6,]    6   13   20   27   34
[7,]    7   14   21   28   35
> sdiag(A,1) ## first super diagonal
[1]  8 16 24 32
> sdiag(A,-1) ## first sub diagonal
[1]  2 10 18 26 34
> 
> sdiag(A) <- 1 ## leading diagonal set to 1
> sdiag(A,3) <- c(-1,-2) ## set 3rd super diagonal 
> 
> 
> 
> 
> cleanEx()
> nameEx("shash")
> ### * shash
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: shash
> ### Title: Sinh-arcsinh location scale and shape model family
> ### Aliases: shash
> 
> ### ** Examples
> 
> 
> ###############
> # Shash dataset
> ###############
> ##  Simulate some data from shash
> set.seed(847)
> n <- 1000
> x <- seq(-4, 4, length.out = n)
> 
> X <- cbind(1, x, x^2)
> beta <- c(4, 1, 1)
> mu <- X %*% beta 
> 
> sigma =  .5+0.4*(x+4)*.5            # Scale
> eps = 2*sin(x)                      # Skewness
> del = 1 + 0.2*cos(3*x)              # Kurtosis
> 
> dat <-  mu + (del * sigma) * sinh((1/del) * asinh(qnorm(runif(n))) + (eps/del))
> dataf <- data.frame(cbind(dat, x))
> names(dataf) <- c("y", "x")
> plot(x, dat, xlab = "x", ylab = "y")
> 
> ## Fit model
> fit <- gam(list(y ~ s(x), # <- model for location 
+                   ~ s(x),   # <- model for log-scale
+                   ~ s(x),   # <- model for skewness
+                   ~ s(x, k = 20)), # <- model for log-kurtosis
+            data = dataf, 
+            family = shash, # <- new family 
+            optimizer = "efs")
> 
> ## Plotting truth and estimates for each parameters of the density 
> muE <- fit$fitted[ , 1]
> sigE <- exp(fit$fitted[ , 2])
> epsE <- fit$fitted[ , 3]
> delE <- exp(fit$fitted[ , 4])
> 
> par(mfrow = c(2, 2))
> plot(x, muE, type = 'l', ylab = expression(mu(x)), lwd = 2)
> lines(x, mu, col = 2, lty = 2, lwd = 2)
> legend("top", c("estimated", "truth"), col = 1:2, lty = 1:2, lwd = 2)
> 
> plot(x, sigE, type = 'l', ylab = expression(sigma(x)), lwd = 2)
> lines(x, sigma, col = 2, lty = 2, lwd = 2)
> 
> plot(x, epsE, type = 'l', ylab = expression(epsilon(x)), lwd = 2)
> lines(x, eps, col = 2, lty = 2, lwd = 2)
> 
> plot(x, delE, type = 'l', ylab = expression(delta(x)), lwd = 2)
> lines(x, del, col = 2, lty = 2, lwd = 2)
> 
> ## Plotting true and estimated conditional density
> par(mfrow = c(1, 1))
> plot(x, dat, pch = '.', col = "grey", ylab = "y", ylim = c(-35, 70))
> for(qq in c(0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999)){
+   est <- fit$family$qf(p=qq, mu = fit$fitted)
+   true <- mu + (del * sigma) * sinh((1/del) * asinh(qnorm(qq)) + (eps/del))
+   lines(x, est, type = 'l', col = 1, lwd = 2)
+   lines(x, true, type = 'l', col = 2, lwd = 2, lty = 2)
+ }
> legend("topleft", c("estimated", "truth"), col = 1:2, lty = 1:2, lwd = 2)
> 
> #####################
> ## Motorcycle example
> #####################
> 
> # Here shash is overkill, in fact the fit is not good, relative
> # to what we would get with mgcv::gaulss
> library(MASS)
> 
> b <- gam(list(accel~s(times, k=20, bs = "ad"), ~ s(times, k = 10), ~ 1, ~ 1),
+          data=mcycle, family=shash)
> 
> par(mfrow = c(1, 1))
> xSeq <- data.frame(cbind("accel" = rep(0, 1e3), "times" = seq(2, 58, length.out = 1e3)))
> pred <- predict(b, newdata = xSeq)
> plot(mcycle$times, mcycle$accel, ylim = c(-180, 100))
> for(qq in c(0.1, 0.3, 0.5, 0.7, 0.9)){
+   est <- b$family$qf(p=qq, mu = pred)
+   lines(xSeq$times, est, type = 'l', col = 2)
+ }
> 
> plot(b, pages = 1, scale = FALSE)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:MASS’

> nameEx("single.index")
> ### * single.index
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: single.index
> ### Title: Single index models with mgcv
> ### Aliases: single.index
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> 
> si <- function(theta,y,x,z,opt=TRUE,k=10,fx=FALSE) {
+ ## Fit single index model using gam call, given theta (defines alpha). 
+ ## Return ML if opt==TRUE and fitted gam with theta added otherwise.
+ ## Suitable for calling from 'optim' to find optimal theta/alpha.
+   alpha <- c(1,theta) ## constrained alpha defined using free theta
+   kk <- sqrt(sum(alpha^2))
+   alpha <- alpha/kk  ## so now ||alpha||=1
+   a <- x%*%alpha     ## argument of smooth
+   b <- gam(y~s(a,fx=fx,k=k)+s(z),family=poisson,method="ML") ## fit model
+   if (opt) return(b$gcv.ubre) else {
+     b$alpha <- alpha  ## add alpha
+     J <- outer(alpha,-theta/kk^2) ## compute Jacobian
+     for (j in 1:length(theta)) J[j+1,j] <- J[j+1,j] + 1/kk
+     b$J <- J ## dalpha_i/dtheta_j 
+     return(b)
+   }
+ } ## si
> 
> ## simulate some data from a single index model...
> 
> set.seed(1)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 200;m <- 3
> x <- matrix(runif(n*m),n,m) ## the covariates for the single index part
> z <- runif(n) ## another covariate
> alpha <- c(1,-1,.5); alpha <- alpha/sqrt(sum(alpha^2))
> eta <- as.numeric(f2((x%*%alpha+.41)/1.4)+1+z^2*2)/4
> mu <- exp(eta)
> y <- rpois(n,mu) ## Poi response 
> 
> ## now fit to the simulated data...
> 
> 
> th0 <- c(-.8,.4) ## close to truth for speed
> ## get initial theta, using no penalization...
> f0 <- nlm(si,th0,y=y,x=x,z=z,fx=TRUE,k=5)
> ## now get theta/alpha with smoothing parameter selection...
> f1 <- nlm(si,f0$estimate,y=y,x=x,z=z,hessian=TRUE,k=10)
> theta.est <-f1$estimate 
> 
> ## Alternative using 'optim'... 
> ## extract and examine fitted model...
> 
> b <- si(theta.est,y,x,z,opt=FALSE) ## extract best fit model
> plot(b,pages=1)
> b

Family: poisson 
Link function: log 

Formula:
y ~ s(a, fx = fx, k = k) + s(z)

Estimated degrees of freedom:
7.3 1.0  total = 9.3 

ML score: 468.4854     
> b$alpha 
[1]  0.6160274 -0.6987148  0.3637415
> ## get sd for alpha...
> Vt <- b$J%*%solve(f1$hessian,t(b$J))
> diag(Vt)^.5
[1] 0.01655776 0.01298555 0.01968088
> 
> 
> 
> 
> cleanEx()
> nameEx("slanczos")
> ### * slanczos
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: slanczos
> ### Title: Compute truncated eigen decomposition of a symmetric matrix
> ### Aliases: slanczos
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
>  require(mgcv)
>  ## create some x's and knots...
>  set.seed(1);
>  n <- 700;A <- matrix(runif(n*n),n,n);A <- A+t(A)
>  
>  ## compare timings of slanczos and eigen
>  system.time(er <- slanczos(A,10))
   user  system elapsed 
  0.064   0.052   0.033 
>  system.time(um <- eigen(A,symmetric=TRUE))
   user  system elapsed 
  0.326   0.202   0.137 
>  
>  ## confirm values are the same...
>  ind <- c(1:6,(n-3):n)
>  range(er$values-um$values[ind]);range(abs(er$vectors)-abs(um$vectors[,ind]))
[1] -2.149392e-12  1.250555e-12
[1] -1.246456e-07  1.332938e-07
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct")
> ### * smooth.construct
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct
> ### Title: Constructor functions for smooth terms in a GAM
> ### Aliases: smooth.construct smooth.construct2 user.defined.smooth
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Adding a penalized truncated power basis class and methods
> ## as favoured by Ruppert, Wand and Carroll (2003) 
> ## Semiparametric regression CUP. (No advantage to actually
> ## using this, since mgcv can happily handle non-identity 
> ## penalties.)
> 
> smooth.construct.tr.smooth.spec<-function(object,data,knots) {
+ ## a truncated power spline constructor method function
+ ## object$p.order = null space dimension
+   m <- object$p.order[1]
+   if (is.na(m)) m <- 2 ## default 
+   if (m<1) stop("silly m supplied")
+   if (object$bs.dim<0) object$bs.dim <- 10 ## default
+   nk<-object$bs.dim-m-1 ## number of knots
+   if (nk<=0) stop("k too small for m")
+   x <- data[[object$term]]  ## the data
+   x.shift <- mean(x) # shift used to enhance stability
+   k <- knots[[object$term]] ## will be NULL if none supplied
+   if (is.null(k)) # space knots through data
+   { n<-length(x)
+     k<-quantile(x[2:(n-1)],seq(0,1,length=nk+2))[2:(nk+1)]
+   }
+   if (length(k)!=nk) # right number of knots?
+   stop(paste("there should be ",nk," supplied knots"))
+   x <- x - x.shift # basis stabilizing shift
+   k <- k - x.shift # knots treated the same!
+   X<-matrix(0,length(x),object$bs.dim)
+   for (i in 1:(m+1)) X[,i] <- x^(i-1)
+   for (i in 1:nk) X[,i+m+1]<-(x-k[i])^m*as.numeric(x>k[i])
+   object$X<-X # the finished model matrix
+   if (!object$fixed) # create the penalty matrix
+   { object$S[[1]]<-diag(c(rep(0,m+1),rep(1,nk)))
+   }
+   object$rank<-nk  # penalty rank
+   object$null.space.dim <- m+1  # dim. of unpenalized space
+   ## store "tr" specific stuff ...
+   object$knots<-k;object$m<-m;object$x.shift <- x.shift
+  
+   object$df<-ncol(object$X)     # maximum DoF (if unconstrained)
+  
+   class(object)<-"tr.smooth"  # Give object a class
+   object
+ }
> 
> Predict.matrix.tr.smooth<-function(object,data) {
+ ## prediction method function for the `tr' smooth class
+   x <- data[[object$term]]
+   x <- x - object$x.shift # stabilizing shift
+   m <- object$m;     # spline order (3=cubic)
+   k<-object$knots    # knot locations
+   nk<-length(k)      # number of knots
+   X<-matrix(0,length(x),object$bs.dim)
+   for (i in 1:(m+1)) X[,i] <- x^(i-1)
+   for (i in 1:nk) X[,i+m+1] <- (x-k[i])^m*as.numeric(x>k[i])
+   X # return the prediction matrix
+ }
> 
> # an example, using the new class....
> require(mgcv)
> set.seed(100)
> dat <- gamSim(1,n=400,scale=2)
Gu & Wahba 4 term additive model
> b<-gam(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
+          s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
> plot(b,pages=1)
> b<-gamm(y~s(x0,bs="tr",m=2)+s(x1,bs="ps",m=c(1,3))+
+          s(x2,bs="tr",m=3)+s(x3,bs="tr",m=2),data=dat)
> plot(b$gam,pages=1)
> # another example using tensor products of the new class
> dat <- gamSim(2,n=400,scale=.1)$data
Bivariate smoothing example
> b <- gam(y~te(x,z,bs=c("tr","tr"),m=c(2,2)),data=dat)
> vis.gam(b)
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.ad.smooth.spec")
> ### * smooth.construct.ad.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ad.smooth.spec
> ### Title: Adaptive smooths in GAMs
> ### Aliases: smooth.construct.ad.smooth.spec adaptive.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Comparison using an example taken from AdaptFit
> ## library(AdaptFit)
> require(mgcv)
> set.seed(0)
> x <- 1:1000/1000
> mu <- exp(-400*(x-.6)^2)+5*exp(-500*(x-.75)^2)/3+2*exp(-500*(x-.9)^2)
> y <- mu+0.5*rnorm(1000)
> 
> ##fit with default knots
> ## y.fit <- asp(y~f(x))
> 
> par(mfrow=c(2,2))
> ## plot(y.fit,main=round(cor(fitted(y.fit),mu),digits=4))
> ## lines(x,mu,col=2)
> 
> b <- gam(y~s(x,bs="ad",k=40,m=5)) ## adaptive
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
>  
> b <- gam(y~s(x,k=40))             ## non-adaptive
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
> 
> b <- gam(y~s(x,bs="ad",k=40,m=5,xt=list(bs="cr")))
> plot(b,shade=TRUE,main=round(cor(fitted(b),mu),digits=4))
> lines(x,mu-mean(mu),col=2)
> 
> ## A 2D example (marked, 'Not run' purely to reduce
> ## checking load on CRAN).
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.bs.smooth.spec")
> ### * smooth.construct.bs.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.bs.smooth.spec
> ### Title: Penalized B-splines in GAMs
> ### Aliases: smooth.construct.bs.smooth.spec Predict.matrix.Bspline.smooth
> ###   b.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
>   require(mgcv)
>   set.seed(5)
>   dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
>   bs <- "bs"
>   ## note the double penalty on the s(x2) term...
>   b <- gam(y~s(x0,bs=bs,m=c(4,2))+s(x1,bs=bs)+s(x2,k=15,bs=bs,m=c(4,3,0))+
+            s(x3,bs=bs,m=c(1,0)),data=dat,method="REML")
>   plot(b,pages=1)
> 
>   ## Extrapolation example, illustrating the importance of considering
>   ## the penalty carefully if extrapolating...
>   f3 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * (10 * x)^3 * 
+               (1 - x)^10 ## test function
>   n <- 100;x <- runif(n)
>   y <- f3(x) + rnorm(n)*2
>   ## first a model with first order penalty over whole real line (red)
>   b0 <- gam(y~s(x,m=1,k=20),method="ML")
>   ## now a model with first order penalty evaluated over (-.5,1.5) (black)
>   op <- options(warn=-1)
>   b <- gam(y~s(x,bs="bs",m=c(3,1),k=20),knots=list(x=c(-.5,0,1,1.5)),
+            method="ML")
>   options(op)
>   ## and the equivalent with same penalty over data range only (blue)
>   b1 <- gam(y~s(x,bs="bs",m=c(3,1),k=20),method="ML")
>   pd <- data.frame(x=seq(-.7,1.7,length=200))
>   fv <- predict(b,pd,se=TRUE)
>   ul <- fv$fit + fv$se.fit*2; ll <- fv$fit - fv$se.fit*2
>   plot(x,y,xlim=c(-.7,1.7),ylim=range(c(y,ll,ul)),main=
+   "Order 1 penalties: red tps; blue bs on (0,1); black bs on (-.5,1.5)")
>   ## penalty defined on (-.5,1.5) gives plausible predictions and intervals
>   ## over this range...
>   lines(pd$x,fv$fit);lines(pd$x,ul,lty=2);lines(pd$x,ll,lty=2)
>   fv <- predict(b0,pd,se=TRUE)
>   ul <- fv$fit + fv$se.fit*2; ll <- fv$fit - fv$se.fit*2
>   ## penalty defined on whole real line gives constant width intervals away
>   ## from data, as slope there must be zero, to avoid infinite penalty:
>   lines(pd$x,fv$fit,col=2)
>   lines(pd$x,ul,lty=2,col=2);lines(pd$x,ll,lty=2,col=2)
>   fv <- predict(b1,pd,se=TRUE)
>   ul <- fv$fit + fv$se.fit*2; ll <- fv$fit - fv$se.fit*2
>   ## penalty defined only over the data interval (0,1) gives wild and wide
>   ## extrapolation since penalty has been `turned off' outside data range:
>   lines(pd$x,fv$fit,col=4)
>   lines(pd$x,ul,lty=2,col=4);lines(pd$x,ll,lty=2,col=4)
> 
>   ## construct smooth of x. Model matrix sm$X and penalty 
>   ## matrix sm$S[[1]] will have many zero entries...
>   x <- seq(0,1,length=100)
>   sm <- smoothCon(s(x,bs="bs"),data.frame(x))[[1]]
> 
>   ## another example checking penalty numerically...
>   m <- c(4,2); k <- 15; b <- runif(k)
>   sm <- smoothCon(s(x,bs="bs",m=m,k=k),data.frame(x),
+                   scale.penalty=FALSE)[[1]]
>   sm$deriv <- m[2]
>   h0 <- 1e-3; xk <- sm$knots[(m[1]+1):(k+1)]
>   Xp <- PredictMat(sm,data.frame(x=seq(xk[1]+h0/2,max(xk)-h0/2,h0)))
>   sum((Xp%*%b)^2*h0) ## numerical approximation to penalty
[1] 1510.832
>   b%*%sm$S[[1]]%*%b  ## `exact' version
         [,1]
[1,] 1510.829
> 
>   ## ...repeated with uneven knot spacing...
>   m <- c(4,2); k <- 15; b <- runif(k)
>   ## produce the required 20 unevenly spaced knots...
>   knots <- data.frame(x=c(-.4,-.3,-.2,-.1,-.001,.05,.15,
+         .21,.3,.32,.4,.6,.65,.75,.9,1.001,1.1,1.2,1.3,1.4))
>   sm <- smoothCon(s(x,bs="bs",m=m,k=k),data.frame(x),
+         knots=knots,scale.penalty=FALSE)[[1]]
>   sm$deriv <- m[2]
>   h0 <- 1e-3; xk <- sm$knots[(m[1]+1):(k+1)]
>   Xp <- PredictMat(sm,data.frame(x=seq(xk[1]+h0/2,max(xk)-h0/2,h0)))
>   sum((Xp%*%b)^2*h0) ## numerical approximation to penalty
[1] 4255.34
>   b%*%sm$S[[1]]%*%b  ## `exact' version
         [,1]
[1,] 4255.341
> 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.cr.smooth.spec")
> ### * smooth.construct.cr.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.cr.smooth.spec
> ### Title: Penalized Cubic regression splines in GAMs
> ### Aliases: smooth.construct.cr.smooth.spec
> ###   smooth.construct.cs.smooth.spec smooth.construct.cc.smooth.spec
> ###   cubic.regression.spline cyclic.cubic.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## cyclic spline example...
>   require(mgcv)
>   set.seed(6)
>   x <- sort(runif(200)*10)
>   z <- runif(200)
>   f <- sin(x*2*pi/10)+.5
>   y <- rpois(exp(f),exp(f)) 
> 
> ## finished simulating data, now fit model...
>   b <- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
+                       knots=list(x=seq(0,10,length=12)))
> ## or more simply
>    b <- gam(y ~ s(x,bs="cc",k=12) + s(z),family=poisson,
+                       knots=list(x=c(0,10)))
> 
> ## plot results...
>   par(mfrow=c(2,2))
>   plot(x,y);plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(b,select=2,shade=TRUE);plot(fitted(b),residuals(b))
>   
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.ds.smooth.spec")
> ### * smooth.construct.ds.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ds.smooth.spec
> ### Title: Low rank Duchon 1977 splines
> ### Aliases: smooth.construct.ds.smooth.spec Predict.matrix.duchon.spline
> ###   Duchon.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> eg <- gamSim(2,n=200,scale=.05)
Bivariate smoothing example
> attach(eg)
> op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
> b0 <- gam(y~s(x,z,bs="ds",m=c(2,0),k=50),data=data)  ## tps
> b <- gam(y~s(x,z,bs="ds",m=c(1,.5),k=50),data=data)  ## first deriv penalty
> b1 <- gam(y~s(x,z,bs="ds",m=c(2,.5),k=50),data=data) ## modified 2nd deriv
> 
> persp(truth$x,truth$z,truth$f,theta=30) ## truth
> vis.gam(b0,theta=30)
> vis.gam(b,theta=30)
> vis.gam(b1,theta=30)
> 
> detach(eg)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.fs.smooth.spec")
> ### * smooth.construct.fs.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.fs.smooth.spec
> ### Title: Factor smooth interactions in GAMs
> ### Aliases: smooth.construct.fs.smooth.spec Predict.matrix.fs.interaction
> ###   factor.smooth.interaction
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> ## simulate data...
> f0 <- function(x) 2 * sin(pi * x)
> f1 <- function(x,a=2,b=-1) exp(a * x)+b
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 500;nf <- 25
> fac <- sample(1:nf,n,replace=TRUE)
> x0 <- runif(n);x1 <- runif(n);x2 <- runif(n)
> a <- rnorm(nf)*.2 + 2;b <- rnorm(nf)*.5
> f <- f0(x0) + f1(x1,a[fac],b[fac]) + f2(x2)
> fac <- factor(fac)
> y <- f + rnorm(n)*2
> ## so response depends on global smooths of x0 and 
> ## x2, and a smooth of x1 for each level of fac.
> 
> ## fit model (note p-values not available when fit 
> ## using gamm)...
> bm <- gamm(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20))
> plot(bm$gam,pages=1)
> summary(bm$gam)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1, fac, bs = "fs", k = 5) + s(x2, k = 20)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.6537     0.1867   40.99   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
             edf Ref.df     F p-value    
s(x0)      3.927  3.927 17.54  <2e-16 ***
s(x1,fac) 41.845     NA    NA      NA    
s(x2)     12.434 12.434 81.26  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.784   
  Scale est. = 3.7696    n = 500
> 
> ## Could also use...
> ## b <- gam(y~s(x0)+ s(x1,fac,bs="fs",k=5)+s(x2,k=20),method="ML")
> ## ... but its slower (increasingly so with increasing nf)
> ## b <- gam(y~s(x0)+ t2(x1,fac,bs=c("tp","re"),k=5,full=TRUE)+
> ##        s(x2,k=20),method="ML"))
> ## ... is exactly equivalent. 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.gp.smooth.spec")
> ### * smooth.construct.gp.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.gp.smooth.spec
> ### Title: Low rank Gaussian process smooths
> ### Aliases: smooth.construct.gp.smooth.spec Predict.matrix.gp.smooth
> ###   gp.smooth
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> eg <- gamSim(2,n=200,scale=.05)
Bivariate smoothing example
> attach(eg)
> op <- par(mfrow=c(2,2),mar=c(4,4,1,1))
> b0 <- gam(y~s(x,z,k=50),data=data)  ## tps
> b <- gam(y~s(x,z,bs="gp",k=50),data=data)  ## Matern spline default range
> b1 <- gam(y~s(x,z,bs="gp",k=50,m=c(1,.5)),data=data)  ## spherical 
> 
> persp(truth$x,truth$z,truth$f,theta=30) ## truth
> vis.gam(b0,theta=30)
> vis.gam(b,theta=30)
> vis.gam(b1,theta=30)
> 
> detach(eg)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.mrf.smooth.spec")
> ### * smooth.construct.mrf.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.mrf.smooth.spec
> ### Title: Markov Random Field Smooths
> ### Aliases: smooth.construct.mrf.smooth.spec Predict.matrix.mrf.smooth mrf
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## Load Columbus Ohio crime data (see ?columbus for details and credits)
> data(columb)       ## data frame
> data(columb.polys) ## district shapes list
> xt <- list(polys=columb.polys) ## neighbourhood structure info for MRF
> par(mfrow=c(2,2))
> ## First a full rank MRF...
> b <- gam(crime ~ s(district,bs="mrf",xt=xt),data=columb,method="REML")
> plot(b,scheme=1)
> ## Compare to reduced rank version...
> b <- gam(crime ~ s(district,bs="mrf",k=20,xt=xt),data=columb,method="REML")
> plot(b,scheme=1)
> ## An important covariate added...
> b <- gam(crime ~ s(district,bs="mrf",k=20,xt=xt)+s(income),
+          data=columb,method="REML")
> plot(b,scheme=c(0,1))
> 
> ## plot fitted values by district
> par(mfrow=c(1,1))
> fv <- fitted(b)
> names(fv) <- as.character(columb$district)
> polys.plot(columb.polys,fv)
> 
> ## Examine an example neighbourhood list - this one auto-generated from
> ## 'polys' above.
> 
> nb <- b$smooth[[1]]$xt$nb 
> head(nb) 
$`0`
[1] 2 3

$`1`
[1] 1 3 4

$`2`
[1] 1 2 4 5

$`3`
[1] 2 3 5 8

$`4`
[1]  3  4  6  8  9 11 15 16

$`5`
[1] 5 9

> names(nb) ## these have to match the factor levels of the smooth
 [1] "0"  "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14"
[16] "15" "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29"
[31] "30" "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44"
[46] "45" "46" "47" "48"
> ## look at the indices of the neighbours of the first entry,
> ## named '0'...
> nb[['0']] ## by name
[1] 2 3
> nb[[1]]   ## same by index 
[1] 2 3
> ## ... and get the names of these neighbours from their indices...
> names(nb)[nb[['0']]]
[1] "1" "2"
> b1 <- gam(crime ~ s(district,bs="mrf",k=20,xt=list(nb=nb))+s(income),
+          data=columb,method="REML")
> b1 ## fit unchanged

Family: gaussian 
Link function: identity 

Formula:
crime ~ s(district, bs = "mrf", k = 20, xt = list(nb = nb)) + 
    s(income)

Estimated degrees of freedom:
5.43 1.00  total = 7.43 

REML score: 187.3534     
> plot(b1) ## but now there is no information with which to plot the mrf
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.ps.smooth.spec")
> ### * smooth.construct.ps.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.ps.smooth.spec
> ### Title: P-splines in GAMs
> ### Aliases: smooth.construct.ps.smooth.spec
> ###   smooth.construct.cp.smooth.spec p.spline cyclic.p.spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam
> ## cyclic example ...
>   require(mgcv)
>   set.seed(6)
>   x <- sort(runif(200)*10)
>   z <- runif(200)
>   f <- sin(x*2*pi/10)+.5
>   y <- rpois(exp(f),exp(f)) 
> 
> ## finished simulating data, now fit model...
>   b <- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson)
> 
> ## example with supplied knot ranges for x and z (can do just one)
>   b <- gam(y ~ s(x,bs="cp") + s(z,bs="ps"),family=poisson,
+            knots=list(x=c(0,10),z=c(0,1))) 
> 
> ## example with supplied knots...
>   bk <- gam(y ~ s(x,bs="cp",k=12) + s(z,bs="ps",k=13),family=poisson,
+                       knots=list(x=seq(0,10,length=13),z=(-3):13/10))
> 
> ## plot results...
>   par(mfrow=c(2,2))
>   plot(b,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(b,select=2,shade=TRUE);lines(z,0*z,col=2)
>   plot(bk,select=1,shade=TRUE);lines(x,f-mean(f),col=2)
>   plot(bk,select=2,shade=TRUE);lines(z,0*z,col=2)
>   
> ## Example using montonic constraints via the SCOP-spline
> ## construction, and of computng derivatives...
>   x <- seq(0,1,length=100); dat <- data.frame(x)
>   sspec <- s(x,bs="ps")
>   sspec$mono <- 1
>   sm <- smoothCon(sspec,dat)[[1]]
>   sm$deriv <- 1
>   Xd <- PredictMat(sm,dat)
> ## generate random coeffients in the unconstrainted 
> ## parameterization...
>   b <- runif(10)*3-2.5
> ## exponentiate those parameters indicated by sm$g.index 
> ## to obtain coefficients meeting the constraints...
>   b[sm$g.index] <- exp(b[sm$g.index]) 
> ## plot monotonic spline and its derivative
>   par(mfrow=c(2,2))
>   plot(x,sm$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd%*%b,type="l",ylab="f'(x)")
> ## repeat for decrease...
>   sspec$mono <- -1
>   sm1 <- smoothCon(sspec,dat)[[1]]
>   sm1$deriv <- 1
>   Xd1 <- PredictMat(sm1,dat)
>   plot(x,sm1$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd1%*%b,type="l",ylab="f'(x)")
> 
> ## Now with sum to zero constraints as well...
>   sspec$mono <- 1
>   sm <- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
>   sm$deriv <- 1
>   Xd <- PredictMat(sm,dat)
>   b <- b[-1] ## dropping first param
>   plot(x,sm$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd%*%b,type="l",ylab="f'(x)")
>   
>   sspec$mono <- -1
>   sm1 <- smoothCon(sspec,dat,absorb.cons=TRUE)[[1]]
>   sm1$deriv <- 1
>   Xd1 <- PredictMat(sm1,dat)
>   plot(x,sm1$X%*%b,type="l",ylab="f(x)")
>   plot(x,Xd1%*%b,type="l",ylab="f'(x)")
>   
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.re.smooth.spec")
> ### * smooth.construct.re.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.re.smooth.spec
> ### Title: Simple random effects in GAMs
> ### Aliases: smooth.construct.re.smooth.spec Predict.matrix.random.effect
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam.vcomp
> 
> require(mgcv)
> ## simulate simple random effect example
> set.seed(4)
> nb <- 50; n <- 400
> b <- rnorm(nb)*2 ## random effect
> r <- sample(1:nb,n,replace=TRUE) ## r.e. levels
> y <- 2 + b[r] + rnorm(n)
> r <- factor(r)
> ## fit model....
> b <- gam(y ~ s(r,bs="re"),method="REML")
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

        std.dev     lower    upper
s(r)  1.8519446 1.5089009 2.272978
scale 0.9510702 0.8831918 1.024165

Rank: 2/2
> 
> ## example with supplied precision matrices...
> b <- c(rnorm(nb/2)*2,rnorm(nb/2)*.5) ## random effect now with 2 variances
> r <- sample(1:nb,n,replace=TRUE) ## r.e. levels
> y <- 2 + b[r] + rnorm(n)
> r <- factor(r)
> ## known precision matrix components...
> S <- list(diag(rep(c(1,0),each=nb/2)),diag(rep(c(0,1),each=nb/2)))
> b <- gam(y ~ s(r,bs="re",xt=list(S=S,rank=c(nb/2,nb/2))),method="REML")
> gam.vcomp(b)

Standard deviations and 0.95 confidence intervals:

        std.dev     lower    upper
s(r)1 2.1393499 1.6065993 2.848762
s(r)2 0.4255335 0.2650868 0.683092
scale 1.0228647 0.9498860 1.101450

Rank: 3/3
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(r, bs = "re", xt = list(S = S, rank = c(nb/2, nb/2)))

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   2.1070     0.1071   19.67   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
       edf Ref.df     F p-value    
s(r) 38.36     49 17.23  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.679   Deviance explained =   71%
-REML = 631.76  Scale est. = 1.0463    n = 400
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.so.smooth.spec")
> ### * smooth.construct.so.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.so.smooth.spec
> ### Title: Soap film smoother constructer
> ### Aliases: smooth.construct.so.smooth.spec
> ###   smooth.construct.sf.smooth.spec smooth.construct.sw.smooth.spec soap
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> require(mgcv)
> 
> ##########################
> ## simple test function...
> ##########################
> 
> fsb <- list(fs.boundary())
> nmax <- 100
> ## create some internal knots...
> knots <- data.frame(v=rep(seq(-.5,3,by=.5),4),
+                     w=rep(c(-.6,-.3,.3,.6),rep(8,4)))
> ## Simulate some fitting data, inside boundary...
> set.seed(0)
> n<-600
> v <- runif(n)*5-1;w<-runif(n)*2-1
> y <- fs.test(v,w,b=1)
> names(fsb[[1]]) <- c("v","w")
> ind <- inSide(fsb,x=v,y=w) ## remove outsiders
> y <- y + rnorm(n)*.3 ## add noise
> y <- y[ind];v <- v[ind]; w <- w[ind] 
> n <- length(y)
> 
> par(mfrow=c(3,2))
> ## plot boundary with knot and data locations
> plot(fsb[[1]]$v,fsb[[1]]$w,type="l");points(knots,pch=20,col=2)
> points(v,w,pch=".");
> 
> ## Now fit the soap film smoother. 'k' is dimension of boundary smooth.
> ## boundary supplied in 'xt', and knots in 'knots'...
>  
> nmax <- 100 ## reduced from default for speed.
> b <- gam(y~s(v,w,k=30,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)
> 
> plot(b) ## default plot
> plot(b,scheme=1)
> plot(b,scheme=2)
> plot(b,scheme=3)
> 
> vis.gam(b,plot.type="contour")
> 
> ################################
> # Fit same model in two parts...
> ################################
> 
> par(mfrow=c(2,2))
> vis.gam(b,plot.type="contour")
> 
> b1 <- gam(y~s(v,w,k=30,bs="sf",xt=list(bnd=fsb,nmax=nmax))+
+             s(v,w,k=30,bs="sw",xt=list(bnd=fsb,nmax=nmax)) ,knots=knots)
> vis.gam(b,plot.type="contour")
> plot(b1)
>  
> ##################################################
> ## Now an example with known boundary condition...
> ##################################################
> 
> ## Evaluate known boundary condition at boundary nodes...
> fsb[[1]]$f <- fs.test(fsb[[1]]$v,fsb[[1]]$w,b=1,exclude=FALSE)
> 
> ## Now fit the smooth...
> bk <- gam(y~s(v,w,bs="so",xt=list(bnd=fsb,nmax=nmax)),knots=knots)
> plot(bk) ## default plot
> 
> ##########################################
> ## tensor product example...
> ##########################################
> 
> #############################
> # nested boundary example...
> #############################
> 
> bnd <- list(list(x=0,y=0),list(x=0,y=0))
> seq(0,2*pi,length=100) -> theta
> bnd[[1]]$x <- sin(theta);bnd[[1]]$y <- cos(theta)
> bnd[[2]]$x <- .3 + .3*sin(theta);
> bnd[[2]]$y <- .3 + .3*cos(theta)
> plot(bnd[[1]]$x,bnd[[1]]$y,type="l")
> lines(bnd[[2]]$x,bnd[[2]]$y)
> 
> ## setup knots
> k <- 8
> xm <- seq(-1,1,length=k);ym <- seq(-1,1,length=k)
> x=rep(xm,k);y=rep(ym,rep(k,k))
> ind <- inSide(bnd,x,y)
> knots <- data.frame(x=x[ind],y=y[ind])
> points(knots$x,knots$y)
> 
> ## a test function
> 
> f1 <- function(x,y) {
+   exp(-(x-.3)^2-(y-.3)^2)
+ }
> 
> ## plot the test function within the domain 
> par(mfrow=c(2,3))
> m<-100;n<-100 
> xm <- seq(-1,1,length=m);yn<-seq(-1,1,length=n)
> x <- rep(xm,n);y<-rep(yn,rep(m,n))
> ff <- f1(x,y)
> ind <- inSide(bnd,x,y)
> ff[!ind] <- NA
> image(xm,yn,matrix(ff,m,n),xlab="x",ylab="y")
> contour(xm,yn,matrix(ff,m,n),add=TRUE)
> lines(bnd[[1]]$x,bnd[[1]]$y,lwd=2);lines(bnd[[2]]$x,bnd[[2]]$y,lwd=2)
> 
> ## Simulate data by noisy sampling from test function...
> 
> set.seed(1)
> x <- runif(300)*2-1;y <- runif(300)*2-1
> ind <- inSide(bnd,x,y)
> x <- x[ind];y <- y[ind]
> n <- length(x)
> z <- f1(x,y) + rnorm(n)*.1
> 
> ## Fit a soap film smooth to the noisy data
> nmax <- 60
> b <- gam(z~s(x,y,k=c(30,15),bs="so",xt=list(bnd=bnd,nmax=nmax)),
+          knots=knots,method="REML")
> plot(b) ## default plot
> vis.gam(b,plot.type="contour") ## prettier version
> 
> ## trying out separated fits....
> ba <- gam(z~s(x,y,k=c(30,15),bs="sf",xt=list(bnd=bnd,nmax=nmax))+
+           s(x,y,k=c(30,15),bs="sw",xt=list(bnd=bnd,nmax=nmax)),
+ 	  knots=knots,method="REML")
> plot(ba)
> vis.gam(ba,plot.type="contour")
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.sos.smooth.spec")
> ### * smooth.construct.sos.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.sos.smooth.spec
> ### Title: Splines on the sphere
> ### Aliases: smooth.construct.sos.smooth.spec Predict.matrix.sos.smooth
> ###   Spherical.Spline
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> set.seed(0)
> n <- 400
> 
> f <- function(la,lo) { ## a test function...
+   sin(lo)*cos(la-.3)
+ }
> 
> ## generate with uniform density on sphere...  
> lo <- runif(n)*2*pi-pi ## longitude
> la <- runif(3*n)*pi-pi/2
> ind <- runif(3*n)<=cos(la)
> la <- la[ind];
> la <- la[1:n]
> 
> ff <- f(la,lo)
> y <- ff + rnorm(n)*.2 ## test data
> 
> ## generate data for plotting truth...
> lam <- seq(-pi/2,pi/2,length=30)
> lom <- seq(-pi,pi,length=60)
> gr <- expand.grid(la=lam,lo=lom)
> fz <- f(gr$la,gr$lo)
> zm <- matrix(fz,30,60)
> 
> require(mgcv)
> dat <- data.frame(la = la *180/pi,lo = lo *180/pi,y=y)
> 
> ## fit spline on sphere model...
> bp <- gam(y~s(la,lo,bs="sos",k=60),data=dat)
> 
> ## pure knot based alternative...
> ind <- sample(1:n,100)
> bk <- gam(y~s(la,lo,bs="sos",k=60),
+       knots=list(la=dat$la[ind],lo=dat$lo[ind]),data=dat)
> 
> b <- bp
> 
> cor(fitted(b),ff)
[1] 0.9950604
> 
> ## plot results and truth...
> 
> pd <- data.frame(la=gr$la*180/pi,lo=gr$lo*180/pi)
> fv <- matrix(predict(b,pd),30,60)
> 
> par(mfrow=c(2,2),mar=c(4,4,1,1))
> contour(lom,lam,t(zm))
> contour(lom,lam,t(fv))
> plot(bp,rug=FALSE)
> plot(bp,scheme=1,theta=-30,phi=20,pch=19,cex=.5)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.construct.t2.smooth.spec")
> ### * smooth.construct.t2.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.t2.smooth.spec
> ### Title: Tensor product smoothing constructor
> ### Aliases: smooth.construct.t2.smooth.spec
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?t2
> 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.tensor.smooth.spec")
> ### * smooth.construct.tensor.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.tensor.smooth.spec
> ### Title: Tensor product smoothing constructor
> ### Aliases: smooth.construct.tensor.smooth.spec
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## see ?gam
> 
> 
> 
> 
> cleanEx()
> nameEx("smooth.construct.tp.smooth.spec")
> ### * smooth.construct.tp.smooth.spec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.construct.tp.smooth.spec
> ### Title: Penalized thin plate regression splines in GAMs
> ### Aliases: smooth.construct.tp.smooth.spec
> ###   smooth.construct.ts.smooth.spec tprs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv); n <- 100; set.seed(2)
> x <- runif(n); y <- x + x^2*.2 + rnorm(n) *.1
> 
> ## is smooth significantly different from straight line?
> summary(gam(y~s(x,m=c(2,0))+x,method="REML")) ## not quite

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x, m = c(2, 0)) + x

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -0.02133    0.05315  -0.401    0.689    
x            1.18249    0.10564  11.193   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df     F p-value  
s(x) 0.9334      8 0.304   0.076 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.91   Deviance explained = 91.1%
-REML = -70.567  Scale est. = 0.012767  n = 100
> 
> ## is smooth significatly different from zero?
> summary(gam(y~s(x),method="REML")) ## yes!

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   0.5600     0.0113   49.56   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
       edf Ref.df     F p-value    
s(x) 1.933  2.417 413.1  <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.91   Deviance explained = 91.1%
-REML = -69.353  Scale est. = 0.012767  n = 100
> 
> ## Fool bam(...,discrete=TRUE) into (strange) nested
> ## model fit...
> set.seed(2) ## simulate some data... 
> dat <- gamSim(1,n=400,dist="normal",scale=2)
Gu & Wahba 4 term additive model
> dat$x1a <- dat$x1 ## copy x1 so bam allows 2 copies of x1
> ## Following removes identifiability problem, by removing
> ## linear terms from second smooth, and then re-inserting
> ## the one that was not a duplicate (x2)...
> b <- bam(y~s(x0,x1)+s(x1a,x2,m=c(2,0))+x2,data=dat,discrete=TRUE)
> 
> ## example of knot based tprs...
> k <- 10; m <- 2
> y <- y[order(x)];x <- x[order(x)]
> b <- gam(y~s(x,k=k,m=m),method="REML",
+          knots=list(x=seq(0,1,length=k)))
> X <- model.matrix(b)
> par(mfrow=c(1,2))
> plot(x,X[,1],ylim=range(X),type="l")
> for (i in 2:ncol(X)) lines(x,X[,i],col=i)
> 
> ## compare with eigen based (default)
> b1 <- gam(y~s(x,k=k,m=m),method="REML")
> X1 <- model.matrix(b1)
> plot(x,X1[,1],ylim=range(X1),type="l")
> for (i in 2:ncol(X1)) lines(x,X1[,i],col=i)
> ## see ?gam
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("smooth.info")
> ### * smooth.info
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.info
> ### Title: Generic function to provide extra information about smooth
> ###   specification
> ### Aliases: smooth.info
> ### Keywords: models smooth regression
> 
> ### ** Examples
> # See smooth.construct examples
> spec <- s(a,bs="re")
> class(spec)
[1] "re.smooth.spec"
> spec$tensor.possible
NULL
> spec <- smooth.info(spec)
> spec$tensor.possible
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("smooth.terms")
> ### * smooth.terms
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth.terms
> ### Title: Smooth terms in GAM
> ### Aliases: smooth.terms
> ### Keywords: regression
> 
> ### ** Examples
> 
> ## see examples for gam and gamm
> 
> 
> 
> cleanEx()
> nameEx("smooth2random")
> ### * smooth2random
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smooth2random
> ### Title: Convert a smooth to a form suitable for estimating as random
> ###   effect
> ### Aliases: smooth2random
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## Simple type 1 'lme' style...
> library(mgcv)
> x <- runif(30)
> sm <- smoothCon(s(x),data.frame(x=x))[[1]]
> smooth2random(sm,"")
$rand
$rand$g
Uninitialized positive definite matrix structure of class pdIdnot.


$Xf
             [,1] [,2]
 [1,] -0.84061128    1
 [2,] -0.47337394    1
 [3,]  0.21804088    1
 [4,]  1.37317285    1
 [5,] -1.06046315    1
 [6,]  1.33935428    1
 [7,]  1.49878547    1
 [8,]  0.52096642    1
 [9,]  0.41183141    1
[10,] -1.54233527    1
[11,] -1.04567709    1
[12,] -1.14700710    1
[13,]  0.61129891    1
[14,] -0.43210932    1
[15,]  0.89656838    1
[16,] -0.04082831    1
[17,]  0.71668600    1
[18,]  1.66147257    1
[19,] -0.44612345    1
[20,]  0.92275976    1
[21,]  1.46444357    1
[22,] -1.02443153    1
[23,]  0.48953861    1
[24,] -1.32268286    1
[25,] -0.83471425    1
[26,] -0.42518456    1
[27,] -1.70903560    1
[28,] -0.43801928    1
[29,]  1.24050081    1
[30,] -0.58282293    1

$trans.U
              [,1]         [,2]         [,3]         [,4]        [,5]
 [1,]  0.010831064 -0.023920186 -0.008413764 -0.076432759  0.05898534
 [2,]  0.013385074 -0.005852515 -0.031566931 -0.003999085 -0.08273579
 [3,] -0.003926992  0.015010925 -0.001801338  0.065961047 -0.09264351
 [4,]  0.014109644 -0.011186157 -0.034817079 -0.047712037  0.98809375
 [5,]  0.008418740 -0.020216617 -0.010896085 -0.993320671 -0.05866250
 [6,] -0.022090091  0.001916760 -0.998577708  0.012911157 -0.03121907
 [7,]  0.012966788 -0.999220778 -0.001234184  0.023639824 -0.01244454
 [8,] -0.999380828 -0.013731925  0.020966080 -0.010161081  0.01387991
 [9,]  0.000000000  0.000000000  0.000000000  0.000000000  0.00000000
[10,]  0.000000000  0.000000000  0.000000000  0.000000000  0.00000000
               [,6]          [,7]         [,8] [,9] [,10]
 [1,]  0.2478649880 -0.0212919461  0.963342143    0     0
 [2,]  0.1317840142  0.9871673047 -0.007911993    0     0
 [3,] -0.9513444935  0.1216630831  0.258774178    0     0
 [4,] -0.0979430149  0.0940217274 -0.037748377    0     0
 [5,] -0.0785718190  0.0005197924 -0.055683423    0     0
 [6,] -0.0001191704 -0.0342190332 -0.006215276    0     0
 [7,] -0.0183924442 -0.0047730356 -0.017703219    0     0
 [8,]  0.0059088856  0.0145389245  0.008223347    0     0
 [9,]  0.0000000000  0.0000000000  0.000000000    0     1
[10,]  0.0000000000  0.0000000000  0.000000000    1     0

$trans.D
 [1] 0.06398597 0.07747095 0.09642774 0.13917622 0.18136619 0.25910870
 [7] 0.44342705 1.24728693 1.00000000 1.00000000

$fixed
[1] FALSE

$rind
[1] 1 2 3 4 5 6 7 8

$rinc
[1] 8 8 8 8 8 8 8 8

$pen.ind
 [1] 1 1 1 1 1 1 1 1 0 0

> 
> ## Now type 2 'lme4' style...
> z <- runif(30)
> dat <- data.frame(x=x,z=z)
> sm <- smoothCon(t2(x,z),dat)[[1]]
> re <- smooth2random(sm,"",2)
> str(re)
List of 7
 $ rand   :List of 3
  ..$ Xr  : num [1:30, 1:9] -0.003305 0.001115 0.003868 0.002479 0.000995 ...
  .. ..- attr(*, "s.label")= chr "t2(x,z)"
  ..$ Xr.0: num [1:30, 1:6] -0.00799 -0.00877 -0.01007 0.01862 0.002 ...
  .. ..- attr(*, "s.label")= chr "t2(x,z)"
  ..$ Xr.1: num [1:30, 1:6] 0.01626 -0.00499 -0.01509 0.00523 0.01958 ...
  .. ..- attr(*, "s.label")= chr "t2(x,z)"
 $ trans.D: num [1:25] 0.0349 0.0349 0.0349 0.0349 0.0349 ...
 $ Xf     : num [1:30, 1:4] 1.12 1.12 1.12 1.12 1.12 ...
 $ fixed  : logi FALSE
 $ rind   : int [1:21] 1 2 3 4 5 6 7 8 9 10 ...
 $ rinc   : num [1:21] 21 21 21 21 21 21 21 21 21 21 ...
 $ pen.ind: num [1:25] 1 1 1 1 1 1 1 1 1 2 ...
> 
> ## For prediction after fitting we might transform parameters back to
> ## original parameterization using 'rind', 'trans.D' and 'trans.U',
> ## and call PredictMat(sm,newdata) to get the prediction matrix to
> ## multiply these transformed parameters by.
> ## Alternatively we could obtain fixed and random effect Prediction
> ## matrices corresponding to the results from smooth2random, which
> ## can be used with the fit parameters without transforming them.
> ## The following shows how...
> 
> s2rPred <- function(sm,re,data) {
+ ## Function to aid prediction from smooths represented as type==2
+ ## random effects. re must be the result of smooth2random(sm,...,type=2).
+   X <- PredictMat(sm,data)   ## get prediction matrix for new data
+   ## transform to r.e. parameterization
+   if (!is.null(re$trans.U)) X <- X%*%re$trans.U
+   X <- t(t(X)*re$trans.D)
+   ## re-order columns according to random effect re-ordering...
+   X[,re$rind] <- X[,re$pen.ind!=0] 
+   ## re-order penalization index in same way  
+   pen.ind <- re$pen.ind; pen.ind[re$rind] <- pen.ind[pen.ind>0]
+   ## start return object...
+   r <- list(rand=list(),Xf=X[,which(re$pen.ind==0),drop=FALSE])
+   for (i in 1:length(re$rand)) { ## loop over random effect matrices
+     r$rand[[i]] <- X[,which(pen.ind==i),drop=FALSE]
+     attr(r$rand[[i]],"s.label") <- attr(re$rand[[i]],"s.label")
+   }
+   names(r$rand) <- names(re$rand)
+   r
+ } ## s2rPred
> 
> ## use function to obtain prediction random and fixed effect matrices
> ## for first 10 elements of 'dat'. Then confirm that these match the
> ## first 10 rows of the original model matrices, as they should...
> 
> r <- s2rPred(sm,re,dat[1:10,])
> range(r$Xf-re$Xf[1:10,])
[1] -6.661338e-16  8.881784e-16
> range(r$rand[[1]]-re$rand[[1]][1:10,])
[1] -2.775558e-17  2.775558e-17
> 
> 
> 
> 
> cleanEx()
> nameEx("smoothCon")
> ### * smoothCon
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smoothCon
> ### Title: Prediction/Construction wrapper functions for GAM smooth terms
> ### Aliases: smoothCon PredictMat
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> ## example of using smoothCon and PredictMat to set up a basis
> ## to use for regression and make predictions using the result
> library(MASS) ## load for mcycle data.
> ## set up a smoother...
> sm <- smoothCon(s(times,k=10),data=mcycle,knots=NULL)[[1]]
> ## use it to fit a regression spline model...
> beta <- coef(lm(mcycle$accel~sm$X-1))
> with(mcycle,plot(times,accel)) ## plot data
> times <- seq(0,60,length=200)  ## creat prediction times
> ## Get matrix mapping beta to spline prediction at 'times'
> Xp <- PredictMat(sm,data.frame(times=times))
> lines(times,Xp%*%beta) ## add smooth to plot
> 
> ## Same again but using a penalized regression spline of
> ## rank 30....
> sm <- smoothCon(s(times,k=30),data=mcycle,knots=NULL)[[1]]
> E <- t(mroot(sm$S[[1]])) ## square root penalty
> X <- rbind(sm$X,0.1*E) ## augmented model matrix
> y <- c(mcycle$accel,rep(0,nrow(E))) ## augmented data
> beta <- coef(lm(y~X-1)) ## fit penalized regression spline
> Xp <- PredictMat(sm,data.frame(times=times)) ## prediction matrix
> with(mcycle,plot(times,accel)) ## plot data
> lines(times,Xp%*%beta) ## overlay smooth
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("sp.vcov")
> ### * sp.vcov
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sp.vcov
> ### Title: Extract smoothing parameter estimator covariance matrix from
> ###   (RE)ML GAM fit
> ### Aliases: sp.vcov
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 100
> x <- runif(n);z <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=10)+s(z),knots=list(x=seq(0,1,length=10)),
+            method="REML")
> sp.vcov(mod)
            [,1]         [,2]        [,3]
[1,]  0.33856935  -0.35798822  0.02229551
[2,] -0.35798822 993.26256579 -0.04377858
[3,]  0.02229551  -0.04377858  0.02187637
> 
> 
> 
> cleanEx()
> nameEx("step.gam")
> ### * step.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step.gam
> ### Title: Alternatives to step.gam
> ### Aliases: step.gam
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## an example of GCV based model selection as
> ## an alternative to stepwise selection, using
> ## shrinkage smoothers...
> library(mgcv)
> set.seed(0);n <- 400
> dat <- gamSim(1,n=n,scale=2)
Gu & Wahba 4 term additive model
> dat$x4 <- runif(n, 0, 1)
> dat$x5 <- runif(n, 0, 1)
> attach(dat)
> ## Note the increased gamma parameter below to favour
> ## slightly smoother models...
> b<-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
+    s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),gamma=1.4)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "ts") + s(x1, bs = "ts") + s(x2, bs = "ts") + 
    s(x3, bs = "ts") + s(x4, bs = "ts") + s(x5, bs = "ts")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1051   75.28   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0) 2.640e+00      9  2.390 2.46e-05 ***
s(x1) 2.151e+00      9 33.466  < 2e-16 ***
s(x2) 8.238e+00      9 83.322  < 2e-16 ***
s(x3) 1.675e-05      9  0.000   0.6331    
s(x4) 5.613e-01      9  0.204   0.0696 .  
s(x5) 1.331e-05      9  0.000   0.4676    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.724   Deviance explained = 73.4%
GCV = 4.7318  Scale est. = 4.4222    n = 400
> plot(b,pages=1)
> 
> ## Same again using REML/ML
> b<-gam(y~s(x0,bs="ts")+s(x1,bs="ts")+s(x2,bs="ts")+
+    s(x3,bs="ts")+s(x4,bs="ts")+s(x5,bs="ts"),method="REML")
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "ts") + s(x1, bs = "ts") + s(x2, bs = "ts") + 
    s(x3, bs = "ts") + s(x4, bs = "ts") + s(x5, bs = "ts")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1051   75.32   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
           edf Ref.df      F  p-value    
s(x0) 2.885762      9  2.522 1.88e-05 ***
s(x1) 2.400354      9 33.539  < 2e-16 ***
s(x2) 8.020248      9 83.252  < 2e-16 ***
s(x3) 0.000138      9  0.000    0.654    
s(x4) 0.696364      9  0.245    0.074 .  
s(x5) 0.001376      9  0.000    0.461    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.725   Deviance explained = 73.4%
-REML = 896.49  Scale est. = 4.4173    n = 400
> plot(b,pages=1)
> 
> ## And once more, but using the null space penalization
> b<-gam(y~s(x0,bs="cr")+s(x1,bs="cr")+s(x2,bs="cr")+
+    s(x3,bs="cr")+s(x4,bs="cr")+s(x5,bs="cr"),
+    method="REML",select=TRUE)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, bs = "cr") + s(x1, bs = "cr") + s(x2, bs = "cr") + 
    s(x3, bs = "cr") + s(x4, bs = "cr") + s(x5, bs = "cr")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.9150     0.1052   75.24   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
            edf Ref.df      F  p-value    
s(x0) 2.2254365      9  2.563 3.49e-06 ***
s(x1) 2.6764675      9 33.409  < 2e-16 ***
s(x2) 7.9724407      9 81.746  < 2e-16 ***
s(x3) 0.0005948      9  0.000   0.9794    
s(x4) 0.7706253      9  0.237   0.0824 .  
s(x5) 0.3861344      9  0.059   0.2319    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.724   Deviance explained = 73.4%
-REML = 889.08  Scale est. = 4.4268    n = 400
> plot(b,pages=1)
> 
> 
> detach(dat);rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("summary.gam")
> ### * summary.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary.gam
> ### Title: Summary for a GAM fit
> ### Aliases: summary.gam print.summary.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> 
> dat <- gamSim(1,n=200,scale=2) ## simulate data
Gu & Wahba 4 term additive model
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat)
> plot(b,pages=1)
> summary(b)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1525      48   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F p-value    
s(x0) 2.318  2.892  4.116  0.0105 *  
s(x1) 2.306  2.859 43.773  <2e-16 ***
s(x2) 7.655  8.524 31.805  <2e-16 ***
s(x3) 1.233  1.425  0.122  0.7503    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.687   Deviance explained = 70.9%
GCV = 5.0135  Scale est. = 4.6497    n = 200
> 
> ## now check the p-values by using a pure regression spline.....
> b.d <- round(summary(b)$edf)+1 ## get edf per smooth
> b.d <- pmax(b.d,3) # can't have basis dimension less than 3!
> bc<-gam(y~s(x0,k=b.d[1],fx=TRUE)+s(x1,k=b.d[2],fx=TRUE)+
+         s(x2,k=b.d[3],fx=TRUE)+s(x3,k=b.d[4],fx=TRUE),data=dat)
> plot(bc,pages=1)
> summary(bc)

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, k = b.d[1], fx = TRUE) + s(x1, k = b.d[2], fx = TRUE) + 
    s(x2, k = b.d[3], fx = TRUE) + s(x3, k = b.d[4], fx = TRUE)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   7.3182     0.1532   47.78   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
      edf Ref.df      F p-value    
s(x0)   2      2  6.723 0.00152 ** 
s(x1)   2      2 60.552 < 2e-16 ***
s(x2)   8      8 34.550 < 2e-16 ***
s(x3)   2      2  0.912 0.40371    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.685   Deviance explained = 70.7%
GCV = 5.0727  Scale est. = 4.6922    n = 200
> 
> ## Example where some p-values are less reliable...
> dat <- gamSim(6,n=200,scale=2)
4 term additive + random effectGu & Wahba 4 term additive model
> b <- gam(y~s(x0,m=1)+s(x1)+s(x2)+s(x3)+s(fac,bs="re"),data=dat)
> ## Here s(x0,m=1) can be penalized to zero, so p-value approximation
> ## cruder than usual...
> summary(b) 

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x0, m = 1) + s(x1) + s(x2) + s(x3) + s(fac, bs = "re")

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   15.277      1.541   9.915   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
         edf Ref.df       F  p-value    
s(x0)  5.003  8.000   4.437 9.38e-06 ***
s(x1)  2.301  2.861  46.024  < 2e-16 ***
s(x2)  6.856  7.947  46.522  < 2e-16 ***
s(x3)  1.000  1.000   0.341     0.56    
s(fac) 2.972  3.000 173.985  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =   0.85   Deviance explained = 86.3%
GCV = 4.6185  Scale est. = 4.1767    n = 200
> 
> ## p-value check - increase k to make this useful!
> k<-20;n <- 200;p <- rep(NA,k)
> for (i in 1:k)
+ { b<-gam(y~te(x,z),data=data.frame(y=rnorm(n),x=runif(n),z=runif(n)),
+          method="ML")
+   p[i]<-summary(b)$s.p[1]
+ }
> plot(((1:k)-0.5)/k,sort(p))
> abline(0,1,col=2)
> ks.test(p,"punif") ## how close to uniform are the p-values?

	One-sample Kolmogorov-Smirnov test

data:  p
D = 0.11195, p-value = 0.9397
alternative hypothesis: two-sided

> 
> ## A Gamma example, by modify `gamSim' output...
>  
> dat <- gamSim(1,n=400,dist="normal",scale=1)
Gu & Wahba 4 term additive model
> dat$f <- dat$f/4 ## true linear predictor 
> Ey <- exp(dat$f);scale <- .5 ## mean and GLM scale parameter
> ## Note that `shape' and `scale' in `rgamma' are almost
> ## opposite terminology to that used with GLM/GAM...
> dat$y <- rgamma(Ey*0,shape=1/scale,scale=Ey*scale)
> bg <- gam(y~ s(x0)+ s(x1)+s(x2)+s(x3),family=Gamma(link=log),
+           data=dat,method="REML")
> summary(bg)

Family: Gamma 
Link function: log 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Parametric coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.95252    0.03225   60.54   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
        edf Ref.df      F  p-value    
s(x0) 4.210  5.179  4.870 0.000287 ***
s(x1) 2.511  3.122 77.634  < 2e-16 ***
s(x2) 7.688  8.559 48.662  < 2e-16 ***
s(x3) 1.001  1.001  0.212 0.646119    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-sq.(adj) =  0.487   Deviance explained = 61.9%
-REML = 1145.9  Scale est. = 0.41609   n = 400
> 
> 
> 
> 
> cleanEx()
> nameEx("t2")
> ### * t2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: t2
> ### Title: Define alternative tensor product smooths in GAM formulae
> ### Aliases: t2
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> # following shows how tensor product deals nicely with 
> # badly scaled covariates (range of x 5% of range of z )
> require(mgcv)
> test1<-function(x,z,sx=0.3,sz=0.4)  
+ { x<-x*20
+   (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+   0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+ }
> n<-500
> old.par<-par(mfrow=c(2,2))
> x<-runif(n)/20;z<-runif(n);
> xs<-seq(0,1,length=30)/20;zs<-seq(0,1,length=30)
> pr<-data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth<-matrix(test1(pr$x,pr$z),30,30)
> f <- test1(x,z)
> y <- f + rnorm(n)*0.2
> b1<-gam(y~s(x,z))
> persp(xs,zs,truth);title("truth")
> vis.gam(b1);title("t.p.r.s")
> b2<-gam(y~t2(x,z))
> vis.gam(b2);title("tensor product")
> b3<-gam(y~t2(x,z,bs=c("tp","tp")))
> vis.gam(b3);title("tensor product")
> par(old.par)
> 
> test2<-function(u,v,w,sv=0.3,sw=0.4)  
+ { ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
+   0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
+ }
> n <- 500
> v <- runif(n);w<-runif(n);u<-runif(n)
> f <- test2(u,v,w)
> y <- f + rnorm(n)*0.2
> 
> ## tensor product of 2D Duchon spline and 1D cr spline
> m <- list(c(1,.5),0)
> b <- gam(y~t2(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))
> 
> ## look at the edf per penalty. "rr" denotes interaction term 
> ## (range space range space). "rn" is interaction of null space
> ## for u with range space for v,w...
> pen.edf(b) 
t2(v,w,u)rr t2(v,w,u)nr t2(v,w,u)rn 
  36.136432    0.483695   41.173645 
> 
> ## plot results...
> op <- par(mfrow=c(2,2))
> vis.gam(b,cond=list(u=0),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.33),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.67),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=1),color="heat",zlim=c(-0.2,3.5))
> par(op)
> 
> b <- gam(y~t2(v,w,u,k=c(25,5),d=c(2,1),bs=c("tp","cr"),full=TRUE),
+          method="ML")
> ## more penalties now. numbers in labels like "r1" indicate which 
> ## basis function of a null space is involved in the term. 
> pen.edf(b) 
 t2(v,w,u)rr  t2(v,w,u)1r  t2(v,w,u)2r  t2(v,w,u)3r  t2(v,w,u)r1  t2(v,w,u)r2 
3.058308e+01 2.899981e+00 2.612023e-01 5.691259e-06 1.884064e+01 4.424669e-06 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("te")
> ### * te
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: te
> ### Title: Define tensor product smooths or tensor product interactions in
> ###   GAM formulae
> ### Aliases: te ti
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> 
> # following shows how tensor pruduct deals nicely with 
> # badly scaled covariates (range of x 5% of range of z )
> require(mgcv)
> test1 <- function(x,z,sx=0.3,sz=0.4) { 
+   x <- x*20
+   (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
+   0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
+ }
> n <- 500
> old.par <- par(mfrow=c(2,2))
> x <- runif(n)/20;z <- runif(n);
> xs <- seq(0,1,length=30)/20;zs <- seq(0,1,length=30)
> pr <- data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
> truth <- matrix(test1(pr$x,pr$z),30,30)
> f <- test1(x,z)
> y <- f + rnorm(n)*0.2
> b1 <- gam(y~s(x,z))
> persp(xs,zs,truth);title("truth")
> vis.gam(b1);title("t.p.r.s")
> b2 <- gam(y~te(x,z))
> vis.gam(b2);title("tensor product")
> b3 <- gam(y~ ti(x) + ti(z) + ti(x,z))
> vis.gam(b3);title("tensor anova")
> 
> ## now illustrate partial ANOVA decomp...
> vis.gam(b3);title("full anova")
> b4 <- gam(y~ ti(x) + ti(x,z,mc=c(0,1))) ## note z constrained!
> vis.gam(b4);title("partial anova")
> plot(b4)
> 
> par(old.par)
> 
> ## now with a multivariate marginal....
> 
> test2<-function(u,v,w,sv=0.3,sw=0.4)  
+ { ((pi**sv*sw)*(1.2*exp(-(v-0.2)^2/sv^2-(w-0.3)^2/sw^2)+
+   0.8*exp(-(v-0.7)^2/sv^2-(w-0.8)^2/sw^2)))*(u-0.5)^2*20
+ }
> n <- 500
> v <- runif(n);w<-runif(n);u<-runif(n)
> f <- test2(u,v,w)
> y <- f + rnorm(n)*0.2
> # tensor product of 2D Duchon spline and 1D cr spline
> m <- list(c(1,.5),rep(0,0)) ## example of list form of m
> b <- gam(y~te(v,w,u,k=c(30,5),d=c(2,1),bs=c("ds","cr"),m=m))
> op <- par(mfrow=c(2,2))
> vis.gam(b,cond=list(u=0),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.33),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=.67),color="heat",zlim=c(-0.2,3.5))
> vis.gam(b,cond=list(u=1),color="heat",zlim=c(-0.2,3.5))
> par(op)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("tensor.prod.model.matrix")
> ### * tensor.prod.model.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tensor.prod.model.matrix
> ### Title: Row Kronecker product/ tensor product smooth construction
> ### Aliases: tensor.prod.model.matrix tensor.prod.penalties %.%
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## Dense row Kronecker product example...
> X <- list(matrix(0:3,2,2),matrix(c(5:8,0,0),2,3))
> tensor.prod.model.matrix(X)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0   10   14    0
[2,]    6    8    0   18   24    0
> X[[1]]%.%X[[2]]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0   10   14    0
[2,]    6    8    0   18   24    0
> 
> ## sparse equivalent...
> Xs <- lapply(X,as,"dgCMatrix")
> tensor.prod.model.matrix(Xs)
2 x 6 sparse Matrix of class "dgCMatrix"
                  
[1,] . . . 10 14 .
[2,] 6 8 . 18 24 .
> Xs[[1]]%.%Xs[[2]]
2 x 6 sparse Matrix of class "dgCMatrix"
                  
[1,] . . . 10 14 .
[2,] 6 8 . 18 24 .
> 
> S <- list(matrix(c(2,1,1,2),2,2),matrix(c(2,1,0,1,2,1,0,1,2),3,3))
> tensor.prod.penalties(S)
[[1]]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    0    0    1    0    0
[2,]    0    2    0    0    1    0
[3,]    0    0    2    0    0    1
[4,]    1    0    0    2    0    0
[5,]    0    1    0    0    2    0
[6,]    0    0    1    0    0    2

[[2]]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    2    1    0    0    0    0
[2,]    1    2    1    0    0    0
[3,]    0    1    2    0    0    0
[4,]    0    0    0    2    1    0
[5,]    0    0    0    1    2    1
[6,]    0    0    0    0    1    2

> ## Sparse equivalent...
> Ss <- lapply(S,as,"dgCMatrix")
> tensor.prod.penalties(Ss)
[[1]]
6 x 6 sparse Matrix of class "dgCMatrix"
                
[1,] 2 . . 1 . .
[2,] . 2 . . 1 .
[3,] . . 2 . . 1
[4,] 1 . . 2 . .
[5,] . 1 . . 2 .
[6,] . . 1 . . 2

[[2]]
6 x 6 sparse Matrix of class "dgCMatrix"
                
[1,] 2 1 . . . .
[2,] 1 2 1 . . .
[3,] . 1 2 . . .
[4,] . . . 2 1 .
[5,] . . . 1 2 1
[6,] . . . . 1 2

> 
> 
> 
> cleanEx()
> nameEx("trichol")
> ### * trichol
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trichol
> ### Title: Choleski decomposition of a tri-diagonal matrix
> ### Aliases: trichol
> ### Keywords: models smooth regression
> 
> ### ** Examples
> 
> require(mgcv)
> ## simulate some diagonals...
> set.seed(19); k <- 7
> ld <- runif(k)+1
> sd <- runif(k-1) -.5
> 
> ## get diagonals of chol factor...
> trichol(ld,sd)
$ld
[1] 1.056944 1.216233 1.254832 1.017814 1.164840 1.105459 1.105716

$sd
[1]  0.06933549  0.27677293  0.18014527 -0.09189728 -0.04316683  0.26319837

> 
> ## compare to dense matrix result...
> A <- diag(ld);for (i in 1:(k-1)) A[i,i+1] <- A[i+1,i] <- sd[i]
> R <- chol(A)
> diag(R);diag(R[,-1])
[1] 1.056944 1.216233 1.254832 1.017814 1.164840 1.105459 1.105716
[1]  0.06933549  0.27677293  0.18014527 -0.09189728 -0.04316683  0.26319837
> 
> 
> 
> 
> cleanEx()
> nameEx("trind.generator")
> ### * trind.generator
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: trind.generator
> ### Title: Generates index arrays for upper triangular storage
> ### Aliases: trind.generator
> 
> ### ** Examples
> 
> library(mgcv)
> A <- trind.generator(3)
> 
> # All permutations of c(1, 2, 3) point to the same index (5)
> A$i3[1, 2, 3] 
[1] 5
> A$i3[2, 1, 3]
[1] 5
> A$i3[2, 3, 1]
[1] 5
> A$i3[3, 1, 2]
[1] 5
> A$i3[1, 3, 2]
[1] 5
> 
> 
> 
> cleanEx()
> nameEx("twlss")
> ### * twlss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: twlss
> ### Title: Tweedie location scale family
> ### Aliases: twlss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(3)
> n<-400
> ## Simulate data...
> dat <- gamSim(1,n=n,dist="poisson",scale=.2)
Gu & Wahba 4 term additive model
> dat$y <- rTweedie(exp(dat$f),p=1.3,phi=.5) ## Tweedie response
> 
> ## Fit a fixed p Tweedie, with wrong link ...
> b <- gam(list(y~s(x0)+s(x1)+s(x2)+s(x3),~1,~1),family=twlss(),
+          data=dat)
> plot(b,pages=1)
> print(b)

Family: twlss 
Link function: log identity identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)
~1
~1

Estimated degrees of freedom:
3.35 3.07 7.92 1.01  total = 18.36 

REML score: 848.5016     
> 
> rm(dat)
> 
> 
> 
> cleanEx()
> nameEx("uniquecombs")
> ### * uniquecombs
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: uniquecombs
> ### Title: find the unique rows in a matrix
> ### Aliases: uniquecombs
> ### Keywords: models regression
> 
> ### ** Examples
> 
> require(mgcv)
> 
> ## matrix example...
> X <- matrix(c(1,2,3,1,2,3,4,5,6,1,3,2,4,5,6,1,1,1),6,3,byrow=TRUE)
> print(X)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    1    2    3
[3,]    4    5    6
[4,]    1    3    2
[5,]    4    5    6
[6,]    1    1    1
> Xu <- uniquecombs(X);Xu
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    1    3    2
[4,]    1    1    1
attr(,"index")
[1] 1 1 2 3 2 4
> ind <- attr(Xu,"index")
> ## find the value for row 3 of the original from Xu
> Xu[ind[3],];X[3,]
[1] 4 5 6
[1] 4 5 6
> 
> ## same with fixed output ordering
> Xu <- uniquecombs(X,TRUE);Xu
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    2    3
[3,]    1    3    2
[4,]    4    5    6
attr(,"index")
[1] 2 2 4 3 4 1
> ind <- attr(Xu,"index")
> ## find the value for row 3 of the original from Xu
> Xu[ind[3],];X[3,]
[1] 4 5 6
[1] 4 5 6
> 
> 
> ## data frame example...
> df <- data.frame(f=factor(c("er",3,"b","er",3,3,1,2,"b")),
+       x=c(.5,1,1.4,.5,1,.6,4,3,1.7),
+       bb = c(rep(TRUE,5),rep(FALSE,4)),
+       fred = c("foo","a","b","foo","a","vf","er","r","g"),
+       stringsAsFactors=FALSE)
> uniquecombs(df)
   f   x    bb fred
1 er 0.5  TRUE  foo
2  3 1.0  TRUE    a
3  b 1.4  TRUE    b
6  3 0.6 FALSE   vf
7  1 4.0 FALSE   er
8  2 3.0 FALSE    r
9  b 1.7 FALSE    g
> 
> 
> 
> cleanEx()
> nameEx("vcov.gam")
> ### * vcov.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vcov.gam
> ### Title: Extract parameter (estimator) covariance matrix from GAM fit
> ### Aliases: vcov.gam
> ### Keywords: models smooth regression
> 
> ### ** Examples
>  
> require(mgcv)
> n <- 100
> x <- runif(n)
> y <- sin(x*2*pi) + rnorm(n)*.2
> mod <- gam(y~s(x,bs="cc",k=10),knots=list(x=seq(0,1,length=10)))
> diag(vcov(mod))
 (Intercept)       s(x).1       s(x).2       s(x).3       s(x).4       s(x).5 
0.0003512824 0.0018908285 0.0020516235 0.0019509499 0.0019280202 0.0022531870 
      s(x).6       s(x).7       s(x).8 
0.0019856105 0.0019067656 0.0014655286 
> 
> 
> 
> cleanEx()
> nameEx("vis.gam")
> ### * vis.gam
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vis.gam
> ### Title: Visualization of GAM objects
> ### Aliases: vis.gam persp.gam
> ### Keywords: hplot models smooth regression
> 
> ### ** Examples
> 
> library(mgcv)
> set.seed(0)
> n<-200;sig2<-4
> x0 <- runif(n, 0, 1);x1 <- runif(n, 0, 1)
> x2 <- runif(n, 0, 1)
> y<-x0^2+x1*x2 +runif(n,-0.3,0.3)
> g<-gam(y~s(x0,x1,x2))
> old.par<-par(mfrow=c(2,2))
> # display the prediction surface in x0, x1 ....
> vis.gam(g,ticktype="detailed",color="heat",theta=-35)  
> vis.gam(g,se=2,theta=-35) # with twice standard error surfaces
> vis.gam(g, view=c("x1","x2"),cond=list(x0=0.75)) # different view 
> vis.gam(g, view=c("x1","x2"),cond=list(x0=.75),theta=210,phi=40,
+         too.far=.07)
> # ..... areas where there is no data are not plotted
> 
> # contour examples....
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="heat")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="terrain")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="topo")
> vis.gam(g, view=c("x1","x2"),plot.type="contour",color="cm")
> 
> 
> par(old.par)
> 
> # Examples with factor and "by" variables
> 
> fac<-rep(1:4,20)
> x<-runif(80)
> y<-fac+2*x^2+rnorm(80)*0.1
> fac<-factor(fac)
> b<-gam(y~fac+s(x))
> 
> vis.gam(b,theta=-35,color="heat") # factor example
> 
> z<-rnorm(80)*0.4   
> y<-as.numeric(fac)+3*x^2*z+rnorm(80)*0.1
> b<-gam(y~fac+s(x,by=z))
> 
> vis.gam(b,theta=-35,color="heat",cond=list(z=1)) # by variable example
> 
> vis.gam(b,view=c("z","x"),theta= -135) # plot against by variable
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ziP")
> ### * ziP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ziP
> ### Title: GAM zero-inflated (hurdle) Poisson regression family
> ### Aliases: ziP
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> rzip <- function(gamma,theta= c(-2,.3)) {
+ ## generate zero inflated Poisson random variables, where 
+ ## lambda = exp(gamma), eta = theta[1] + exp(theta[2])*gamma
+ ## and 1-p = exp(-exp(eta)).
+    y <- gamma; n <- length(y)
+    lambda <- exp(gamma)
+    eta <- theta[1] + exp(theta[2])*gamma
+    p <- 1- exp(-exp(eta))
+    ind <- p > runif(n)
+    y[!ind] <- 0
+    np <- sum(ind)
+    ## generate from zero truncated Poisson, given presence...
+    y[ind] <- qpois(runif(np,dpois(0,lambda[ind]),1),lambda[ind])
+    y
+ } 
> 
> library(mgcv)
> ## Simulate some ziP data...
> set.seed(1);n<-400
> dat <- gamSim(1,n=n)
Gu & Wahba 4 term additive model
> dat$y <- rzip(dat$f/4-1)
> 
> b <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(),data=dat)
> 
> b$outer.info ## check convergence!!
$conv
[1] "full convergence"

$iter
[1] 10

$grad
              [,1]
[1,]  3.216960e-04
[2,]  1.718293e-03
[3,]  4.372002e-05
[4,]  1.001571e-04
[5,] -7.564596e-05
[6,] -5.846652e-04

$hess
             [,1]        [,2]         [,3]        [,4]        [,5]         [,6]
[1,] 101.93262678 175.2537686  0.232671668  0.44079859  0.81421001 -0.068086736
[2,] 175.25376860 380.7369768 -0.100772446  1.03832132  0.14196719 -0.371676466
[3,]   0.23267167  -0.1007724  0.682549166 -0.03902081 -0.03111944 -0.008807434
[4,]   0.44079859   1.0383213 -0.039020807  0.33279908  0.00478774 -0.020386558
[5,]   0.81421001   0.1419672 -0.031119440  0.00478774  2.89502919  0.016320649
[6,]  -0.06808674  -0.3716765 -0.008807434 -0.02038656  0.01632065  0.030793118

$score.hist
 [1] 715.5231 655.8980 630.4674 615.6185 612.3983 611.4905 611.4670 611.4402
 [9] 611.4034 611.4030

> b

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.76 7.40 1.30  total = 14.88 

REML score: 611.403     
> plot(b,pages=1)
> plot(b,pages=1,unconditional=TRUE) ## add s.p. uncertainty 
> gam.check(b)

Method: REML   Optimizer: outer newton
full convergence after 10 iterations.
Gradient range [-0.0005846652,0.001718293]
(score 611.403 & scale 1).
Hessian positive definite, eigenvalue range [0.02840486,465.2732].
Model rank =  37 / 37 

Basis dimension (k) checking results. Low p-value (k-index<1) may
indicate that k is too low, especially if edf is close to k'.

        k'  edf k-index p-value  
s(x0) 9.00 2.42    0.84   0.075 .
s(x1) 9.00 2.76    0.94   0.845  
s(x2) 9.00 7.40    0.88   0.280  
s(x3) 9.00 1.30    0.87   0.275  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> ## more checking...
> ## 1. If the zero inflation rate becomes decoupled from the linear predictor, 
> ## it is possible for the linear predictor to be almost unbounded in regions
> ## containing many zeroes. So examine if the range of predicted values 
> ## is sane for the zero cases? 
> range(predict(b,type="response")[b$y==0])
[1] 0.060769 9.696370
> 
> ## 2. Further plots...
> par(mfrow=c(2,2))
> plot(predict(b,type="response"),residuals(b))
> plot(predict(b,type="response"),b$y);abline(0,1,col=2)
> plot(b$linear.predictors,b$y)
> qq.gam(b,rep=20,level=1)
> 
> ## 3. Refit fixing the theta parameters at their estimated values, to check we 
> ## get essentially the same fit...
> thb <- b$family$getTheta()
> b0 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(theta=thb),data=dat)
> b;b0

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.76 7.40 1.30  total = 14.88 

REML score: 611.403     

Family: Zero inflated Poisson(-1.77,1.187) 
Link function: identity 

Formula:
y ~ s(x0) + s(x1) + s(x2) + s(x3)

Estimated degrees of freedom:
2.42 2.76 7.40 1.30  total = 14.88 

REML score: 611.403     
> 
> ## Example fit forcing minimum linkage of prob present and
> ## linear predictor. Can fix some identifiability problems.
> b2 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=ziP(b=.3),data=dat)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("ziplss")
> ### * ziplss
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ziplss
> ### Title: Zero inflated (hurdle) Poisson location-scale model family
> ### Aliases: ziplss
> ### Keywords: models regression
> 
> ### ** Examples
> 
> library(mgcv)
> ## simulate some data...
> f0 <- function(x) 2 * sin(pi * x); f1 <- function(x) exp(2 * x)
> f2 <- function(x) 0.2 * x^11 * (10 * (1 - x))^6 + 10 * 
+             (10 * x)^3 * (1 - x)^10
> n <- 500;set.seed(5)
> x0 <- runif(n); x1 <- runif(n)
> x2 <- runif(n); x3 <- runif(n)
> 
> ## Simulate probability of potential presence...
> eta1 <- f0(x0) + f1(x1) - 3
> p <- binomial()$linkinv(eta1) 
> y <- as.numeric(runif(n)<p) ## 1 for presence, 0 for absence
> 
> ## Simulate y given potentially present (not exactly model fitted!)...
> ind <- y>0
> eta2 <- f2(x2[ind])/3
> y[ind] <- rpois(exp(eta2),exp(eta2))
> 
> ## Fit ZIP model... 
> b <- gam(list(y~s(x2)+s(x3),~s(x0)+s(x1)),family=ziplss())
> b$outer.info ## check convergence
$conv
[1] "full convergence"

$iter
[1] 7

$grad
              [,1]
[1,] -2.597607e-06
[2,] -1.918936e-05
[3,] -1.186818e-04
[4,] -3.540047e-04

$hess
            [,1]        [,2]         [,3]         [,4]
[1,] 3.444649816 0.004447179 0.0000000000 0.0000000000
[2,] 0.004447179 0.008913900 0.0000000000 0.0000000000
[3,] 0.000000000 0.000000000 1.3632659544 0.0001291374
[4,] 0.000000000 0.000000000 0.0001291374 0.0003571959

$score.hist
[1] 881.4945 879.6256 879.0983 879.0793 879.0736 879.0719 879.0713

> 
> summary(b) 

Family: ziplss 
Link function: identity identity 

Formula:
y ~ s(x2) + s(x3)
~s(x0) + s(x1)

Parametric coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept)    1.14488    0.04888  23.420   <2e-16 ***
(Intercept).1 -0.05605    0.06467  -0.867    0.386    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Approximate significance of smooth terms:
          edf Ref.df   Chi.sq  p-value    
s(x2)   8.025  8.714 1071.346  < 2e-16 ***
s(x3)   1.148  1.282    0.208    0.864    
s.1(x0) 3.869  4.787   31.414 9.15e-06 ***
s.1(x1) 1.004  1.008   70.563  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Deviance explained = 61.6%
-REML = 879.07  Scale est. = 1         n = 500
> plot(b,pages=1)
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  198.804 150.565 101.537 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
